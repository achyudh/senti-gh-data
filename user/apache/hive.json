{"_default": {"1": {"ekoifman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/727581d749306d5df3b6ab8c4adb21ef0fe31506", "message": "HIVE-18460 - Compactor doesn't pass Table properties to the Orc writer (Eugene Koifman, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/dfd7ea368dc8ea2ef071543c3d0cea9e05cd115a", "message": "HIVE-18419 - CliDriver loads different hive-site.xml into HiveConf and MetastoreConf (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fe3190d19fa1324a6835692e71f747e6cb37c7d1", "message": "HIVE-18429 - Compaction should handle a case when it produces no output (Eugene Koifman, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vihangk1": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/ef9d3ee97cee30725381013f4051790c432aa726", "message": "HIVE-18323 : Vectorization: add the support of timestamp in VectorizedPrimitiveColumnReader for parquet (Vihang Karajgaonkar, reviewed by Aihua Xu and Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/798a17c679fc04e5a20b15675b6883f52a26ea57", "message": "HIVE-18461 : Fix precommit hive job (Vihang Karajgaonkar)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/287", "title": "HIVE-17580 : Remove standalone-metastore's dependency with serdes", "body": "Removing the dependency on serdes for the metastore requires a series of changes. I have created multiple commits which hopefully would be easier to review. Each major commit has a descriptive commit message to give a high level idea of what the change is doing. There are still some bits which need to be completed but it would be good to a review.\r\n\r\nOverview of all the changes done:\r\n1. Creates a new module called serde-api under storage-api like discussed. Although I think we can keep it separate as well.\r\n2. Moved List, Map, Struct, Constant, Primitive, Union ObjectInspectors to serde-api\r\n3. Moved PrimitiveTypeInfo, PrimitiveTypeEntry and TypeInfo to serde-api.\r\n4. Moved TypeInfoParser, TypeInfoFactory to serde-api\r\n5. Added a new class which reading avro storage schema by copying the code from AvroSerde and AvroSerdeUtils. The parsing is done such that String value is first converted into TypeInfos and then into FieldSchemas bypassing the need for ObjectInspectors. In theory we could get rid of TypeInfos as well but that path was getting too difficult with lot of duplicate code between Hive and metastore.\r\n6. Introduces a default storage schema reader. I noticed that most of the serdes use the same logic to parse the metadata information. This code should be refactored to a common place instead of having many copies (one in standalone hms and another set in multiple serdes)\r\n7. Moved HiveChar, HiveVarchar, HiveCharWritable, HiveVarcharWritable to storage-api. I noticed that HiveDecimal is already in storage-api. It probably makes sense to move the other primitive types (timestamp, interval etc)to storage-api as well but it requires storage-api to be upgraded to Java 8.\r\n8. Adds a basic test for the schema reader. I plan to add more tests as this code is reviewed.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcamachor": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/80e6f7b0f13c134642763b0a692c3abbf3ffe106", "message": "HIVE-18386 : Create dummy materialized views registry and make it configurable (Jesus Camacho Rodriguez via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/a45becb1602573cae32673e32161b1a3aa10a86b", "message": "HIVE-18473: Infer timezone information correctly in DruidSerde (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ac247817f441eba5daf5b07cc831347d1c762ee4", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum II)\n\n* Fix dangling tests"}, {"url": "https://api.github.com/repos/apache/hive/commits/b1cdbc60d6ef856b852afe1aa44bd3520d5fb84a", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum)"}, {"url": "https://api.github.com/repos/apache/hive/commits/7e64114ddca5c07a0c4ac332c1b34b534cc2e9ed", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdere": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/01816fca2c2620f4f57a2b2a5c6ea404e1f0038f", "message": "HIVE-18430: Add new determinism category for runtime constants (current_date, current_timestamp) (Jason Dere, reviewed by Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/790976907070a2a737186543d81c71eb542a5337", "message": "HIVE-18385: mergejoin fails with java.lang.IllegalStateException (Jason Dere, reviewed by Deepak Jaiswal)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vaibhavgumashta": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/456a65180dcb84f69f26b4c9b9265165ad16dfe4", "message": "HIVE-17495: CachedStore: prewarm improvement (avoid multiple sql calls to read partition column stats), refactoring and caching some aggregate stats (Vaibhav Gumashta reviewed by Daniel Dai)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313", "body": "Looks good, +1.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083", "body": "+1. Looks good.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "pvary": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/fbb3ed15fd0f078369c1fcb2edd68326d272758a", "message": "HIVE-18372: Create testing infra to test different HMS instances (Peter Vary, reviewed by Marta Kuczora, Vihang Karajgaonkar and Adam Szita)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6938fcabf0944817fed8241c48b57fb3f5d98c69", "message": "HIVE-18443: Ensure git gc finished in ptest prep phase before copying repo (Adam Szita, via Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b826072edb429214e9fe073dea9381449396f05d", "message": "HIVE-18355: Add builder for metastore Thrift classes missed in the first pass - FunctionBuilder (Peter Vary, reviewed by Alan Gates)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "winningsix": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/17abdb211c1b2b749fc7d8265d31e6c5987cea4b", "message": "HIVE-18411: Fix ArrayIndexOutOfBoundsException for VectorizedListColumnReader (Colin Ma, reviewed by Ferdinand Xu)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/251", "title": "HIVE-14836: Test the predicate pushing down support for Parquet vecto\u2026", "body": "\u2026rization read path\r\n\r\nAdd more unit test for Predicate pushing down for Parquet Vectorization.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380", "body": "Thanks @thejasmn for working on it. Just some minor issues. @sundapeng Can you take a look at this PR? Thank you!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "deepeshk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/7942bc6c9a38e71ff3f8936508cc773a464b0d87", "message": "HIVE-18465: Hive metastore schema initialization failing on postgres (Deepesh Khandelwal, reviewed by Jesus Camacho Rodriguez)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sershe-apache": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/78d5572f8e165a907c63e1ef8579f591b8c34563", "message": "HIVE-18452 : work around HADOOP-15171 (Sergey Shelukhin, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/72684f10da8f124dfa624b16f3ffebeb2155664d", "message": "HIVE-18273 : add LLAP-level counters for WM (Sergey Shelukhin, reviewed by Harish Jaiprakash and Gunther Hagleitner)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fde503dcaa1f3e322eb4dc34bfffbec89aefd240", "message": "HIVE-18437 : use plan parallelism for the default pool if both are present (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alanfgates": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/d9801d9c6c406d5871147b80bc2e0359c3dbd085", "message": "HIVE-17982 Move metastore specific itests.  This closes #279.  (Alan Gates, reviewed by Peter Vary)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/291", "title": "HIVE-17983 Make the standalone metastore generate tarballs etc.", "body": "See JIRA for full comments.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sahilTakiar": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/87860fbca42b2477f504c8b1cefd43c865a2629c", "message": "HIVE-18214: Flaky test: TestSparkClient (Sahil Takiar, reviewed by Peter Vary)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kgyrtkirk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/2402f3ca55e3f304e041ff53591f846634fbc9ac", "message": "HIVE-18149 HIVE-18416: addendum"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "k0b3rIT": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/13c1bf9e903c502dbfe3f2fb10959ac1a64bc0ad", "message": "HIVE-18161: Remove hive.stats.atomic (Bertalan Kondrat via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "abstractdog": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/5d4559bbdb5fb36450b67841d0da4fa2fecdeeea", "message": "HIVE-18309: qtests: smb_mapjoin_19.q breaks bucketsortoptimize_insert_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/19c2b7225e1c072cba5c1a93329f70059e7e063d", "message": "HIVE-18314: qtests: semijoin_hint.q breaks hybridgrace_hashjoin_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anishek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/22df53b6c223f03edee1a8c271a77e91d66bb2a1", "message": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys (Anishek Agarwal, reviewed Daniel Dai)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/289", "title": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/286", "title": "HIVE-18352: introduce a METADATAONLY option while doing REPL DUMP to allow integrations of other tools", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/283", "title": "HIVE-17829: ArrayIndexOutOfBoundsException - HBASE-backed tables with Avro schema in Hive2", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/263", "title": "HIVE-17830: dbnotification fails to work with rdbms other than postgres", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/262", "title": "HIVE-17825: Socket not closed when trying to read files to copy over in replication from metadata", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/259", "title": "HIVE-17615 : Task.executeTask has to be thread safe for parallel execution", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/246", "title": "HIVE-17426: Execution framework in hive to run tasks in parallel other than MR Tasks", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/240", "title": "HIVE-17410 : repl load task during subsequent DAG generation does notstart from the last partition processed", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/237", "title": "HIVE-16886: HMS log notifications may have duplicated event IDs if multiple HMS are running concurrently", "body": "\u2026ltiple HMS are running concurrently", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daijyc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/970d8c9680d89708c4007de214fdc72b77deb7da", "message": "HIVE-18299: DbNotificationListener fail on mysql with \"select for update\" (Daniel Dai, reviewed by Anishek Agarwal)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c0734ac91bdd3e81b17d93fcfeddd4503430eea8", "message": "HIVE-18298: Fix TestReplicationScenarios.testConstraints (Daniel Dai, reviewed by Sankar Hariappan)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/236", "title": "HIVE-17366: Constraint replication in bootstrap", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dosoft": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/292", "title": "HIVE-17331: Use Path instead of String as key type of the pathToAliases", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/234", "title": "HIVE-17314: Removed obsolete code", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/230", "title": "HIVE-17313: Fixed 'case fall-through'", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/229", "title": "HIVE-17311: Fixed numeric overflow, added test", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sankarh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/290", "title": "HIVE-18192: Introduce WriteID per table rather than using global transaction ID", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/280", "title": "HIVE-18031: Support replication for Alter Database operation", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "msydoron": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/288", "title": "HIVE-18423", "body": "Added full support for jdbc external tables in hive.\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/285", "title": "HIVE-16480 (ORC-285) Empty vector batches of floats or doubles gets", "body": "EOFException.\r\n\r\nSigned-off-by: Owen O'Malley <omalley@apache.org>", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738", "body": "This has been committed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17354974", "body": "But it recreates target/tmp in the next line. Which test assumes that files are pre-located there?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17354974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005", "body": "I did port all of the TestOrcFile to the vectorized writer in the new file TestVectorOrcFile. Does that address your concerns?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685", "body": "This patch doesn't actually change the write path. I've done this in HIVE-12055 including adding a new writer version.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185", "body": "The CompressionCodec isn't really an API class, since the user isn't able to add their own implementations (other than the external LZO implementation). I guess I can move it over, if you think it is important. In general LLAP is accessing ORC beneath the API level.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588", "body": "The DirectDecompressor API was added in Hadoop 2.3. I'd rather go ahead and make it a real shim so that we can support older Hadoop versions.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527", "body": "I'll add a comment, but it does clone the FileHandle so that they can be closed separately.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369", "body": "Ok, I've updated both Char and Varchar with the check.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229", "body": "For now we should stick with what Hive supports. I copied this code directly from Hive's HadoopShims.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104", "body": "I'm not sure what you think is misaligned, but I simplified this code.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "amrk7s": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/284", "title": "HIVE-18338 Exposing asynchronous execution through hive-jdbc client", "body": "**Problem statement**\r\n\r\nHive JDBC currently exposes 2 methods related to asynchronous execution\r\n**executeAsync()** - to trigger a query execution and return immediately.\r\n**waitForOperationToComplete()** - which waits till the current execution is complete **blocking the user thread**. \r\n\r\nThis has one problem\r\n- If the client process goes down, there is no way to resume queries although hive server is completely asynchronous. \r\n\r\n**Proposal**\r\n\r\nIf operation handle could be exposed, we can latch on to an active execution of a query.\r\n\r\n**Code changes**\r\n\r\nOperation handle is exposed. So client can keep a copy.\r\nlatchSync() and latchAsync() methods take an operation handle and try to latch on to the current execution in hive server if present", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mattk42": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/278", "title": "HIVE-18295 - Add ability to ignore invalid values in JSON SerDe ", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ymwdalex": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/273", "title": "HIVE-18130: Update table path to storage parameters when alter a table", "body": "When an managed table is created by Spark, table path information is not only store in `location` field (first figure), but also in `parameters.path` fields (second figure).\r\n<img width=\"445\" alt=\"location\" src=\"https://user-images.githubusercontent.com/1458656/33123050-466c6bdc-cf79-11e7-8b53-85ad7dc83f17.png\">\r\n<img width=\"677\" alt=\"storage_parameter\" src=\"https://user-images.githubusercontent.com/1458656/33123051-469576da-cf79-11e7-9c6c-9793a28f8389.png\">\r\n\r\nWhen hive alter a table, `storage parameter` is ignored. Then, spark cannot access the table anymore because spark use the parameter.\r\n\r\nIn this PR, when altering a table, the field `storageDescription.parameters.path` is also updated, if `path` key exists\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hejian991": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/269", "title": "fix small bug: when disable partition statistics in hive-site.xml, bu\u2026", "body": "\u2026t it does not work.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ishitbatra": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/268", "title": "hive installation link", "body": "hive link given", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thejasmn": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/266", "title": "Hive-17897", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268", "body": "addressing the comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518", "body": "Updated patch addresses additional comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "kanna14243": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/265", "title": "Update TBinarySortableProtocol.java", "body": "writeTextBytes doesn't respect the start parameter. It'll work only for cases where start = 0. Fixing it so that it'll work for any value of start.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "b-slim": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/249", "title": "[HIVE-17523] Fix insert into bug", "body": "https://issues.apache.org/jira/browse/HIVE-17523", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmartin-rp": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/232", "title": "Update 039-HIVE-12274.oracle.sql", "body": "You cannot modify a column from VARCHAR2(4000) to CLOB directly. You need to add a new column and drop the old one or recreate the table. I chose to recreate the table because it's more clean.\r\n\r\nhttps://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:1770086700346491686\r\n\r\nAlso, if you do something like:\r\nALTER TABLE COLUMNS_V2 MODIFY (COLUMN_NAME VARCHAR(767) NOT NULL);\r\nAnd the column is already NOT NULL, the ALTER TABLE fails.  I removed 2 like this.\r\n\r\nI hope it helps.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jarcec": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868", "body": "Hi Viraj,\nunfortunately we can't accept pull requests due to licensing issues. You need to attach your patch to the JIRA in order to enable Hive community to get it in.\n\nJarcec\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "brockn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540", "body": "See https://issues.apache.org/jira/browse/HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pavel-sakun": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553", "body": "Closing this one in favour of HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "codingtony": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235", "body": "This is the fix  release 0.13.1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "slavag-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547", "body": "please do not merge for now till we can get a release out.  thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "srowen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047", "body": "PS I found this from your cross-posted question about Kendall coefficient on Hive. What is this? PRs aren't for questions and this is some arbitrary branch of commits you've opened.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "rdblue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346", "body": "Hi @dguy, I've been working on this problem recently in [HIVE-8909](https://issues.apache.org/jira/browse/HIVE-8909). That's already been merged, but I'm wondering if there's a case that we've missed that you fix here?\n\nIt looks like this allows the object inspector to work with the extra level of nesting that is included by the deserializer or detect if it isn't there. For HIVE-8909, we opted to keep the extra level of nesting just so we wouldn't need to change the object inspector code. We will need to follow up with a commit that removes the added layer from both. Did you come across a case where the added layer isn't there?\n\nAlso, I don't think Hive uses pull requests, so the best way to follow up is to open an issue in the JIRA issue tracker: https://issues.apache.org/jira/browse/HIVE\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972", "body": "Hi Damian, thanks for following up and letting us know it's working!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dguy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937", "body": "Hi Ryan,\n\nThanks for getting in touch. We are writing avro/parquet data-sets from\nvarious Map/Reduce jobs - we then create external hive tables for these\ndata-sets. The issue we are seeing is that AvroSchemaConverter\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/main/java/parquet/avro/AvroSchemaConverter.java\nconverts avro array types like this:\n\n{\n     \"name\" : \"myarray\",\n     \"type\" : {\n       \"type\" : \"array\",\n       \"items\" : \"int\"\n }\n\ninto Parquet:\nrequired group myarray (LIST) {\n       repeated int32 array;\n }\n\nYou can see an example here\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/test/java/parquet/avro/TestAvroSchemaConverter.java\n\nWhere as hive is expecting something like:\nrequired group myarray  {\n       repeated group bag {\n          optional int32 array;\n       }\n }\n\nWithout this change our external tables with array types currently won't\ndeserialize. So my patch was an attempt to make it work for both cases i\nknow about. I'll have to have a look through your patch and see if it\nhelps.\n\nThanks for the pointer.\nCheers,\nDamian\n\nOn 10 December 2014 at 01:50, Ryan Blue notifications@github.com wrote:\n\n> Hi @dguy https://github.com/dguy, I've been working on this problem\n> recently in [HIVE-8909|https://issues.apache.org/jira/browse/HIVE-8909].\n> That's already been merged, but I'm wondering if there's a case that we've\n> missed that you fix here?\n> \n> It looks like this allows the object inspector to work with the extra\n> level of nesting that is included by the deserializer or detect if it isn't\n> there. For HIVE-8909, we opted to keep the extra level of nesting just so\n> we wouldn't need to change the object inspector code. We will need to\n> follow up with a commit that removes the added layer from both. Did you\n> come across a case where the added layer isn't there?\n> \n> Also, I don't think Hive uses pull requests, so the best way to follow up\n> is to open an issue in the JIRA issue tracker:\n> https://issues.apache.org/jira/browse/HIVE\n> \n> Thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hive/pull/26#issuecomment-66392346.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730", "body": "Hi,\nI've verified it works fine with your fix. Thanks for pointing me to this.\nCheers,\nDamian\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "swarnim87": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308", "body": "Finish javadoc\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "apivovarov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "zhichao-li": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233", "body": "cc @apivovarov \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Lewuathe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402", "body": "Sorry for mistake.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "prasanthj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388", "body": "@kirill-vlasov Could you please create a JIRA here https://issues.apache.org/jira/browse/HIVE/ and add the JIRA id in the title of the PR?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15379454", "body": "@phoenixhadoop hive trunk is updated with new hive-log4j2.properties based configuration. If log4j.configurationFile is not explicitly set then by default hive-log4j2.properties file will be looked up in CLASSPATH. Typically, HIVE_CONF_DIR will be set to conf directory which contains hive-log4j2.properties file. HIVE_CONF_DIR is automatically added to CLASSPATH by the hive script. Are you still seeing this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15379454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629", "body": "Would it be easier to support bulk writes? As isPresent is called for every column.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414", "body": "Actually never mind, since the output stream is buffered. This won't be big win.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051", "body": "What happens if the vec is null and started is false? A run can start with null structs (struct column) right?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177", "body": "Again ignore this :) I guess that case is handled by super(). Correct me if I am wrong.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540", "body": "It will be good to add tests that uses WriterOptions.setSchema() to specify the types instead of OI. I see that you have fixed bunch of OI casts that would have cause NPE otherwise (if TypeDescription is set through writer).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842", "body": "Do we need to bump up the writer version? To differentiate ORC files written with row-by-row vs vector writer?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511", "body": "Yes. Sorry I missed that file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544", "body": "Can we put all the interfaces in api package?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318", "body": "This isn't a shim right? Can this be moved over to OrcUtils?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625", "body": "Does it clone the options/properties alone? or the fs input stream as well? Can you please add a comment as in what does it clone and what not.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043", "body": "Should we check if it has hasMaximumLength()?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417", "body": "Is there a minimum supported hadoop version for orc? If we don't support older hadoop version then this shim might not be required.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431", "body": "Where do we close this stream?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440", "body": "nit: bad alignment\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450", "body": "ql dependency?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466", "body": "Can this be moved to common instead?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488", "body": "Never mind. This is already part of storage-api. So not a problem.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "joshelser": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513", "body": "Thanks for submitting a PR for this, @chutium. I left you some comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867", "body": "Isn't the `RecordWriter<Text, Mutation>` duplicative since `AccumuloRecordWriter` already implements that?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048", "body": "This seems wrong. For the AccumuloRecordWriter, the `Text` can signify an Accumulo table to which this update should be written. Where is this `Writable` coming from?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "chutium": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846", "body": "thanks for reviewing, to reproduce the issue, you can create a spark context with following jars:\naccumulo-core-1.6.0.jar\naccumulo-fate-1.6.0.jar\naccumulo-start-1.6.0.jar\naccumulo-trace-1.6.0.jar\nhive-accumulo-handler-1.2.1.jar\n\nthen run some hive query like:\n\n```\nset accumulo.instance.name=instance_name;\nset accumulo.zookeepers=zk_host:2181;\nset accumulo.user.name=user;\nset accumulo.user.pass=pass;\n\nCREATE TABLE testtable(rowid STRING, value STRING)\nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,colf:colq');\n\nINSERT OVERWRITE TABLE testtable SELECT col1, col2 FROM some_hive_table;\n```\n\nthen will get error `java.lang.ClassCastException` says `HiveAccumuloTableOutputFormat cannot be cast to org.apache.hadoop.hive.ql.io.HiveOutputFormat`\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969", "body": "yes... seems like RecordWriter is unnecessary, will check this soon\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651", "body": "noticed and wondering as well, but it works in our trials... the mutations will be written into the target table, that defined in hive query -_- my assume is the table name is already set in JobConf, so we do not need to specify table name for each write operation.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832", "body": "I might pick this up at a later date.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jiangxt2": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183", "body": "Is it my fault? How can i fix it? thx my bro. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475", "body": "@dmtolpeko Hi, I created a JIRA for this problem.\nThe url is https://issues.apache.org/jira/browse/HIVE-13877\nWhat should I do next?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588", "body": "@dmtolpeko OK, I GOT IT. Thanks very much. I'll wait for your response.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dmtolpeko": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191", "body": "Can you create a Hive JIRA for this problem? Github is just a mirror\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901", "body": "I will review your changes and create a patch. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "dark0dave": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930", "body": "named travis file wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "yoni": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/2375698", "body": "This change breaks development on OS X. Not sure if anyone else is having the same issues I am, but I can't seem to work around this. `git status` constantly reports changes have been made:\n\n```\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n$ git reset --hard\nHEAD is now at af0162d HIVE-446 Implement TRUNCATE\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/2375698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sameeragarwal": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/3868174", "body": "Shouldn't this be: \n\n``` java\nmyagg.variance += varianceFieldOI.get(partialVariance)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/3868174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "ksumit": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/5973358", "body": "shouldn't this be 0.13.0 for this branch? i tried compiling the current tip, it's broken and changing it to 0.13.0 works just fine.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/5973358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "HZMengYue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/8795892", "body": "hi,i checkout a spark branch today,and after built it,derby-10.11.1.1.jar in hive dir lib,but when i run hive shell ,like  ./hive --auxpath /home/hispark/spark-1.2.0/assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar,it give me follow error:and i think u had fixed this problem.\nLogging initialized using configuration in file:/home/hispark/apache-hive-0.15.0-SNAPSHOT-bin/conf/hive-log4j.properties\nException in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:449)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:634)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:578)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1481)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2674)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2693)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:430)\n        ... 7 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1479)\n        ... 12 more\nCaused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory\nNestedThrowables:\njava.lang.reflect.InvocationTargetException\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n        at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:341)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:370)\n        at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:267)\n        at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:234)\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:572)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:550)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:603)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:441)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5598)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:182)\n        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)\n        ... 17 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)\n        at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)\n        at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)\n        at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n        at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n        at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n        ... 46 more\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccess\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/8795892/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sershe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/9465637", "body": "should be gone\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/9465752", "body": "this is wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465752/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jalajthanaki": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/11626965", "body": "I want to ask you ,Is there full text index kind of functionality available in Apache Hive for searching, matching & ranking keywords of the text?\n\nIf any functionality will be there then let me know.\n\nThanks,\nJalaj T\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/11626965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "merlin-zaraza": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14273837", "body": "Ok, initialize method is deprecated. How to use GenericUDTF now? (e.g. check for arguments)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14273837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "walla2sl": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14312112", "body": "I'm having the same issue. How is GenericUDTF usable now? After trying to use a UTDF I created, I keep getting \"Error while compiling statement: FAILED: IllegalStateException Should not be called directly\". \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/14312367", "body": "I opened https://issues.apache.org/jira/browse/HIVE-12377\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "phoenixhadoop": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15241484", "body": "I am using hive-2.0.0-SNAPSHOT\nwhen starting metastore I meet the ERROR: ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\n\nwhen  I configure :export HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \",the ERROR disappear.\n\nHow can I do?\n\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15241484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/15244734", "body": "Hi,prasanthj \nNow we use hive-2.0.0-SNAPSHOT,we meet an error when starting the service metastore .\nThe error is :ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\nIf we configure the variable HADOOP_CLIENT_OPTS  as\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \" ,the error disappear,but entering hive cli slower.\nPlease help me how can avoid the ERROR.\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15244734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "prongs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17350037", "body": "This is deleting `target/tmp/`causing all subsequent tests to fail. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17350037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363722", "body": "All of them!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363749", "body": "Check this: https://github.com/apache/hive/blob/master/pom.xml#L914. Then this: https://github.com/apache/hive/blob/master/pom.xml#L991\n\nFirst one sets up target/tmp/conf and second one is adding that to classpath. If the directory gets deleted, subsequent tests that create a `new HiveConf()` don't get to read `target/tmp/conf/hive-site.xml` and there are some test specific properties there. You can't just delete the directory and recreate. This should have been caught in review. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "goyalr41": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17575099", "body": "Hi,\n\nJust wanted to know how to access this function now? Should I flatten the map returned by new function getMapRedStats().\n\nThanks,\nRaman\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17575099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "djhwx": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20446807", "body": "The build is failing in tests. Can you please revert this back to unblock others and check this in which fixed?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20446807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nguyenhoan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20889577", "body": "Hi @rbalamohan \r\nWe are a team of researchers from Iowa State, The University of Texs at Dallas and Oregon State University, USA. We are investigating common/repeated code changes.\r\nWe have four short questions regarding the change to this statement of the commit.\r\n\r\nQuestions:\r\n\r\nQ1- Is the change at these lines similar to another change from before? (yes, no, not sure)\r\n\r\nQ2- Can you briefly describe the change and why you made it? (for example, checking parameter before calling the method to avoid a Null Pointer Exception)\r\n\r\nQ3- Can you give it a name? (for example, Null Check)\r\n\r\nQ4- Would you like to have this change automated by a tool? (Yes, No, Already automated)\r\n\r\nThe data collected from the answers will never be associated with you or your project. Our questions are about recurring code changes from the developer community, not about personal information. All the data is merged across recurring changes from GitHub repositories. We will publish aggregated data from the trends of the whole community. \r\nWe have a long tradition of developing refactoring tools and contributing them freely to the Eclipse, Netbeans, Android Studio under their respective FLOSS licenses. For example, look at some of our recently released refactoring tools: http://refactoring.info/tools/ \r\n\r\nThank you,\r\nHoan Nguyen https://sites.google.com/site/nguyenanhhoan/\r\nMichael Hilton http://web.engr.oregonstate.edu/~hiltonm/\r\nTien Nguyen http://www.utdallas.edu/~tien.n.nguyen/\r\nDanny Dig http://eecs.oregonstate.edu/people/dig-danny\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20889577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "kiddingbaby": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/26133483", "body": "The property **hive.server.read.socket.timeout** not found in Wiki: Hive Configuration, can I use it in hive v1.2.1?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/26133483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}}, "2": {"ekoifman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/727581d749306d5df3b6ab8c4adb21ef0fe31506", "message": "HIVE-18460 - Compactor doesn't pass Table properties to the Orc writer (Eugene Koifman, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/dfd7ea368dc8ea2ef071543c3d0cea9e05cd115a", "message": "HIVE-18419 - CliDriver loads different hive-site.xml into HiveConf and MetastoreConf (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fe3190d19fa1324a6835692e71f747e6cb37c7d1", "message": "HIVE-18429 - Compaction should handle a case when it produces no output (Eugene Koifman, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vihangk1": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/ef9d3ee97cee30725381013f4051790c432aa726", "message": "HIVE-18323 : Vectorization: add the support of timestamp in VectorizedPrimitiveColumnReader for parquet (Vihang Karajgaonkar, reviewed by Aihua Xu and Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/798a17c679fc04e5a20b15675b6883f52a26ea57", "message": "HIVE-18461 : Fix precommit hive job (Vihang Karajgaonkar)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/287", "title": "HIVE-17580 : Remove standalone-metastore's dependency with serdes", "body": "Removing the dependency on serdes for the metastore requires a series of changes. I have created multiple commits which hopefully would be easier to review. Each major commit has a descriptive commit message to give a high level idea of what the change is doing. There are still some bits which need to be completed but it would be good to a review.\r\n\r\nOverview of all the changes done:\r\n1. Creates a new module called serde-api under storage-api like discussed. Although I think we can keep it separate as well.\r\n2. Moved List, Map, Struct, Constant, Primitive, Union ObjectInspectors to serde-api\r\n3. Moved PrimitiveTypeInfo, PrimitiveTypeEntry and TypeInfo to serde-api.\r\n4. Moved TypeInfoParser, TypeInfoFactory to serde-api\r\n5. Added a new class which reading avro storage schema by copying the code from AvroSerde and AvroSerdeUtils. The parsing is done such that String value is first converted into TypeInfos and then into FieldSchemas bypassing the need for ObjectInspectors. In theory we could get rid of TypeInfos as well but that path was getting too difficult with lot of duplicate code between Hive and metastore.\r\n6. Introduces a default storage schema reader. I noticed that most of the serdes use the same logic to parse the metadata information. This code should be refactored to a common place instead of having many copies (one in standalone hms and another set in multiple serdes)\r\n7. Moved HiveChar, HiveVarchar, HiveCharWritable, HiveVarcharWritable to storage-api. I noticed that HiveDecimal is already in storage-api. It probably makes sense to move the other primitive types (timestamp, interval etc)to storage-api as well but it requires storage-api to be upgraded to Java 8.\r\n8. Adds a basic test for the schema reader. I plan to add more tests as this code is reviewed.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcamachor": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/80e6f7b0f13c134642763b0a692c3abbf3ffe106", "message": "HIVE-18386 : Create dummy materialized views registry and make it configurable (Jesus Camacho Rodriguez via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/a45becb1602573cae32673e32161b1a3aa10a86b", "message": "HIVE-18473: Infer timezone information correctly in DruidSerde (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ac247817f441eba5daf5b07cc831347d1c762ee4", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum II)\n\n* Fix dangling tests"}, {"url": "https://api.github.com/repos/apache/hive/commits/b1cdbc60d6ef856b852afe1aa44bd3520d5fb84a", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum)"}, {"url": "https://api.github.com/repos/apache/hive/commits/7e64114ddca5c07a0c4ac332c1b34b534cc2e9ed", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdere": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/01816fca2c2620f4f57a2b2a5c6ea404e1f0038f", "message": "HIVE-18430: Add new determinism category for runtime constants (current_date, current_timestamp) (Jason Dere, reviewed by Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/790976907070a2a737186543d81c71eb542a5337", "message": "HIVE-18385: mergejoin fails with java.lang.IllegalStateException (Jason Dere, reviewed by Deepak Jaiswal)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vaibhavgumashta": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/456a65180dcb84f69f26b4c9b9265165ad16dfe4", "message": "HIVE-17495: CachedStore: prewarm improvement (avoid multiple sql calls to read partition column stats), refactoring and caching some aggregate stats (Vaibhav Gumashta reviewed by Daniel Dai)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313", "body": "Looks good, +1.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083", "body": "+1. Looks good.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "pvary": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/fbb3ed15fd0f078369c1fcb2edd68326d272758a", "message": "HIVE-18372: Create testing infra to test different HMS instances (Peter Vary, reviewed by Marta Kuczora, Vihang Karajgaonkar and Adam Szita)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6938fcabf0944817fed8241c48b57fb3f5d98c69", "message": "HIVE-18443: Ensure git gc finished in ptest prep phase before copying repo (Adam Szita, via Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b826072edb429214e9fe073dea9381449396f05d", "message": "HIVE-18355: Add builder for metastore Thrift classes missed in the first pass - FunctionBuilder (Peter Vary, reviewed by Alan Gates)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "winningsix": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/17abdb211c1b2b749fc7d8265d31e6c5987cea4b", "message": "HIVE-18411: Fix ArrayIndexOutOfBoundsException for VectorizedListColumnReader (Colin Ma, reviewed by Ferdinand Xu)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/251", "title": "HIVE-14836: Test the predicate pushing down support for Parquet vecto\u2026", "body": "\u2026rization read path\r\n\r\nAdd more unit test for Predicate pushing down for Parquet Vectorization.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380", "body": "Thanks @thejasmn for working on it. Just some minor issues. @sundapeng Can you take a look at this PR? Thank you!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "deepeshk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/7942bc6c9a38e71ff3f8936508cc773a464b0d87", "message": "HIVE-18465: Hive metastore schema initialization failing on postgres (Deepesh Khandelwal, reviewed by Jesus Camacho Rodriguez)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sershe-apache": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/78d5572f8e165a907c63e1ef8579f591b8c34563", "message": "HIVE-18452 : work around HADOOP-15171 (Sergey Shelukhin, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/72684f10da8f124dfa624b16f3ffebeb2155664d", "message": "HIVE-18273 : add LLAP-level counters for WM (Sergey Shelukhin, reviewed by Harish Jaiprakash and Gunther Hagleitner)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fde503dcaa1f3e322eb4dc34bfffbec89aefd240", "message": "HIVE-18437 : use plan parallelism for the default pool if both are present (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alanfgates": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/d9801d9c6c406d5871147b80bc2e0359c3dbd085", "message": "HIVE-17982 Move metastore specific itests.  This closes #279.  (Alan Gates, reviewed by Peter Vary)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/291", "title": "HIVE-17983 Make the standalone metastore generate tarballs etc.", "body": "See JIRA for full comments.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sahilTakiar": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/87860fbca42b2477f504c8b1cefd43c865a2629c", "message": "HIVE-18214: Flaky test: TestSparkClient (Sahil Takiar, reviewed by Peter Vary)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kgyrtkirk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/2402f3ca55e3f304e041ff53591f846634fbc9ac", "message": "HIVE-18149 HIVE-18416: addendum"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "k0b3rIT": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/13c1bf9e903c502dbfe3f2fb10959ac1a64bc0ad", "message": "HIVE-18161: Remove hive.stats.atomic (Bertalan Kondrat via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "abstractdog": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/5d4559bbdb5fb36450b67841d0da4fa2fecdeeea", "message": "HIVE-18309: qtests: smb_mapjoin_19.q breaks bucketsortoptimize_insert_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/19c2b7225e1c072cba5c1a93329f70059e7e063d", "message": "HIVE-18314: qtests: semijoin_hint.q breaks hybridgrace_hashjoin_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anishek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/22df53b6c223f03edee1a8c271a77e91d66bb2a1", "message": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys (Anishek Agarwal, reviewed Daniel Dai)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/289", "title": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/286", "title": "HIVE-18352: introduce a METADATAONLY option while doing REPL DUMP to allow integrations of other tools", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/283", "title": "HIVE-17829: ArrayIndexOutOfBoundsException - HBASE-backed tables with Avro schema in Hive2", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/263", "title": "HIVE-17830: dbnotification fails to work with rdbms other than postgres", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/262", "title": "HIVE-17825: Socket not closed when trying to read files to copy over in replication from metadata", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/259", "title": "HIVE-17615 : Task.executeTask has to be thread safe for parallel execution", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/246", "title": "HIVE-17426: Execution framework in hive to run tasks in parallel other than MR Tasks", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/240", "title": "HIVE-17410 : repl load task during subsequent DAG generation does notstart from the last partition processed", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/237", "title": "HIVE-16886: HMS log notifications may have duplicated event IDs if multiple HMS are running concurrently", "body": "\u2026ltiple HMS are running concurrently", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daijyc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/970d8c9680d89708c4007de214fdc72b77deb7da", "message": "HIVE-18299: DbNotificationListener fail on mysql with \"select for update\" (Daniel Dai, reviewed by Anishek Agarwal)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c0734ac91bdd3e81b17d93fcfeddd4503430eea8", "message": "HIVE-18298: Fix TestReplicationScenarios.testConstraints (Daniel Dai, reviewed by Sankar Hariappan)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/236", "title": "HIVE-17366: Constraint replication in bootstrap", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dosoft": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/292", "title": "HIVE-17331: Use Path instead of String as key type of the pathToAliases", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/234", "title": "HIVE-17314: Removed obsolete code", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/230", "title": "HIVE-17313: Fixed 'case fall-through'", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/229", "title": "HIVE-17311: Fixed numeric overflow, added test", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sankarh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/290", "title": "HIVE-18192: Introduce WriteID per table rather than using global transaction ID", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/280", "title": "HIVE-18031: Support replication for Alter Database operation", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "msydoron": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/288", "title": "HIVE-18423", "body": "Added full support for jdbc external tables in hive.\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/285", "title": "HIVE-16480 (ORC-285) Empty vector batches of floats or doubles gets", "body": "EOFException.\r\n\r\nSigned-off-by: Owen O'Malley <omalley@apache.org>", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738", "body": "This has been committed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17354974", "body": "But it recreates target/tmp in the next line. Which test assumes that files are pre-located there?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17354974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005", "body": "I did port all of the TestOrcFile to the vectorized writer in the new file TestVectorOrcFile. Does that address your concerns?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685", "body": "This patch doesn't actually change the write path. I've done this in HIVE-12055 including adding a new writer version.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185", "body": "The CompressionCodec isn't really an API class, since the user isn't able to add their own implementations (other than the external LZO implementation). I guess I can move it over, if you think it is important. In general LLAP is accessing ORC beneath the API level.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588", "body": "The DirectDecompressor API was added in Hadoop 2.3. I'd rather go ahead and make it a real shim so that we can support older Hadoop versions.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527", "body": "I'll add a comment, but it does clone the FileHandle so that they can be closed separately.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369", "body": "Ok, I've updated both Char and Varchar with the check.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229", "body": "For now we should stick with what Hive supports. I copied this code directly from Hive's HadoopShims.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104", "body": "I'm not sure what you think is misaligned, but I simplified this code.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "amrk7s": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/284", "title": "HIVE-18338 Exposing asynchronous execution through hive-jdbc client", "body": "**Problem statement**\r\n\r\nHive JDBC currently exposes 2 methods related to asynchronous execution\r\n**executeAsync()** - to trigger a query execution and return immediately.\r\n**waitForOperationToComplete()** - which waits till the current execution is complete **blocking the user thread**. \r\n\r\nThis has one problem\r\n- If the client process goes down, there is no way to resume queries although hive server is completely asynchronous. \r\n\r\n**Proposal**\r\n\r\nIf operation handle could be exposed, we can latch on to an active execution of a query.\r\n\r\n**Code changes**\r\n\r\nOperation handle is exposed. So client can keep a copy.\r\nlatchSync() and latchAsync() methods take an operation handle and try to latch on to the current execution in hive server if present", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mattk42": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/278", "title": "HIVE-18295 - Add ability to ignore invalid values in JSON SerDe ", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ymwdalex": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/273", "title": "HIVE-18130: Update table path to storage parameters when alter a table", "body": "When an managed table is created by Spark, table path information is not only store in `location` field (first figure), but also in `parameters.path` fields (second figure).\r\n<img width=\"445\" alt=\"location\" src=\"https://user-images.githubusercontent.com/1458656/33123050-466c6bdc-cf79-11e7-8b53-85ad7dc83f17.png\">\r\n<img width=\"677\" alt=\"storage_parameter\" src=\"https://user-images.githubusercontent.com/1458656/33123051-469576da-cf79-11e7-9c6c-9793a28f8389.png\">\r\n\r\nWhen hive alter a table, `storage parameter` is ignored. Then, spark cannot access the table anymore because spark use the parameter.\r\n\r\nIn this PR, when altering a table, the field `storageDescription.parameters.path` is also updated, if `path` key exists\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hejian991": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/269", "title": "fix small bug: when disable partition statistics in hive-site.xml, bu\u2026", "body": "\u2026t it does not work.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ishitbatra": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/268", "title": "hive installation link", "body": "hive link given", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thejasmn": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/266", "title": "Hive-17897", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268", "body": "addressing the comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518", "body": "Updated patch addresses additional comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "kanna14243": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/265", "title": "Update TBinarySortableProtocol.java", "body": "writeTextBytes doesn't respect the start parameter. It'll work only for cases where start = 0. Fixing it so that it'll work for any value of start.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "b-slim": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/249", "title": "[HIVE-17523] Fix insert into bug", "body": "https://issues.apache.org/jira/browse/HIVE-17523", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmartin-rp": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/232", "title": "Update 039-HIVE-12274.oracle.sql", "body": "You cannot modify a column from VARCHAR2(4000) to CLOB directly. You need to add a new column and drop the old one or recreate the table. I chose to recreate the table because it's more clean.\r\n\r\nhttps://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:1770086700346491686\r\n\r\nAlso, if you do something like:\r\nALTER TABLE COLUMNS_V2 MODIFY (COLUMN_NAME VARCHAR(767) NOT NULL);\r\nAnd the column is already NOT NULL, the ALTER TABLE fails.  I removed 2 like this.\r\n\r\nI hope it helps.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jarcec": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868", "body": "Hi Viraj,\nunfortunately we can't accept pull requests due to licensing issues. You need to attach your patch to the JIRA in order to enable Hive community to get it in.\n\nJarcec\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "brockn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540", "body": "See https://issues.apache.org/jira/browse/HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pavel-sakun": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553", "body": "Closing this one in favour of HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "codingtony": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235", "body": "This is the fix  release 0.13.1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "slavag-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547", "body": "please do not merge for now till we can get a release out.  thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "srowen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047", "body": "PS I found this from your cross-posted question about Kendall coefficient on Hive. What is this? PRs aren't for questions and this is some arbitrary branch of commits you've opened.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "rdblue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346", "body": "Hi @dguy, I've been working on this problem recently in [HIVE-8909](https://issues.apache.org/jira/browse/HIVE-8909). That's already been merged, but I'm wondering if there's a case that we've missed that you fix here?\n\nIt looks like this allows the object inspector to work with the extra level of nesting that is included by the deserializer or detect if it isn't there. For HIVE-8909, we opted to keep the extra level of nesting just so we wouldn't need to change the object inspector code. We will need to follow up with a commit that removes the added layer from both. Did you come across a case where the added layer isn't there?\n\nAlso, I don't think Hive uses pull requests, so the best way to follow up is to open an issue in the JIRA issue tracker: https://issues.apache.org/jira/browse/HIVE\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972", "body": "Hi Damian, thanks for following up and letting us know it's working!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dguy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937", "body": "Hi Ryan,\n\nThanks for getting in touch. We are writing avro/parquet data-sets from\nvarious Map/Reduce jobs - we then create external hive tables for these\ndata-sets. The issue we are seeing is that AvroSchemaConverter\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/main/java/parquet/avro/AvroSchemaConverter.java\nconverts avro array types like this:\n\n{\n     \"name\" : \"myarray\",\n     \"type\" : {\n       \"type\" : \"array\",\n       \"items\" : \"int\"\n }\n\ninto Parquet:\nrequired group myarray (LIST) {\n       repeated int32 array;\n }\n\nYou can see an example here\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/test/java/parquet/avro/TestAvroSchemaConverter.java\n\nWhere as hive is expecting something like:\nrequired group myarray  {\n       repeated group bag {\n          optional int32 array;\n       }\n }\n\nWithout this change our external tables with array types currently won't\ndeserialize. So my patch was an attempt to make it work for both cases i\nknow about. I'll have to have a look through your patch and see if it\nhelps.\n\nThanks for the pointer.\nCheers,\nDamian\n\nOn 10 December 2014 at 01:50, Ryan Blue notifications@github.com wrote:\n\n> Hi @dguy https://github.com/dguy, I've been working on this problem\n> recently in [HIVE-8909|https://issues.apache.org/jira/browse/HIVE-8909].\n> That's already been merged, but I'm wondering if there's a case that we've\n> missed that you fix here?\n> \n> It looks like this allows the object inspector to work with the extra\n> level of nesting that is included by the deserializer or detect if it isn't\n> there. For HIVE-8909, we opted to keep the extra level of nesting just so\n> we wouldn't need to change the object inspector code. We will need to\n> follow up with a commit that removes the added layer from both. Did you\n> come across a case where the added layer isn't there?\n> \n> Also, I don't think Hive uses pull requests, so the best way to follow up\n> is to open an issue in the JIRA issue tracker:\n> https://issues.apache.org/jira/browse/HIVE\n> \n> Thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hive/pull/26#issuecomment-66392346.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730", "body": "Hi,\nI've verified it works fine with your fix. Thanks for pointing me to this.\nCheers,\nDamian\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "swarnim87": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308", "body": "Finish javadoc\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "apivovarov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "zhichao-li": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233", "body": "cc @apivovarov \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Lewuathe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402", "body": "Sorry for mistake.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "prasanthj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388", "body": "@kirill-vlasov Could you please create a JIRA here https://issues.apache.org/jira/browse/HIVE/ and add the JIRA id in the title of the PR?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15379454", "body": "@phoenixhadoop hive trunk is updated with new hive-log4j2.properties based configuration. If log4j.configurationFile is not explicitly set then by default hive-log4j2.properties file will be looked up in CLASSPATH. Typically, HIVE_CONF_DIR will be set to conf directory which contains hive-log4j2.properties file. HIVE_CONF_DIR is automatically added to CLASSPATH by the hive script. Are you still seeing this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15379454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629", "body": "Would it be easier to support bulk writes? As isPresent is called for every column.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414", "body": "Actually never mind, since the output stream is buffered. This won't be big win.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051", "body": "What happens if the vec is null and started is false? A run can start with null structs (struct column) right?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177", "body": "Again ignore this :) I guess that case is handled by super(). Correct me if I am wrong.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540", "body": "It will be good to add tests that uses WriterOptions.setSchema() to specify the types instead of OI. I see that you have fixed bunch of OI casts that would have cause NPE otherwise (if TypeDescription is set through writer).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842", "body": "Do we need to bump up the writer version? To differentiate ORC files written with row-by-row vs vector writer?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511", "body": "Yes. Sorry I missed that file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544", "body": "Can we put all the interfaces in api package?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318", "body": "This isn't a shim right? Can this be moved over to OrcUtils?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625", "body": "Does it clone the options/properties alone? or the fs input stream as well? Can you please add a comment as in what does it clone and what not.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043", "body": "Should we check if it has hasMaximumLength()?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417", "body": "Is there a minimum supported hadoop version for orc? If we don't support older hadoop version then this shim might not be required.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431", "body": "Where do we close this stream?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440", "body": "nit: bad alignment\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450", "body": "ql dependency?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466", "body": "Can this be moved to common instead?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488", "body": "Never mind. This is already part of storage-api. So not a problem.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "joshelser": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513", "body": "Thanks for submitting a PR for this, @chutium. I left you some comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867", "body": "Isn't the `RecordWriter<Text, Mutation>` duplicative since `AccumuloRecordWriter` already implements that?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048", "body": "This seems wrong. For the AccumuloRecordWriter, the `Text` can signify an Accumulo table to which this update should be written. Where is this `Writable` coming from?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "chutium": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846", "body": "thanks for reviewing, to reproduce the issue, you can create a spark context with following jars:\naccumulo-core-1.6.0.jar\naccumulo-fate-1.6.0.jar\naccumulo-start-1.6.0.jar\naccumulo-trace-1.6.0.jar\nhive-accumulo-handler-1.2.1.jar\n\nthen run some hive query like:\n\n```\nset accumulo.instance.name=instance_name;\nset accumulo.zookeepers=zk_host:2181;\nset accumulo.user.name=user;\nset accumulo.user.pass=pass;\n\nCREATE TABLE testtable(rowid STRING, value STRING)\nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,colf:colq');\n\nINSERT OVERWRITE TABLE testtable SELECT col1, col2 FROM some_hive_table;\n```\n\nthen will get error `java.lang.ClassCastException` says `HiveAccumuloTableOutputFormat cannot be cast to org.apache.hadoop.hive.ql.io.HiveOutputFormat`\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969", "body": "yes... seems like RecordWriter is unnecessary, will check this soon\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651", "body": "noticed and wondering as well, but it works in our trials... the mutations will be written into the target table, that defined in hive query -_- my assume is the table name is already set in JobConf, so we do not need to specify table name for each write operation.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832", "body": "I might pick this up at a later date.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jiangxt2": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183", "body": "Is it my fault? How can i fix it? thx my bro. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475", "body": "@dmtolpeko Hi, I created a JIRA for this problem.\nThe url is https://issues.apache.org/jira/browse/HIVE-13877\nWhat should I do next?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588", "body": "@dmtolpeko OK, I GOT IT. Thanks very much. I'll wait for your response.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dmtolpeko": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191", "body": "Can you create a Hive JIRA for this problem? Github is just a mirror\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901", "body": "I will review your changes and create a patch. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "dark0dave": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930", "body": "named travis file wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "yoni": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/2375698", "body": "This change breaks development on OS X. Not sure if anyone else is having the same issues I am, but I can't seem to work around this. `git status` constantly reports changes have been made:\n\n```\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n$ git reset --hard\nHEAD is now at af0162d HIVE-446 Implement TRUNCATE\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/2375698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sameeragarwal": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/3868174", "body": "Shouldn't this be: \n\n``` java\nmyagg.variance += varianceFieldOI.get(partialVariance)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/3868174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "ksumit": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/5973358", "body": "shouldn't this be 0.13.0 for this branch? i tried compiling the current tip, it's broken and changing it to 0.13.0 works just fine.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/5973358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "HZMengYue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/8795892", "body": "hi,i checkout a spark branch today,and after built it,derby-10.11.1.1.jar in hive dir lib,but when i run hive shell ,like  ./hive --auxpath /home/hispark/spark-1.2.0/assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar,it give me follow error:and i think u had fixed this problem.\nLogging initialized using configuration in file:/home/hispark/apache-hive-0.15.0-SNAPSHOT-bin/conf/hive-log4j.properties\nException in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:449)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:634)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:578)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1481)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2674)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2693)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:430)\n        ... 7 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1479)\n        ... 12 more\nCaused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory\nNestedThrowables:\njava.lang.reflect.InvocationTargetException\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n        at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:341)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:370)\n        at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:267)\n        at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:234)\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:572)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:550)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:603)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:441)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5598)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:182)\n        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)\n        ... 17 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)\n        at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)\n        at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)\n        at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n        at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n        at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n        ... 46 more\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccess\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/8795892/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sershe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/9465637", "body": "should be gone\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/9465752", "body": "this is wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465752/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jalajthanaki": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/11626965", "body": "I want to ask you ,Is there full text index kind of functionality available in Apache Hive for searching, matching & ranking keywords of the text?\n\nIf any functionality will be there then let me know.\n\nThanks,\nJalaj T\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/11626965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "merlin-zaraza": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14273837", "body": "Ok, initialize method is deprecated. How to use GenericUDTF now? (e.g. check for arguments)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14273837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "walla2sl": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14312112", "body": "I'm having the same issue. How is GenericUDTF usable now? After trying to use a UTDF I created, I keep getting \"Error while compiling statement: FAILED: IllegalStateException Should not be called directly\". \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/14312367", "body": "I opened https://issues.apache.org/jira/browse/HIVE-12377\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "phoenixhadoop": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15241484", "body": "I am using hive-2.0.0-SNAPSHOT\nwhen starting metastore I meet the ERROR: ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\n\nwhen  I configure :export HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \",the ERROR disappear.\n\nHow can I do?\n\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15241484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/15244734", "body": "Hi,prasanthj \nNow we use hive-2.0.0-SNAPSHOT,we meet an error when starting the service metastore .\nThe error is :ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\nIf we configure the variable HADOOP_CLIENT_OPTS  as\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \" ,the error disappear,but entering hive cli slower.\nPlease help me how can avoid the ERROR.\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15244734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "prongs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17350037", "body": "This is deleting `target/tmp/`causing all subsequent tests to fail. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17350037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363722", "body": "All of them!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363749", "body": "Check this: https://github.com/apache/hive/blob/master/pom.xml#L914. Then this: https://github.com/apache/hive/blob/master/pom.xml#L991\n\nFirst one sets up target/tmp/conf and second one is adding that to classpath. If the directory gets deleted, subsequent tests that create a `new HiveConf()` don't get to read `target/tmp/conf/hive-site.xml` and there are some test specific properties there. You can't just delete the directory and recreate. This should have been caught in review. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "goyalr41": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17575099", "body": "Hi,\n\nJust wanted to know how to access this function now? Should I flatten the map returned by new function getMapRedStats().\n\nThanks,\nRaman\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17575099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "djhwx": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20446807", "body": "The build is failing in tests. Can you please revert this back to unblock others and check this in which fixed?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20446807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nguyenhoan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20889577", "body": "Hi @rbalamohan \r\nWe are a team of researchers from Iowa State, The University of Texs at Dallas and Oregon State University, USA. We are investigating common/repeated code changes.\r\nWe have four short questions regarding the change to this statement of the commit.\r\n\r\nQuestions:\r\n\r\nQ1- Is the change at these lines similar to another change from before? (yes, no, not sure)\r\n\r\nQ2- Can you briefly describe the change and why you made it? (for example, checking parameter before calling the method to avoid a Null Pointer Exception)\r\n\r\nQ3- Can you give it a name? (for example, Null Check)\r\n\r\nQ4- Would you like to have this change automated by a tool? (Yes, No, Already automated)\r\n\r\nThe data collected from the answers will never be associated with you or your project. Our questions are about recurring code changes from the developer community, not about personal information. All the data is merged across recurring changes from GitHub repositories. We will publish aggregated data from the trends of the whole community. \r\nWe have a long tradition of developing refactoring tools and contributing them freely to the Eclipse, Netbeans, Android Studio under their respective FLOSS licenses. For example, look at some of our recently released refactoring tools: http://refactoring.info/tools/ \r\n\r\nThank you,\r\nHoan Nguyen https://sites.google.com/site/nguyenanhhoan/\r\nMichael Hilton http://web.engr.oregonstate.edu/~hiltonm/\r\nTien Nguyen http://www.utdallas.edu/~tien.n.nguyen/\r\nDanny Dig http://eecs.oregonstate.edu/people/dig-danny\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20889577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "kiddingbaby": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/26133483", "body": "The property **hive.server.read.socket.timeout** not found in Wiki: Hive Configuration, can I use it in hive v1.2.1?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/26133483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}}, "3": {"ekoifman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/727581d749306d5df3b6ab8c4adb21ef0fe31506", "message": "HIVE-18460 - Compactor doesn't pass Table properties to the Orc writer (Eugene Koifman, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/dfd7ea368dc8ea2ef071543c3d0cea9e05cd115a", "message": "HIVE-18419 - CliDriver loads different hive-site.xml into HiveConf and MetastoreConf (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fe3190d19fa1324a6835692e71f747e6cb37c7d1", "message": "HIVE-18429 - Compaction should handle a case when it produces no output (Eugene Koifman, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vihangk1": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/ef9d3ee97cee30725381013f4051790c432aa726", "message": "HIVE-18323 : Vectorization: add the support of timestamp in VectorizedPrimitiveColumnReader for parquet (Vihang Karajgaonkar, reviewed by Aihua Xu and Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/798a17c679fc04e5a20b15675b6883f52a26ea57", "message": "HIVE-18461 : Fix precommit hive job (Vihang Karajgaonkar)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/287", "title": "HIVE-17580 : Remove standalone-metastore's dependency with serdes", "body": "Removing the dependency on serdes for the metastore requires a series of changes. I have created multiple commits which hopefully would be easier to review. Each major commit has a descriptive commit message to give a high level idea of what the change is doing. There are still some bits which need to be completed but it would be good to a review.\r\n\r\nOverview of all the changes done:\r\n1. Creates a new module called serde-api under storage-api like discussed. Although I think we can keep it separate as well.\r\n2. Moved List, Map, Struct, Constant, Primitive, Union ObjectInspectors to serde-api\r\n3. Moved PrimitiveTypeInfo, PrimitiveTypeEntry and TypeInfo to serde-api.\r\n4. Moved TypeInfoParser, TypeInfoFactory to serde-api\r\n5. Added a new class which reading avro storage schema by copying the code from AvroSerde and AvroSerdeUtils. The parsing is done such that String value is first converted into TypeInfos and then into FieldSchemas bypassing the need for ObjectInspectors. In theory we could get rid of TypeInfos as well but that path was getting too difficult with lot of duplicate code between Hive and metastore.\r\n6. Introduces a default storage schema reader. I noticed that most of the serdes use the same logic to parse the metadata information. This code should be refactored to a common place instead of having many copies (one in standalone hms and another set in multiple serdes)\r\n7. Moved HiveChar, HiveVarchar, HiveCharWritable, HiveVarcharWritable to storage-api. I noticed that HiveDecimal is already in storage-api. It probably makes sense to move the other primitive types (timestamp, interval etc)to storage-api as well but it requires storage-api to be upgraded to Java 8.\r\n8. Adds a basic test for the schema reader. I plan to add more tests as this code is reviewed.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcamachor": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/80e6f7b0f13c134642763b0a692c3abbf3ffe106", "message": "HIVE-18386 : Create dummy materialized views registry and make it configurable (Jesus Camacho Rodriguez via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/a45becb1602573cae32673e32161b1a3aa10a86b", "message": "HIVE-18473: Infer timezone information correctly in DruidSerde (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ac247817f441eba5daf5b07cc831347d1c762ee4", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum II)\n\n* Fix dangling tests"}, {"url": "https://api.github.com/repos/apache/hive/commits/b1cdbc60d6ef856b852afe1aa44bd3520d5fb84a", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum)"}, {"url": "https://api.github.com/repos/apache/hive/commits/7e64114ddca5c07a0c4ac332c1b34b534cc2e9ed", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdere": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/01816fca2c2620f4f57a2b2a5c6ea404e1f0038f", "message": "HIVE-18430: Add new determinism category for runtime constants (current_date, current_timestamp) (Jason Dere, reviewed by Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/790976907070a2a737186543d81c71eb542a5337", "message": "HIVE-18385: mergejoin fails with java.lang.IllegalStateException (Jason Dere, reviewed by Deepak Jaiswal)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vaibhavgumashta": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/456a65180dcb84f69f26b4c9b9265165ad16dfe4", "message": "HIVE-17495: CachedStore: prewarm improvement (avoid multiple sql calls to read partition column stats), refactoring and caching some aggregate stats (Vaibhav Gumashta reviewed by Daniel Dai)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313", "body": "Looks good, +1.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083", "body": "+1. Looks good.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "pvary": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/fbb3ed15fd0f078369c1fcb2edd68326d272758a", "message": "HIVE-18372: Create testing infra to test different HMS instances (Peter Vary, reviewed by Marta Kuczora, Vihang Karajgaonkar and Adam Szita)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6938fcabf0944817fed8241c48b57fb3f5d98c69", "message": "HIVE-18443: Ensure git gc finished in ptest prep phase before copying repo (Adam Szita, via Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b826072edb429214e9fe073dea9381449396f05d", "message": "HIVE-18355: Add builder for metastore Thrift classes missed in the first pass - FunctionBuilder (Peter Vary, reviewed by Alan Gates)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "winningsix": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/17abdb211c1b2b749fc7d8265d31e6c5987cea4b", "message": "HIVE-18411: Fix ArrayIndexOutOfBoundsException for VectorizedListColumnReader (Colin Ma, reviewed by Ferdinand Xu)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/251", "title": "HIVE-14836: Test the predicate pushing down support for Parquet vecto\u2026", "body": "\u2026rization read path\r\n\r\nAdd more unit test for Predicate pushing down for Parquet Vectorization.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380", "body": "Thanks @thejasmn for working on it. Just some minor issues. @sundapeng Can you take a look at this PR? Thank you!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "deepeshk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/7942bc6c9a38e71ff3f8936508cc773a464b0d87", "message": "HIVE-18465: Hive metastore schema initialization failing on postgres (Deepesh Khandelwal, reviewed by Jesus Camacho Rodriguez)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sershe-apache": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/78d5572f8e165a907c63e1ef8579f591b8c34563", "message": "HIVE-18452 : work around HADOOP-15171 (Sergey Shelukhin, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/72684f10da8f124dfa624b16f3ffebeb2155664d", "message": "HIVE-18273 : add LLAP-level counters for WM (Sergey Shelukhin, reviewed by Harish Jaiprakash and Gunther Hagleitner)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fde503dcaa1f3e322eb4dc34bfffbec89aefd240", "message": "HIVE-18437 : use plan parallelism for the default pool if both are present (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alanfgates": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/d9801d9c6c406d5871147b80bc2e0359c3dbd085", "message": "HIVE-17982 Move metastore specific itests.  This closes #279.  (Alan Gates, reviewed by Peter Vary)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/291", "title": "HIVE-17983 Make the standalone metastore generate tarballs etc.", "body": "See JIRA for full comments.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sahilTakiar": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/87860fbca42b2477f504c8b1cefd43c865a2629c", "message": "HIVE-18214: Flaky test: TestSparkClient (Sahil Takiar, reviewed by Peter Vary)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kgyrtkirk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/2402f3ca55e3f304e041ff53591f846634fbc9ac", "message": "HIVE-18149 HIVE-18416: addendum"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "k0b3rIT": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/13c1bf9e903c502dbfe3f2fb10959ac1a64bc0ad", "message": "HIVE-18161: Remove hive.stats.atomic (Bertalan Kondrat via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "abstractdog": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/5d4559bbdb5fb36450b67841d0da4fa2fecdeeea", "message": "HIVE-18309: qtests: smb_mapjoin_19.q breaks bucketsortoptimize_insert_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/19c2b7225e1c072cba5c1a93329f70059e7e063d", "message": "HIVE-18314: qtests: semijoin_hint.q breaks hybridgrace_hashjoin_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anishek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/22df53b6c223f03edee1a8c271a77e91d66bb2a1", "message": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys (Anishek Agarwal, reviewed Daniel Dai)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/289", "title": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/286", "title": "HIVE-18352: introduce a METADATAONLY option while doing REPL DUMP to allow integrations of other tools", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/283", "title": "HIVE-17829: ArrayIndexOutOfBoundsException - HBASE-backed tables with Avro schema in Hive2", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/263", "title": "HIVE-17830: dbnotification fails to work with rdbms other than postgres", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/262", "title": "HIVE-17825: Socket not closed when trying to read files to copy over in replication from metadata", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/259", "title": "HIVE-17615 : Task.executeTask has to be thread safe for parallel execution", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/246", "title": "HIVE-17426: Execution framework in hive to run tasks in parallel other than MR Tasks", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/240", "title": "HIVE-17410 : repl load task during subsequent DAG generation does notstart from the last partition processed", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/237", "title": "HIVE-16886: HMS log notifications may have duplicated event IDs if multiple HMS are running concurrently", "body": "\u2026ltiple HMS are running concurrently", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daijyc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/970d8c9680d89708c4007de214fdc72b77deb7da", "message": "HIVE-18299: DbNotificationListener fail on mysql with \"select for update\" (Daniel Dai, reviewed by Anishek Agarwal)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c0734ac91bdd3e81b17d93fcfeddd4503430eea8", "message": "HIVE-18298: Fix TestReplicationScenarios.testConstraints (Daniel Dai, reviewed by Sankar Hariappan)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/236", "title": "HIVE-17366: Constraint replication in bootstrap", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dosoft": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/292", "title": "HIVE-17331: Use Path instead of String as key type of the pathToAliases", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/234", "title": "HIVE-17314: Removed obsolete code", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/230", "title": "HIVE-17313: Fixed 'case fall-through'", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/229", "title": "HIVE-17311: Fixed numeric overflow, added test", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sankarh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/290", "title": "HIVE-18192: Introduce WriteID per table rather than using global transaction ID", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/280", "title": "HIVE-18031: Support replication for Alter Database operation", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "msydoron": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/288", "title": "HIVE-18423", "body": "Added full support for jdbc external tables in hive.\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/285", "title": "HIVE-16480 (ORC-285) Empty vector batches of floats or doubles gets", "body": "EOFException.\r\n\r\nSigned-off-by: Owen O'Malley <omalley@apache.org>", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738", "body": "This has been committed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17354974", "body": "But it recreates target/tmp in the next line. Which test assumes that files are pre-located there?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17354974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005", "body": "I did port all of the TestOrcFile to the vectorized writer in the new file TestVectorOrcFile. Does that address your concerns?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685", "body": "This patch doesn't actually change the write path. I've done this in HIVE-12055 including adding a new writer version.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185", "body": "The CompressionCodec isn't really an API class, since the user isn't able to add their own implementations (other than the external LZO implementation). I guess I can move it over, if you think it is important. In general LLAP is accessing ORC beneath the API level.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588", "body": "The DirectDecompressor API was added in Hadoop 2.3. I'd rather go ahead and make it a real shim so that we can support older Hadoop versions.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527", "body": "I'll add a comment, but it does clone the FileHandle so that they can be closed separately.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369", "body": "Ok, I've updated both Char and Varchar with the check.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229", "body": "For now we should stick with what Hive supports. I copied this code directly from Hive's HadoopShims.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104", "body": "I'm not sure what you think is misaligned, but I simplified this code.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "amrk7s": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/284", "title": "HIVE-18338 Exposing asynchronous execution through hive-jdbc client", "body": "**Problem statement**\r\n\r\nHive JDBC currently exposes 2 methods related to asynchronous execution\r\n**executeAsync()** - to trigger a query execution and return immediately.\r\n**waitForOperationToComplete()** - which waits till the current execution is complete **blocking the user thread**. \r\n\r\nThis has one problem\r\n- If the client process goes down, there is no way to resume queries although hive server is completely asynchronous. \r\n\r\n**Proposal**\r\n\r\nIf operation handle could be exposed, we can latch on to an active execution of a query.\r\n\r\n**Code changes**\r\n\r\nOperation handle is exposed. So client can keep a copy.\r\nlatchSync() and latchAsync() methods take an operation handle and try to latch on to the current execution in hive server if present", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mattk42": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/278", "title": "HIVE-18295 - Add ability to ignore invalid values in JSON SerDe ", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ymwdalex": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/273", "title": "HIVE-18130: Update table path to storage parameters when alter a table", "body": "When an managed table is created by Spark, table path information is not only store in `location` field (first figure), but also in `parameters.path` fields (second figure).\r\n<img width=\"445\" alt=\"location\" src=\"https://user-images.githubusercontent.com/1458656/33123050-466c6bdc-cf79-11e7-8b53-85ad7dc83f17.png\">\r\n<img width=\"677\" alt=\"storage_parameter\" src=\"https://user-images.githubusercontent.com/1458656/33123051-469576da-cf79-11e7-9c6c-9793a28f8389.png\">\r\n\r\nWhen hive alter a table, `storage parameter` is ignored. Then, spark cannot access the table anymore because spark use the parameter.\r\n\r\nIn this PR, when altering a table, the field `storageDescription.parameters.path` is also updated, if `path` key exists\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hejian991": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/269", "title": "fix small bug: when disable partition statistics in hive-site.xml, bu\u2026", "body": "\u2026t it does not work.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ishitbatra": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/268", "title": "hive installation link", "body": "hive link given", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thejasmn": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/266", "title": "Hive-17897", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268", "body": "addressing the comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518", "body": "Updated patch addresses additional comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "kanna14243": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/265", "title": "Update TBinarySortableProtocol.java", "body": "writeTextBytes doesn't respect the start parameter. It'll work only for cases where start = 0. Fixing it so that it'll work for any value of start.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "b-slim": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/249", "title": "[HIVE-17523] Fix insert into bug", "body": "https://issues.apache.org/jira/browse/HIVE-17523", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmartin-rp": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/232", "title": "Update 039-HIVE-12274.oracle.sql", "body": "You cannot modify a column from VARCHAR2(4000) to CLOB directly. You need to add a new column and drop the old one or recreate the table. I chose to recreate the table because it's more clean.\r\n\r\nhttps://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:1770086700346491686\r\n\r\nAlso, if you do something like:\r\nALTER TABLE COLUMNS_V2 MODIFY (COLUMN_NAME VARCHAR(767) NOT NULL);\r\nAnd the column is already NOT NULL, the ALTER TABLE fails.  I removed 2 like this.\r\n\r\nI hope it helps.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jarcec": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868", "body": "Hi Viraj,\nunfortunately we can't accept pull requests due to licensing issues. You need to attach your patch to the JIRA in order to enable Hive community to get it in.\n\nJarcec\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "brockn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540", "body": "See https://issues.apache.org/jira/browse/HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pavel-sakun": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553", "body": "Closing this one in favour of HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "codingtony": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235", "body": "This is the fix  release 0.13.1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "slavag-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547", "body": "please do not merge for now till we can get a release out.  thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "srowen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047", "body": "PS I found this from your cross-posted question about Kendall coefficient on Hive. What is this? PRs aren't for questions and this is some arbitrary branch of commits you've opened.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "rdblue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346", "body": "Hi @dguy, I've been working on this problem recently in [HIVE-8909](https://issues.apache.org/jira/browse/HIVE-8909). That's already been merged, but I'm wondering if there's a case that we've missed that you fix here?\n\nIt looks like this allows the object inspector to work with the extra level of nesting that is included by the deserializer or detect if it isn't there. For HIVE-8909, we opted to keep the extra level of nesting just so we wouldn't need to change the object inspector code. We will need to follow up with a commit that removes the added layer from both. Did you come across a case where the added layer isn't there?\n\nAlso, I don't think Hive uses pull requests, so the best way to follow up is to open an issue in the JIRA issue tracker: https://issues.apache.org/jira/browse/HIVE\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972", "body": "Hi Damian, thanks for following up and letting us know it's working!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dguy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937", "body": "Hi Ryan,\n\nThanks for getting in touch. We are writing avro/parquet data-sets from\nvarious Map/Reduce jobs - we then create external hive tables for these\ndata-sets. The issue we are seeing is that AvroSchemaConverter\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/main/java/parquet/avro/AvroSchemaConverter.java\nconverts avro array types like this:\n\n{\n     \"name\" : \"myarray\",\n     \"type\" : {\n       \"type\" : \"array\",\n       \"items\" : \"int\"\n }\n\ninto Parquet:\nrequired group myarray (LIST) {\n       repeated int32 array;\n }\n\nYou can see an example here\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/test/java/parquet/avro/TestAvroSchemaConverter.java\n\nWhere as hive is expecting something like:\nrequired group myarray  {\n       repeated group bag {\n          optional int32 array;\n       }\n }\n\nWithout this change our external tables with array types currently won't\ndeserialize. So my patch was an attempt to make it work for both cases i\nknow about. I'll have to have a look through your patch and see if it\nhelps.\n\nThanks for the pointer.\nCheers,\nDamian\n\nOn 10 December 2014 at 01:50, Ryan Blue notifications@github.com wrote:\n\n> Hi @dguy https://github.com/dguy, I've been working on this problem\n> recently in [HIVE-8909|https://issues.apache.org/jira/browse/HIVE-8909].\n> That's already been merged, but I'm wondering if there's a case that we've\n> missed that you fix here?\n> \n> It looks like this allows the object inspector to work with the extra\n> level of nesting that is included by the deserializer or detect if it isn't\n> there. For HIVE-8909, we opted to keep the extra level of nesting just so\n> we wouldn't need to change the object inspector code. We will need to\n> follow up with a commit that removes the added layer from both. Did you\n> come across a case where the added layer isn't there?\n> \n> Also, I don't think Hive uses pull requests, so the best way to follow up\n> is to open an issue in the JIRA issue tracker:\n> https://issues.apache.org/jira/browse/HIVE\n> \n> Thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hive/pull/26#issuecomment-66392346.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730", "body": "Hi,\nI've verified it works fine with your fix. Thanks for pointing me to this.\nCheers,\nDamian\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "swarnim87": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308", "body": "Finish javadoc\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "apivovarov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "zhichao-li": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233", "body": "cc @apivovarov \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Lewuathe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402", "body": "Sorry for mistake.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "prasanthj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388", "body": "@kirill-vlasov Could you please create a JIRA here https://issues.apache.org/jira/browse/HIVE/ and add the JIRA id in the title of the PR?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15379454", "body": "@phoenixhadoop hive trunk is updated with new hive-log4j2.properties based configuration. If log4j.configurationFile is not explicitly set then by default hive-log4j2.properties file will be looked up in CLASSPATH. Typically, HIVE_CONF_DIR will be set to conf directory which contains hive-log4j2.properties file. HIVE_CONF_DIR is automatically added to CLASSPATH by the hive script. Are you still seeing this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15379454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629", "body": "Would it be easier to support bulk writes? As isPresent is called for every column.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414", "body": "Actually never mind, since the output stream is buffered. This won't be big win.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051", "body": "What happens if the vec is null and started is false? A run can start with null structs (struct column) right?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177", "body": "Again ignore this :) I guess that case is handled by super(). Correct me if I am wrong.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540", "body": "It will be good to add tests that uses WriterOptions.setSchema() to specify the types instead of OI. I see that you have fixed bunch of OI casts that would have cause NPE otherwise (if TypeDescription is set through writer).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842", "body": "Do we need to bump up the writer version? To differentiate ORC files written with row-by-row vs vector writer?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511", "body": "Yes. Sorry I missed that file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544", "body": "Can we put all the interfaces in api package?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318", "body": "This isn't a shim right? Can this be moved over to OrcUtils?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625", "body": "Does it clone the options/properties alone? or the fs input stream as well? Can you please add a comment as in what does it clone and what not.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043", "body": "Should we check if it has hasMaximumLength()?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417", "body": "Is there a minimum supported hadoop version for orc? If we don't support older hadoop version then this shim might not be required.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431", "body": "Where do we close this stream?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440", "body": "nit: bad alignment\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450", "body": "ql dependency?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466", "body": "Can this be moved to common instead?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488", "body": "Never mind. This is already part of storage-api. So not a problem.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "joshelser": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513", "body": "Thanks for submitting a PR for this, @chutium. I left you some comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867", "body": "Isn't the `RecordWriter<Text, Mutation>` duplicative since `AccumuloRecordWriter` already implements that?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048", "body": "This seems wrong. For the AccumuloRecordWriter, the `Text` can signify an Accumulo table to which this update should be written. Where is this `Writable` coming from?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "chutium": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846", "body": "thanks for reviewing, to reproduce the issue, you can create a spark context with following jars:\naccumulo-core-1.6.0.jar\naccumulo-fate-1.6.0.jar\naccumulo-start-1.6.0.jar\naccumulo-trace-1.6.0.jar\nhive-accumulo-handler-1.2.1.jar\n\nthen run some hive query like:\n\n```\nset accumulo.instance.name=instance_name;\nset accumulo.zookeepers=zk_host:2181;\nset accumulo.user.name=user;\nset accumulo.user.pass=pass;\n\nCREATE TABLE testtable(rowid STRING, value STRING)\nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,colf:colq');\n\nINSERT OVERWRITE TABLE testtable SELECT col1, col2 FROM some_hive_table;\n```\n\nthen will get error `java.lang.ClassCastException` says `HiveAccumuloTableOutputFormat cannot be cast to org.apache.hadoop.hive.ql.io.HiveOutputFormat`\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969", "body": "yes... seems like RecordWriter is unnecessary, will check this soon\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651", "body": "noticed and wondering as well, but it works in our trials... the mutations will be written into the target table, that defined in hive query -_- my assume is the table name is already set in JobConf, so we do not need to specify table name for each write operation.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832", "body": "I might pick this up at a later date.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jiangxt2": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183", "body": "Is it my fault? How can i fix it? thx my bro. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475", "body": "@dmtolpeko Hi, I created a JIRA for this problem.\nThe url is https://issues.apache.org/jira/browse/HIVE-13877\nWhat should I do next?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588", "body": "@dmtolpeko OK, I GOT IT. Thanks very much. I'll wait for your response.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dmtolpeko": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191", "body": "Can you create a Hive JIRA for this problem? Github is just a mirror\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901", "body": "I will review your changes and create a patch. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "dark0dave": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930", "body": "named travis file wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "yoni": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/2375698", "body": "This change breaks development on OS X. Not sure if anyone else is having the same issues I am, but I can't seem to work around this. `git status` constantly reports changes have been made:\n\n```\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n$ git reset --hard\nHEAD is now at af0162d HIVE-446 Implement TRUNCATE\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/2375698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sameeragarwal": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/3868174", "body": "Shouldn't this be: \n\n``` java\nmyagg.variance += varianceFieldOI.get(partialVariance)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/3868174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "ksumit": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/5973358", "body": "shouldn't this be 0.13.0 for this branch? i tried compiling the current tip, it's broken and changing it to 0.13.0 works just fine.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/5973358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "HZMengYue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/8795892", "body": "hi,i checkout a spark branch today,and after built it,derby-10.11.1.1.jar in hive dir lib,but when i run hive shell ,like  ./hive --auxpath /home/hispark/spark-1.2.0/assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar,it give me follow error:and i think u had fixed this problem.\nLogging initialized using configuration in file:/home/hispark/apache-hive-0.15.0-SNAPSHOT-bin/conf/hive-log4j.properties\nException in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:449)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:634)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:578)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1481)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2674)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2693)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:430)\n        ... 7 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1479)\n        ... 12 more\nCaused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory\nNestedThrowables:\njava.lang.reflect.InvocationTargetException\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n        at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:341)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:370)\n        at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:267)\n        at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:234)\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:572)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:550)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:603)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:441)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5598)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:182)\n        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)\n        ... 17 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)\n        at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)\n        at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)\n        at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n        at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n        at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n        ... 46 more\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccess\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/8795892/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sershe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/9465637", "body": "should be gone\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/9465752", "body": "this is wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465752/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jalajthanaki": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/11626965", "body": "I want to ask you ,Is there full text index kind of functionality available in Apache Hive for searching, matching & ranking keywords of the text?\n\nIf any functionality will be there then let me know.\n\nThanks,\nJalaj T\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/11626965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "merlin-zaraza": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14273837", "body": "Ok, initialize method is deprecated. How to use GenericUDTF now? (e.g. check for arguments)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14273837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "walla2sl": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14312112", "body": "I'm having the same issue. How is GenericUDTF usable now? After trying to use a UTDF I created, I keep getting \"Error while compiling statement: FAILED: IllegalStateException Should not be called directly\". \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/14312367", "body": "I opened https://issues.apache.org/jira/browse/HIVE-12377\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "phoenixhadoop": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15241484", "body": "I am using hive-2.0.0-SNAPSHOT\nwhen starting metastore I meet the ERROR: ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\n\nwhen  I configure :export HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \",the ERROR disappear.\n\nHow can I do?\n\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15241484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/15244734", "body": "Hi,prasanthj \nNow we use hive-2.0.0-SNAPSHOT,we meet an error when starting the service metastore .\nThe error is :ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\nIf we configure the variable HADOOP_CLIENT_OPTS  as\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \" ,the error disappear,but entering hive cli slower.\nPlease help me how can avoid the ERROR.\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15244734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "prongs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17350037", "body": "This is deleting `target/tmp/`causing all subsequent tests to fail. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17350037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363722", "body": "All of them!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363749", "body": "Check this: https://github.com/apache/hive/blob/master/pom.xml#L914. Then this: https://github.com/apache/hive/blob/master/pom.xml#L991\n\nFirst one sets up target/tmp/conf and second one is adding that to classpath. If the directory gets deleted, subsequent tests that create a `new HiveConf()` don't get to read `target/tmp/conf/hive-site.xml` and there are some test specific properties there. You can't just delete the directory and recreate. This should have been caught in review. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "goyalr41": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17575099", "body": "Hi,\n\nJust wanted to know how to access this function now? Should I flatten the map returned by new function getMapRedStats().\n\nThanks,\nRaman\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17575099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "djhwx": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20446807", "body": "The build is failing in tests. Can you please revert this back to unblock others and check this in which fixed?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20446807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nguyenhoan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20889577", "body": "Hi @rbalamohan \r\nWe are a team of researchers from Iowa State, The University of Texs at Dallas and Oregon State University, USA. We are investigating common/repeated code changes.\r\nWe have four short questions regarding the change to this statement of the commit.\r\n\r\nQuestions:\r\n\r\nQ1- Is the change at these lines similar to another change from before? (yes, no, not sure)\r\n\r\nQ2- Can you briefly describe the change and why you made it? (for example, checking parameter before calling the method to avoid a Null Pointer Exception)\r\n\r\nQ3- Can you give it a name? (for example, Null Check)\r\n\r\nQ4- Would you like to have this change automated by a tool? (Yes, No, Already automated)\r\n\r\nThe data collected from the answers will never be associated with you or your project. Our questions are about recurring code changes from the developer community, not about personal information. All the data is merged across recurring changes from GitHub repositories. We will publish aggregated data from the trends of the whole community. \r\nWe have a long tradition of developing refactoring tools and contributing them freely to the Eclipse, Netbeans, Android Studio under their respective FLOSS licenses. For example, look at some of our recently released refactoring tools: http://refactoring.info/tools/ \r\n\r\nThank you,\r\nHoan Nguyen https://sites.google.com/site/nguyenanhhoan/\r\nMichael Hilton http://web.engr.oregonstate.edu/~hiltonm/\r\nTien Nguyen http://www.utdallas.edu/~tien.n.nguyen/\r\nDanny Dig http://eecs.oregonstate.edu/people/dig-danny\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20889577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "kiddingbaby": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/26133483", "body": "The property **hive.server.read.socket.timeout** not found in Wiki: Hive Configuration, can I use it in hive v1.2.1?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/26133483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}}, "4": {"ekoifman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/727581d749306d5df3b6ab8c4adb21ef0fe31506", "message": "HIVE-18460 - Compactor doesn't pass Table properties to the Orc writer (Eugene Koifman, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/dfd7ea368dc8ea2ef071543c3d0cea9e05cd115a", "message": "HIVE-18419 - CliDriver loads different hive-site.xml into HiveConf and MetastoreConf (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fe3190d19fa1324a6835692e71f747e6cb37c7d1", "message": "HIVE-18429 - Compaction should handle a case when it produces no output (Eugene Koifman, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vihangk1": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/ef9d3ee97cee30725381013f4051790c432aa726", "message": "HIVE-18323 : Vectorization: add the support of timestamp in VectorizedPrimitiveColumnReader for parquet (Vihang Karajgaonkar, reviewed by Aihua Xu and Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/798a17c679fc04e5a20b15675b6883f52a26ea57", "message": "HIVE-18461 : Fix precommit hive job (Vihang Karajgaonkar)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/287", "title": "HIVE-17580 : Remove standalone-metastore's dependency with serdes", "body": "Removing the dependency on serdes for the metastore requires a series of changes. I have created multiple commits which hopefully would be easier to review. Each major commit has a descriptive commit message to give a high level idea of what the change is doing. There are still some bits which need to be completed but it would be good to a review.\r\n\r\nOverview of all the changes done:\r\n1. Creates a new module called serde-api under storage-api like discussed. Although I think we can keep it separate as well.\r\n2. Moved List, Map, Struct, Constant, Primitive, Union ObjectInspectors to serde-api\r\n3. Moved PrimitiveTypeInfo, PrimitiveTypeEntry and TypeInfo to serde-api.\r\n4. Moved TypeInfoParser, TypeInfoFactory to serde-api\r\n5. Added a new class which reading avro storage schema by copying the code from AvroSerde and AvroSerdeUtils. The parsing is done such that String value is first converted into TypeInfos and then into FieldSchemas bypassing the need for ObjectInspectors. In theory we could get rid of TypeInfos as well but that path was getting too difficult with lot of duplicate code between Hive and metastore.\r\n6. Introduces a default storage schema reader. I noticed that most of the serdes use the same logic to parse the metadata information. This code should be refactored to a common place instead of having many copies (one in standalone hms and another set in multiple serdes)\r\n7. Moved HiveChar, HiveVarchar, HiveCharWritable, HiveVarcharWritable to storage-api. I noticed that HiveDecimal is already in storage-api. It probably makes sense to move the other primitive types (timestamp, interval etc)to storage-api as well but it requires storage-api to be upgraded to Java 8.\r\n8. Adds a basic test for the schema reader. I plan to add more tests as this code is reviewed.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcamachor": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/80e6f7b0f13c134642763b0a692c3abbf3ffe106", "message": "HIVE-18386 : Create dummy materialized views registry and make it configurable (Jesus Camacho Rodriguez via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/a45becb1602573cae32673e32161b1a3aa10a86b", "message": "HIVE-18473: Infer timezone information correctly in DruidSerde (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ac247817f441eba5daf5b07cc831347d1c762ee4", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum II)\n\n* Fix dangling tests"}, {"url": "https://api.github.com/repos/apache/hive/commits/b1cdbc60d6ef856b852afe1aa44bd3520d5fb84a", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum)"}, {"url": "https://api.github.com/repos/apache/hive/commits/7e64114ddca5c07a0c4ac332c1b34b534cc2e9ed", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdere": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/01816fca2c2620f4f57a2b2a5c6ea404e1f0038f", "message": "HIVE-18430: Add new determinism category for runtime constants (current_date, current_timestamp) (Jason Dere, reviewed by Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/790976907070a2a737186543d81c71eb542a5337", "message": "HIVE-18385: mergejoin fails with java.lang.IllegalStateException (Jason Dere, reviewed by Deepak Jaiswal)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vaibhavgumashta": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/456a65180dcb84f69f26b4c9b9265165ad16dfe4", "message": "HIVE-17495: CachedStore: prewarm improvement (avoid multiple sql calls to read partition column stats), refactoring and caching some aggregate stats (Vaibhav Gumashta reviewed by Daniel Dai)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313", "body": "Looks good, +1.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083", "body": "+1. Looks good.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "pvary": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/fbb3ed15fd0f078369c1fcb2edd68326d272758a", "message": "HIVE-18372: Create testing infra to test different HMS instances (Peter Vary, reviewed by Marta Kuczora, Vihang Karajgaonkar and Adam Szita)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6938fcabf0944817fed8241c48b57fb3f5d98c69", "message": "HIVE-18443: Ensure git gc finished in ptest prep phase before copying repo (Adam Szita, via Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b826072edb429214e9fe073dea9381449396f05d", "message": "HIVE-18355: Add builder for metastore Thrift classes missed in the first pass - FunctionBuilder (Peter Vary, reviewed by Alan Gates)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "winningsix": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/17abdb211c1b2b749fc7d8265d31e6c5987cea4b", "message": "HIVE-18411: Fix ArrayIndexOutOfBoundsException for VectorizedListColumnReader (Colin Ma, reviewed by Ferdinand Xu)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/251", "title": "HIVE-14836: Test the predicate pushing down support for Parquet vecto\u2026", "body": "\u2026rization read path\r\n\r\nAdd more unit test for Predicate pushing down for Parquet Vectorization.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380", "body": "Thanks @thejasmn for working on it. Just some minor issues. @sundapeng Can you take a look at this PR? Thank you!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "deepeshk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/7942bc6c9a38e71ff3f8936508cc773a464b0d87", "message": "HIVE-18465: Hive metastore schema initialization failing on postgres (Deepesh Khandelwal, reviewed by Jesus Camacho Rodriguez)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sershe-apache": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/78d5572f8e165a907c63e1ef8579f591b8c34563", "message": "HIVE-18452 : work around HADOOP-15171 (Sergey Shelukhin, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/72684f10da8f124dfa624b16f3ffebeb2155664d", "message": "HIVE-18273 : add LLAP-level counters for WM (Sergey Shelukhin, reviewed by Harish Jaiprakash and Gunther Hagleitner)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fde503dcaa1f3e322eb4dc34bfffbec89aefd240", "message": "HIVE-18437 : use plan parallelism for the default pool if both are present (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alanfgates": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/d9801d9c6c406d5871147b80bc2e0359c3dbd085", "message": "HIVE-17982 Move metastore specific itests.  This closes #279.  (Alan Gates, reviewed by Peter Vary)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/291", "title": "HIVE-17983 Make the standalone metastore generate tarballs etc.", "body": "See JIRA for full comments.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sahilTakiar": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/87860fbca42b2477f504c8b1cefd43c865a2629c", "message": "HIVE-18214: Flaky test: TestSparkClient (Sahil Takiar, reviewed by Peter Vary)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kgyrtkirk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/2402f3ca55e3f304e041ff53591f846634fbc9ac", "message": "HIVE-18149 HIVE-18416: addendum"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "k0b3rIT": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/13c1bf9e903c502dbfe3f2fb10959ac1a64bc0ad", "message": "HIVE-18161: Remove hive.stats.atomic (Bertalan Kondrat via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "abstractdog": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/5d4559bbdb5fb36450b67841d0da4fa2fecdeeea", "message": "HIVE-18309: qtests: smb_mapjoin_19.q breaks bucketsortoptimize_insert_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/19c2b7225e1c072cba5c1a93329f70059e7e063d", "message": "HIVE-18314: qtests: semijoin_hint.q breaks hybridgrace_hashjoin_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anishek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/22df53b6c223f03edee1a8c271a77e91d66bb2a1", "message": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys (Anishek Agarwal, reviewed Daniel Dai)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/289", "title": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/286", "title": "HIVE-18352: introduce a METADATAONLY option while doing REPL DUMP to allow integrations of other tools", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/283", "title": "HIVE-17829: ArrayIndexOutOfBoundsException - HBASE-backed tables with Avro schema in Hive2", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/263", "title": "HIVE-17830: dbnotification fails to work with rdbms other than postgres", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/262", "title": "HIVE-17825: Socket not closed when trying to read files to copy over in replication from metadata", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/259", "title": "HIVE-17615 : Task.executeTask has to be thread safe for parallel execution", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/246", "title": "HIVE-17426: Execution framework in hive to run tasks in parallel other than MR Tasks", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/240", "title": "HIVE-17410 : repl load task during subsequent DAG generation does notstart from the last partition processed", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/237", "title": "HIVE-16886: HMS log notifications may have duplicated event IDs if multiple HMS are running concurrently", "body": "\u2026ltiple HMS are running concurrently", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daijyc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/970d8c9680d89708c4007de214fdc72b77deb7da", "message": "HIVE-18299: DbNotificationListener fail on mysql with \"select for update\" (Daniel Dai, reviewed by Anishek Agarwal)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c0734ac91bdd3e81b17d93fcfeddd4503430eea8", "message": "HIVE-18298: Fix TestReplicationScenarios.testConstraints (Daniel Dai, reviewed by Sankar Hariappan)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/236", "title": "HIVE-17366: Constraint replication in bootstrap", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dosoft": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/292", "title": "HIVE-17331: Use Path instead of String as key type of the pathToAliases", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/234", "title": "HIVE-17314: Removed obsolete code", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/230", "title": "HIVE-17313: Fixed 'case fall-through'", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/229", "title": "HIVE-17311: Fixed numeric overflow, added test", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sankarh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/290", "title": "HIVE-18192: Introduce WriteID per table rather than using global transaction ID", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/280", "title": "HIVE-18031: Support replication for Alter Database operation", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "msydoron": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/288", "title": "HIVE-18423", "body": "Added full support for jdbc external tables in hive.\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/285", "title": "HIVE-16480 (ORC-285) Empty vector batches of floats or doubles gets", "body": "EOFException.\r\n\r\nSigned-off-by: Owen O'Malley <omalley@apache.org>", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738", "body": "This has been committed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005", "body": "I did port all of the TestOrcFile to the vectorized writer in the new file TestVectorOrcFile. Does that address your concerns?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685", "body": "This patch doesn't actually change the write path. I've done this in HIVE-12055 including adding a new writer version.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185", "body": "The CompressionCodec isn't really an API class, since the user isn't able to add their own implementations (other than the external LZO implementation). I guess I can move it over, if you think it is important. In general LLAP is accessing ORC beneath the API level.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588", "body": "The DirectDecompressor API was added in Hadoop 2.3. I'd rather go ahead and make it a real shim so that we can support older Hadoop versions.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527", "body": "I'll add a comment, but it does clone the FileHandle so that they can be closed separately.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369", "body": "Ok, I've updated both Char and Varchar with the check.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229", "body": "For now we should stick with what Hive supports. I copied this code directly from Hive's HadoopShims.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104", "body": "I'm not sure what you think is misaligned, but I simplified this code.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "amrk7s": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/284", "title": "HIVE-18338 Exposing asynchronous execution through hive-jdbc client", "body": "**Problem statement**\r\n\r\nHive JDBC currently exposes 2 methods related to asynchronous execution\r\n**executeAsync()** - to trigger a query execution and return immediately.\r\n**waitForOperationToComplete()** - which waits till the current execution is complete **blocking the user thread**. \r\n\r\nThis has one problem\r\n- If the client process goes down, there is no way to resume queries although hive server is completely asynchronous. \r\n\r\n**Proposal**\r\n\r\nIf operation handle could be exposed, we can latch on to an active execution of a query.\r\n\r\n**Code changes**\r\n\r\nOperation handle is exposed. So client can keep a copy.\r\nlatchSync() and latchAsync() methods take an operation handle and try to latch on to the current execution in hive server if present", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mattk42": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/278", "title": "HIVE-18295 - Add ability to ignore invalid values in JSON SerDe ", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ymwdalex": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/273", "title": "HIVE-18130: Update table path to storage parameters when alter a table", "body": "When an managed table is created by Spark, table path information is not only store in `location` field (first figure), but also in `parameters.path` fields (second figure).\r\n<img width=\"445\" alt=\"location\" src=\"https://user-images.githubusercontent.com/1458656/33123050-466c6bdc-cf79-11e7-8b53-85ad7dc83f17.png\">\r\n<img width=\"677\" alt=\"storage_parameter\" src=\"https://user-images.githubusercontent.com/1458656/33123051-469576da-cf79-11e7-9c6c-9793a28f8389.png\">\r\n\r\nWhen hive alter a table, `storage parameter` is ignored. Then, spark cannot access the table anymore because spark use the parameter.\r\n\r\nIn this PR, when altering a table, the field `storageDescription.parameters.path` is also updated, if `path` key exists\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hejian991": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/269", "title": "fix small bug: when disable partition statistics in hive-site.xml, bu\u2026", "body": "\u2026t it does not work.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ishitbatra": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/268", "title": "hive installation link", "body": "hive link given", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thejasmn": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/266", "title": "Hive-17897", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268", "body": "addressing the comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518", "body": "Updated patch addresses additional comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "kanna14243": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/265", "title": "Update TBinarySortableProtocol.java", "body": "writeTextBytes doesn't respect the start parameter. It'll work only for cases where start = 0. Fixing it so that it'll work for any value of start.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "b-slim": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/249", "title": "[HIVE-17523] Fix insert into bug", "body": "https://issues.apache.org/jira/browse/HIVE-17523", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmartin-rp": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/232", "title": "Update 039-HIVE-12274.oracle.sql", "body": "You cannot modify a column from VARCHAR2(4000) to CLOB directly. You need to add a new column and drop the old one or recreate the table. I chose to recreate the table because it's more clean.\r\n\r\nhttps://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:1770086700346491686\r\n\r\nAlso, if you do something like:\r\nALTER TABLE COLUMNS_V2 MODIFY (COLUMN_NAME VARCHAR(767) NOT NULL);\r\nAnd the column is already NOT NULL, the ALTER TABLE fails.  I removed 2 like this.\r\n\r\nI hope it helps.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jarcec": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868", "body": "Hi Viraj,\nunfortunately we can't accept pull requests due to licensing issues. You need to attach your patch to the JIRA in order to enable Hive community to get it in.\n\nJarcec\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "brockn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540", "body": "See https://issues.apache.org/jira/browse/HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pavel-sakun": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553", "body": "Closing this one in favour of HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "codingtony": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235", "body": "This is the fix  release 0.13.1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "slavag-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547", "body": "please do not merge for now till we can get a release out.  thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "srowen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047", "body": "PS I found this from your cross-posted question about Kendall coefficient on Hive. What is this? PRs aren't for questions and this is some arbitrary branch of commits you've opened.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "rdblue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346", "body": "Hi @dguy, I've been working on this problem recently in [HIVE-8909](https://issues.apache.org/jira/browse/HIVE-8909). That's already been merged, but I'm wondering if there's a case that we've missed that you fix here?\n\nIt looks like this allows the object inspector to work with the extra level of nesting that is included by the deserializer or detect if it isn't there. For HIVE-8909, we opted to keep the extra level of nesting just so we wouldn't need to change the object inspector code. We will need to follow up with a commit that removes the added layer from both. Did you come across a case where the added layer isn't there?\n\nAlso, I don't think Hive uses pull requests, so the best way to follow up is to open an issue in the JIRA issue tracker: https://issues.apache.org/jira/browse/HIVE\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972", "body": "Hi Damian, thanks for following up and letting us know it's working!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dguy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937", "body": "Hi Ryan,\n\nThanks for getting in touch. We are writing avro/parquet data-sets from\nvarious Map/Reduce jobs - we then create external hive tables for these\ndata-sets. The issue we are seeing is that AvroSchemaConverter\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/main/java/parquet/avro/AvroSchemaConverter.java\nconverts avro array types like this:\n\n{\n     \"name\" : \"myarray\",\n     \"type\" : {\n       \"type\" : \"array\",\n       \"items\" : \"int\"\n }\n\ninto Parquet:\nrequired group myarray (LIST) {\n       repeated int32 array;\n }\n\nYou can see an example here\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/test/java/parquet/avro/TestAvroSchemaConverter.java\n\nWhere as hive is expecting something like:\nrequired group myarray  {\n       repeated group bag {\n          optional int32 array;\n       }\n }\n\nWithout this change our external tables with array types currently won't\ndeserialize. So my patch was an attempt to make it work for both cases i\nknow about. I'll have to have a look through your patch and see if it\nhelps.\n\nThanks for the pointer.\nCheers,\nDamian\n\nOn 10 December 2014 at 01:50, Ryan Blue notifications@github.com wrote:\n\n> Hi @dguy https://github.com/dguy, I've been working on this problem\n> recently in [HIVE-8909|https://issues.apache.org/jira/browse/HIVE-8909].\n> That's already been merged, but I'm wondering if there's a case that we've\n> missed that you fix here?\n> \n> It looks like this allows the object inspector to work with the extra\n> level of nesting that is included by the deserializer or detect if it isn't\n> there. For HIVE-8909, we opted to keep the extra level of nesting just so\n> we wouldn't need to change the object inspector code. We will need to\n> follow up with a commit that removes the added layer from both. Did you\n> come across a case where the added layer isn't there?\n> \n> Also, I don't think Hive uses pull requests, so the best way to follow up\n> is to open an issue in the JIRA issue tracker:\n> https://issues.apache.org/jira/browse/HIVE\n> \n> Thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hive/pull/26#issuecomment-66392346.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730", "body": "Hi,\nI've verified it works fine with your fix. Thanks for pointing me to this.\nCheers,\nDamian\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "swarnim87": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308", "body": "Finish javadoc\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "apivovarov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "zhichao-li": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233", "body": "cc @apivovarov \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Lewuathe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402", "body": "Sorry for mistake.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "prasanthj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388", "body": "@kirill-vlasov Could you please create a JIRA here https://issues.apache.org/jira/browse/HIVE/ and add the JIRA id in the title of the PR?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629", "body": "Would it be easier to support bulk writes? As isPresent is called for every column.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414", "body": "Actually never mind, since the output stream is buffered. This won't be big win.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051", "body": "What happens if the vec is null and started is false? A run can start with null structs (struct column) right?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177", "body": "Again ignore this :) I guess that case is handled by super(). Correct me if I am wrong.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540", "body": "It will be good to add tests that uses WriterOptions.setSchema() to specify the types instead of OI. I see that you have fixed bunch of OI casts that would have cause NPE otherwise (if TypeDescription is set through writer).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842", "body": "Do we need to bump up the writer version? To differentiate ORC files written with row-by-row vs vector writer?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511", "body": "Yes. Sorry I missed that file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544", "body": "Can we put all the interfaces in api package?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318", "body": "This isn't a shim right? Can this be moved over to OrcUtils?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625", "body": "Does it clone the options/properties alone? or the fs input stream as well? Can you please add a comment as in what does it clone and what not.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043", "body": "Should we check if it has hasMaximumLength()?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417", "body": "Is there a minimum supported hadoop version for orc? If we don't support older hadoop version then this shim might not be required.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431", "body": "Where do we close this stream?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440", "body": "nit: bad alignment\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450", "body": "ql dependency?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466", "body": "Can this be moved to common instead?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488", "body": "Never mind. This is already part of storage-api. So not a problem.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "joshelser": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513", "body": "Thanks for submitting a PR for this, @chutium. I left you some comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867", "body": "Isn't the `RecordWriter<Text, Mutation>` duplicative since `AccumuloRecordWriter` already implements that?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048", "body": "This seems wrong. For the AccumuloRecordWriter, the `Text` can signify an Accumulo table to which this update should be written. Where is this `Writable` coming from?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "chutium": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846", "body": "thanks for reviewing, to reproduce the issue, you can create a spark context with following jars:\naccumulo-core-1.6.0.jar\naccumulo-fate-1.6.0.jar\naccumulo-start-1.6.0.jar\naccumulo-trace-1.6.0.jar\nhive-accumulo-handler-1.2.1.jar\n\nthen run some hive query like:\n\n```\nset accumulo.instance.name=instance_name;\nset accumulo.zookeepers=zk_host:2181;\nset accumulo.user.name=user;\nset accumulo.user.pass=pass;\n\nCREATE TABLE testtable(rowid STRING, value STRING)\nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,colf:colq');\n\nINSERT OVERWRITE TABLE testtable SELECT col1, col2 FROM some_hive_table;\n```\n\nthen will get error `java.lang.ClassCastException` says `HiveAccumuloTableOutputFormat cannot be cast to org.apache.hadoop.hive.ql.io.HiveOutputFormat`\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969", "body": "yes... seems like RecordWriter is unnecessary, will check this soon\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651", "body": "noticed and wondering as well, but it works in our trials... the mutations will be written into the target table, that defined in hive query -_- my assume is the table name is already set in JobConf, so we do not need to specify table name for each write operation.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832", "body": "I might pick this up at a later date.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jiangxt2": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183", "body": "Is it my fault? How can i fix it? thx my bro. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475", "body": "@dmtolpeko Hi, I created a JIRA for this problem.\nThe url is https://issues.apache.org/jira/browse/HIVE-13877\nWhat should I do next?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588", "body": "@dmtolpeko OK, I GOT IT. Thanks very much. I'll wait for your response.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dmtolpeko": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191", "body": "Can you create a Hive JIRA for this problem? Github is just a mirror\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901", "body": "I will review your changes and create a patch. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "dark0dave": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930", "body": "named travis file wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}}, "5": {"kgyrtkirk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/ad1243bef60f1b1f5f15a511761959df9abae615", "message": "HIVE-18413 : Grouping of an empty result set may only contain null values (Zoltan Haindrich via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/2402f3ca55e3f304e041ff53591f846634fbc9ac", "message": "HIVE-18149 HIVE-18416: addendum"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ekoifman": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/727581d749306d5df3b6ab8c4adb21ef0fe31506", "message": "HIVE-18460 - Compactor doesn't pass Table properties to the Orc writer (Eugene Koifman, reviewed by Prasanth Jayachandran)"}, {"url": "https://api.github.com/repos/apache/hive/commits/dfd7ea368dc8ea2ef071543c3d0cea9e05cd115a", "message": "HIVE-18419 - CliDriver loads different hive-site.xml into HiveConf and MetastoreConf (Eugene Koifman, reviewed by Alan Gates)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fe3190d19fa1324a6835692e71f747e6cb37c7d1", "message": "HIVE-18429 - Compaction should handle a case when it produces no output (Eugene Koifman, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vihangk1": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/ef9d3ee97cee30725381013f4051790c432aa726", "message": "HIVE-18323 : Vectorization: add the support of timestamp in VectorizedPrimitiveColumnReader for parquet (Vihang Karajgaonkar, reviewed by Aihua Xu and Ferdinand Xu)"}, {"url": "https://api.github.com/repos/apache/hive/commits/798a17c679fc04e5a20b15675b6883f52a26ea57", "message": "HIVE-18461 : Fix precommit hive job (Vihang Karajgaonkar)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/287", "title": "HIVE-17580 : Remove standalone-metastore's dependency with serdes", "body": "Removing the dependency on serdes for the metastore requires a series of changes. I have created multiple commits which hopefully would be easier to review. Each major commit has a descriptive commit message to give a high level idea of what the change is doing. There are still some bits which need to be completed but it would be good to a review.\r\n\r\nOverview of all the changes done:\r\n1. Creates a new module called serde-api under storage-api like discussed. Although I think we can keep it separate as well.\r\n2. Moved List, Map, Struct, Constant, Primitive, Union ObjectInspectors to serde-api\r\n3. Moved PrimitiveTypeInfo, PrimitiveTypeEntry and TypeInfo to serde-api.\r\n4. Moved TypeInfoParser, TypeInfoFactory to serde-api\r\n5. Added a new class which reading avro storage schema by copying the code from AvroSerde and AvroSerdeUtils. The parsing is done such that String value is first converted into TypeInfos and then into FieldSchemas bypassing the need for ObjectInspectors. In theory we could get rid of TypeInfos as well but that path was getting too difficult with lot of duplicate code between Hive and metastore.\r\n6. Introduces a default storage schema reader. I noticed that most of the serdes use the same logic to parse the metadata information. This code should be refactored to a common place instead of having many copies (one in standalone hms and another set in multiple serdes)\r\n7. Moved HiveChar, HiveVarchar, HiveCharWritable, HiveVarcharWritable to storage-api. I noticed that HiveDecimal is already in storage-api. It probably makes sense to move the other primitive types (timestamp, interval etc)to storage-api as well but it requires storage-api to be upgraded to Java 8.\r\n8. Adds a basic test for the schema reader. I plan to add more tests as this code is reviewed.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcamachor": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/80e6f7b0f13c134642763b0a692c3abbf3ffe106", "message": "HIVE-18386 : Create dummy materialized views registry and make it configurable (Jesus Camacho Rodriguez via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>"}, {"url": "https://api.github.com/repos/apache/hive/commits/a45becb1602573cae32673e32161b1a3aa10a86b", "message": "HIVE-18473: Infer timezone information correctly in DruidSerde (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/ac247817f441eba5daf5b07cc831347d1c762ee4", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum II)\n\n* Fix dangling tests"}, {"url": "https://api.github.com/repos/apache/hive/commits/b1cdbc60d6ef856b852afe1aa44bd3520d5fb84a", "message": "HIVE-18416: Initial support for TABLE function (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan) (addendum)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdere": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/01816fca2c2620f4f57a2b2a5c6ea404e1f0038f", "message": "HIVE-18430: Add new determinism category for runtime constants (current_date, current_timestamp) (Jason Dere, reviewed by Jesus Camacho Rodriguez)"}, {"url": "https://api.github.com/repos/apache/hive/commits/790976907070a2a737186543d81c71eb542a5337", "message": "HIVE-18385: mergejoin fails with java.lang.IllegalStateException (Jason Dere, reviewed by Deepak Jaiswal)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vaibhavgumashta": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/456a65180dcb84f69f26b4c9b9265165ad16dfe4", "message": "HIVE-17495: CachedStore: prewarm improvement (avoid multiple sql calls to read partition column stats), refactoring and caching some aggregate stats (Vaibhav Gumashta reviewed by Daniel Dai)"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313", "body": "Looks good, +1.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/106633313/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083", "body": "+1. Looks good.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170460083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "pvary": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/fbb3ed15fd0f078369c1fcb2edd68326d272758a", "message": "HIVE-18372: Create testing infra to test different HMS instances (Peter Vary, reviewed by Marta Kuczora, Vihang Karajgaonkar and Adam Szita)"}, {"url": "https://api.github.com/repos/apache/hive/commits/6938fcabf0944817fed8241c48b57fb3f5d98c69", "message": "HIVE-18443: Ensure git gc finished in ptest prep phase before copying repo (Adam Szita, via Peter Vary)"}, {"url": "https://api.github.com/repos/apache/hive/commits/b826072edb429214e9fe073dea9381449396f05d", "message": "HIVE-18355: Add builder for metastore Thrift classes missed in the first pass - FunctionBuilder (Peter Vary, reviewed by Alan Gates)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "winningsix": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/17abdb211c1b2b749fc7d8265d31e6c5987cea4b", "message": "HIVE-18411: Fix ArrayIndexOutOfBoundsException for VectorizedListColumnReader (Colin Ma, reviewed by Ferdinand Xu)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/251", "title": "HIVE-14836: Test the predicate pushing down support for Parquet vecto\u2026", "body": "\u2026rization read path\r\n\r\nAdd more unit test for Predicate pushing down for Parquet Vectorization.", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380", "body": "Thanks @thejasmn for working on it. Just some minor issues. @sundapeng Can you take a look at this PR? Thank you!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165320380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "deepeshk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/7942bc6c9a38e71ff3f8936508cc773a464b0d87", "message": "HIVE-18465: Hive metastore schema initialization failing on postgres (Deepesh Khandelwal, reviewed by Jesus Camacho Rodriguez)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sershe-apache": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/78d5572f8e165a907c63e1ef8579f591b8c34563", "message": "HIVE-18452 : work around HADOOP-15171 (Sergey Shelukhin, reviewed by Ashutosh Chauhan)"}, {"url": "https://api.github.com/repos/apache/hive/commits/72684f10da8f124dfa624b16f3ffebeb2155664d", "message": "HIVE-18273 : add LLAP-level counters for WM (Sergey Shelukhin, reviewed by Harish Jaiprakash and Gunther Hagleitner)"}, {"url": "https://api.github.com/repos/apache/hive/commits/fde503dcaa1f3e322eb4dc34bfffbec89aefd240", "message": "HIVE-18437 : use plan parallelism for the default pool if both are present (Sergey Shelukhin, reviewed by Prasanth Jayachandran)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alanfgates": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/d9801d9c6c406d5871147b80bc2e0359c3dbd085", "message": "HIVE-17982 Move metastore specific itests.  This closes #279.  (Alan Gates, reviewed by Peter Vary)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/291", "title": "HIVE-17983 Make the standalone metastore generate tarballs etc.", "body": "See JIRA for full comments.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sahilTakiar": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/87860fbca42b2477f504c8b1cefd43c865a2629c", "message": "HIVE-18214: Flaky test: TestSparkClient (Sahil Takiar, reviewed by Peter Vary)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "k0b3rIT": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/13c1bf9e903c502dbfe3f2fb10959ac1a64bc0ad", "message": "HIVE-18161: Remove hive.stats.atomic (Bertalan Kondrat via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "abstractdog": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/5d4559bbdb5fb36450b67841d0da4fa2fecdeeea", "message": "HIVE-18309: qtests: smb_mapjoin_19.q breaks bucketsortoptimize_insert_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}, {"url": "https://api.github.com/repos/apache/hive/commits/19c2b7225e1c072cba5c1a93329f70059e7e063d", "message": "HIVE-18314: qtests: semijoin_hint.q breaks hybridgrace_hashjoin_2.q (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "anishek": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/22df53b6c223f03edee1a8c271a77e91d66bb2a1", "message": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys (Anishek Agarwal, reviewed Daniel Dai)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/289", "title": "HIVE-18341: Add repl load support for adding \"raw\" namespace for TDE with same encryption keys", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/286", "title": "HIVE-18352: introduce a METADATAONLY option while doing REPL DUMP to allow integrations of other tools", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/283", "title": "HIVE-17829: ArrayIndexOutOfBoundsException - HBASE-backed tables with Avro schema in Hive2", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/263", "title": "HIVE-17830: dbnotification fails to work with rdbms other than postgres", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/262", "title": "HIVE-17825: Socket not closed when trying to read files to copy over in replication from metadata", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/259", "title": "HIVE-17615 : Task.executeTask has to be thread safe for parallel execution", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/246", "title": "HIVE-17426: Execution framework in hive to run tasks in parallel other than MR Tasks", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/240", "title": "HIVE-17410 : repl load task during subsequent DAG generation does notstart from the last partition processed", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/237", "title": "HIVE-16886: HMS log notifications may have duplicated event IDs if multiple HMS are running concurrently", "body": "\u2026ltiple HMS are running concurrently", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daijyc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hive/commits/970d8c9680d89708c4007de214fdc72b77deb7da", "message": "HIVE-18299: DbNotificationListener fail on mysql with \"select for update\" (Daniel Dai, reviewed by Anishek Agarwal)"}, {"url": "https://api.github.com/repos/apache/hive/commits/c0734ac91bdd3e81b17d93fcfeddd4503430eea8", "message": "HIVE-18298: Fix TestReplicationScenarios.testConstraints (Daniel Dai, reviewed by Sankar Hariappan)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/236", "title": "HIVE-17366: Constraint replication in bootstrap", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dosoft": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/292", "title": "HIVE-17331: Use Path instead of String as key type of the pathToAliases", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/234", "title": "HIVE-17314: Removed obsolete code", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/230", "title": "HIVE-17313: Fixed 'case fall-through'", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/229", "title": "HIVE-17311: Fixed numeric overflow, added test", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sankarh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/290", "title": "HIVE-18192: Introduce WriteID per table rather than using global transaction ID", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/280", "title": "HIVE-18031: Support replication for Alter Database operation", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "msydoron": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/288", "title": "HIVE-18423", "body": "Added full support for jdbc external tables in hive.\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/285", "title": "HIVE-16480 (ORC-285) Empty vector batches of floats or doubles gets", "body": "EOFException.\r\n\r\nSigned-off-by: Owen O'Malley <omalley@apache.org>", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738", "body": "This has been committed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/157602738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17354974", "body": "But it recreates target/tmp in the next line. Which test assumes that files are pre-located there?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17354974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005", "body": "I did port all of the TestOrcFile to the vectorized writer in the new file TestVectorOrcFile. Does that address your concerns?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44687005/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685", "body": "This patch doesn't actually change the write path. I've done this in HIVE-12055 including adding a new writer version.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45020685/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185", "body": "The CompressionCodec isn't really an API class, since the user isn't able to add their own implementations (other than the external LZO implementation). I guess I can move it over, if you think it is important. In general LLAP is accessing ORC beneath the API level.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46189185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588", "body": "The DirectDecompressor API was added in Hadoop 2.3. I'd rather go ahead and make it a real shim so that we can support older Hadoop versions.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/46200588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527", "body": "I'll add a comment, but it does clone the FileHandle so that they can be closed separately.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60961527/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369", "body": "Ok, I've updated both Char and Varchar with the check.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60963369/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229", "body": "For now we should stick with what Hive supports. I copied this code directly from Hive's HadoopShims.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63047229/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104", "body": "I'm not sure what you think is misaligned, but I simplified this code.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/63048104/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "amrk7s": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/284", "title": "HIVE-18338 Exposing asynchronous execution through hive-jdbc client", "body": "**Problem statement**\r\n\r\nHive JDBC currently exposes 2 methods related to asynchronous execution\r\n**executeAsync()** - to trigger a query execution and return immediately.\r\n**waitForOperationToComplete()** - which waits till the current execution is complete **blocking the user thread**. \r\n\r\nThis has one problem\r\n- If the client process goes down, there is no way to resume queries although hive server is completely asynchronous. \r\n\r\n**Proposal**\r\n\r\nIf operation handle could be exposed, we can latch on to an active execution of a query.\r\n\r\n**Code changes**\r\n\r\nOperation handle is exposed. So client can keep a copy.\r\nlatchSync() and latchAsync() methods take an operation handle and try to latch on to the current execution in hive server if present", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mattk42": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/278", "title": "HIVE-18295 - Add ability to ignore invalid values in JSON SerDe ", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ymwdalex": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/273", "title": "HIVE-18130: Update table path to storage parameters when alter a table", "body": "When an managed table is created by Spark, table path information is not only store in `location` field (first figure), but also in `parameters.path` fields (second figure).\r\n<img width=\"445\" alt=\"location\" src=\"https://user-images.githubusercontent.com/1458656/33123050-466c6bdc-cf79-11e7-8b53-85ad7dc83f17.png\">\r\n<img width=\"677\" alt=\"storage_parameter\" src=\"https://user-images.githubusercontent.com/1458656/33123051-469576da-cf79-11e7-9c6c-9793a28f8389.png\">\r\n\r\nWhen hive alter a table, `storage parameter` is ignored. Then, spark cannot access the table anymore because spark use the parameter.\r\n\r\nIn this PR, when altering a table, the field `storageDescription.parameters.path` is also updated, if `path` key exists\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hejian991": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/269", "title": "fix small bug: when disable partition statistics in hive-site.xml, bu\u2026", "body": "\u2026t it does not work.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ishitbatra": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/268", "title": "hive installation link", "body": "hive link given", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thejasmn": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/266", "title": "Hive-17897", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268", "body": "addressing the comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165341268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518", "body": "Updated patch addresses additional comments\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/165342518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "kanna14243": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/265", "title": "Update TBinarySortableProtocol.java", "body": "writeTextBytes doesn't respect the start parameter. It'll work only for cases where start = 0. Fixing it so that it'll work for any value of start.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "b-slim": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/249", "title": "[HIVE-17523] Fix insert into bug", "body": "https://issues.apache.org/jira/browse/HIVE-17523", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmartin-rp": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hive/pulls/232", "title": "Update 039-HIVE-12274.oracle.sql", "body": "You cannot modify a column from VARCHAR2(4000) to CLOB directly. You need to add a new column and drop the old one or recreate the table. I chose to recreate the table because it's more clean.\r\n\r\nhttps://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:1770086700346491686\r\n\r\nAlso, if you do something like:\r\nALTER TABLE COLUMNS_V2 MODIFY (COLUMN_NAME VARCHAR(767) NOT NULL);\r\nAnd the column is already NOT NULL, the ALTER TABLE fails.  I removed 2 like this.\r\n\r\nI hope it helps.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jarcec": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868", "body": "Hi Viraj,\nunfortunately we can't accept pull requests due to licensing issues. You need to attach your patch to the JIRA in order to enable Hive community to get it in.\n\nJarcec\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/17135868/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "brockn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540", "body": "See https://issues.apache.org/jira/browse/HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47619540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "pavel-sakun": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553", "body": "Closing this one in favour of HIVE-7303\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/47620553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "codingtony": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235", "body": "This is the fix  release 0.13.1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/49483235/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "slavag-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547", "body": "please do not merge for now till we can get a release out.  thanks\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/52104547/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "srowen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047", "body": "PS I found this from your cross-posted question about Kendall coefficient on Hive. What is this? PRs aren't for questions and this is some arbitrary branch of commits you've opened.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66122047/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "rdblue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346", "body": "Hi @dguy, I've been working on this problem recently in [HIVE-8909](https://issues.apache.org/jira/browse/HIVE-8909). That's already been merged, but I'm wondering if there's a case that we've missed that you fix here?\n\nIt looks like this allows the object inspector to work with the extra level of nesting that is included by the deserializer or detect if it isn't there. For HIVE-8909, we opted to keep the extra level of nesting just so we wouldn't need to change the object inspector code. We will need to follow up with a commit that removes the added layer from both. Did you come across a case where the added layer isn't there?\n\nAlso, I don't think Hive uses pull requests, so the best way to follow up is to open an issue in the JIRA issue tracker: https://issues.apache.org/jira/browse/HIVE\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66392346/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972", "body": "Hi Damian, thanks for following up and letting us know it's working!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66649972/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dguy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937", "body": "Hi Ryan,\n\nThanks for getting in touch. We are writing avro/parquet data-sets from\nvarious Map/Reduce jobs - we then create external hive tables for these\ndata-sets. The issue we are seeing is that AvroSchemaConverter\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/main/java/parquet/avro/AvroSchemaConverter.java\nconverts avro array types like this:\n\n{\n     \"name\" : \"myarray\",\n     \"type\" : {\n       \"type\" : \"array\",\n       \"items\" : \"int\"\n }\n\ninto Parquet:\nrequired group myarray (LIST) {\n       repeated int32 array;\n }\n\nYou can see an example here\nhttps://github.com/apache/incubator-parquet-mr/blob/master/parquet-avro/src/test/java/parquet/avro/TestAvroSchemaConverter.java\n\nWhere as hive is expecting something like:\nrequired group myarray  {\n       repeated group bag {\n          optional int32 array;\n       }\n }\n\nWithout this change our external tables with array types currently won't\ndeserialize. So my patch was an attempt to make it work for both cases i\nknow about. I'll have to have a look through your patch and see if it\nhelps.\n\nThanks for the pointer.\nCheers,\nDamian\n\nOn 10 December 2014 at 01:50, Ryan Blue notifications@github.com wrote:\n\n> Hi @dguy https://github.com/dguy, I've been working on this problem\n> recently in [HIVE-8909|https://issues.apache.org/jira/browse/HIVE-8909].\n> That's already been merged, but I'm wondering if there's a case that we've\n> missed that you fix here?\n> \n> It looks like this allows the object inspector to work with the extra\n> level of nesting that is included by the deserializer or detect if it isn't\n> there. For HIVE-8909, we opted to keep the extra level of nesting just so\n> we wouldn't need to change the object inspector code. We will need to\n> follow up with a commit that removes the added layer from both. Did you\n> come across a case where the added layer isn't there?\n> \n> Also, I don't think Hive uses pull requests, so the best way to follow up\n> is to open an issue in the JIRA issue tracker:\n> https://issues.apache.org/jira/browse/HIVE\n> \n> Thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hive/pull/26#issuecomment-66392346.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66427937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730", "body": "Hi,\nI've verified it works fine with your fix. Thanks for pointing me to this.\nCheers,\nDamian\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/66442730/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "swarnim87": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/105753655/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308", "body": "Finish javadoc\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/31102308/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "apivovarov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/116139013/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "zhichao-li": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233", "body": "cc @apivovarov \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/123561233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Lewuathe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402", "body": "Sorry for mistake.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/153306402/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "prasanthj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388", "body": "@kirill-vlasov Could you please create a JIRA here https://issues.apache.org/jira/browse/HIVE/ and add the JIRA id in the title of the PR?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/170674388/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15379454", "body": "@phoenixhadoop hive trunk is updated with new hive-log4j2.properties based configuration. If log4j.configurationFile is not explicitly set then by default hive-log4j2.properties file will be looked up in CLASSPATH. Typically, HIVE_CONF_DIR will be set to conf directory which contains hive-log4j2.properties file. HIVE_CONF_DIR is automatically added to CLASSPATH by the hive script. Are you still seeing this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15379454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629", "body": "Would it be easier to support bulk writes? As isPresent is called for every column.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44605629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414", "body": "Actually never mind, since the output stream is buffered. This won't be big win.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44606414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051", "body": "What happens if the vec is null and started is false? A run can start with null structs (struct column) right?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607051/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177", "body": "Again ignore this :) I guess that case is handled by super(). Correct me if I am wrong.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607177/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540", "body": "It will be good to add tests that uses WriterOptions.setSchema() to specify the types instead of OI. I see that you have fixed bunch of OI casts that would have cause NPE otherwise (if TypeDescription is set through writer).\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842", "body": "Do we need to bump up the writer version? To differentiate ORC files written with row-by-row vs vector writer?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44607842/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511", "body": "Yes. Sorry I missed that file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/44994511/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544", "body": "Can we put all the interfaces in api package?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45911544/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318", "body": "This isn't a shim right? Can this be moved over to OrcUtils?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/45920318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625", "body": "Does it clone the options/properties alone? or the fs input stream as well? Can you please add a comment as in what does it clone and what not.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60957625/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043", "body": "Should we check if it has hasMaximumLength()?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/60958043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417", "body": "Is there a minimum supported hadoop version for orc? If we don't support older hadoop version then this shim might not be required.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431", "body": "Where do we close this stream?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424431/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440", "body": "nit: bad alignment\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450", "body": "ql dependency?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466", "body": "Can this be moved to common instead?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488", "body": "Never mind. This is already part of storage-api. So not a problem.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/62424488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "joshelser": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513", "body": "Thanks for submitting a PR for this, @chutium. I left you some comments.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/189366513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867", "body": "Isn't the `RecordWriter<Text, Mutation>` duplicative since `AccumuloRecordWriter` already implements that?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54267867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048", "body": "This seems wrong. For the AccumuloRecordWriter, the `Text` can signify an Accumulo table to which this update should be written. Where is this `Writable` coming from?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54268048/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "chutium": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846", "body": "thanks for reviewing, to reproduce the issue, you can create a spark context with following jars:\naccumulo-core-1.6.0.jar\naccumulo-fate-1.6.0.jar\naccumulo-start-1.6.0.jar\naccumulo-trace-1.6.0.jar\nhive-accumulo-handler-1.2.1.jar\n\nthen run some hive query like:\n\n```\nset accumulo.instance.name=instance_name;\nset accumulo.zookeepers=zk_host:2181;\nset accumulo.user.name=user;\nset accumulo.user.pass=pass;\n\nCREATE TABLE testtable(rowid STRING, value STRING)\nSTORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'\nWITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,colf:colq');\n\nINSERT OVERWRITE TABLE testtable SELECT col1, col2 FROM some_hive_table;\n```\n\nthen will get error `java.lang.ClassCastException` says `HiveAccumuloTableOutputFormat cannot be cast to org.apache.hadoop.hive.ql.io.HiveOutputFormat`\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/191775846/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969", "body": "yes... seems like RecordWriter is unnecessary, will check this soon\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54878969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651", "body": "noticed and wondering as well, but it works in our trials... the mutations will be written into the target table, that defined in hive query -_- my assume is the table name is already set in JobConf, so we do not need to specify table name for each write operation.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/pulls/comments/54881651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832", "body": "I might pick this up at a later date.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/217591832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "jiangxt2": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183", "body": "Is it my fault? How can i fix it? thx my bro. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222101183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475", "body": "@dmtolpeko Hi, I created a JIRA for this problem.\nThe url is https://issues.apache.org/jira/browse/HIVE-13877\nWhat should I do next?\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222108475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588", "body": "@dmtolpeko OK, I GOT IT. Thanks very much. I'll wait for your response.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222111588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "dmtolpeko": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191", "body": "Can you create a Hive JIRA for this problem? Github is just a mirror\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222104191/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901", "body": "I will review your changes and create a patch. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/222110901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "dark0dave": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930", "body": "named travis file wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/issues/comments/235252930/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "yoni": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/2375698", "body": "This change breaks development on OS X. Not sure if anyone else is having the same issues I am, but I can't seem to work around this. `git status` constantly reports changes have been made:\n\n```\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n$ git reset --hard\nHEAD is now at af0162d HIVE-446 Implement TRUNCATE\n$ git status\n# On branch trunk\n# Changes not staged for commit:\n#   (use \"git add <file>...\" to update what will be committed)\n#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n#\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/StandardUnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/UnionObjectInspector.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/ListTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/MapTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/PrimitiveTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfo.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java\n#   modified:   serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java\n#   modified:   serde/src/test/org/apache/hadoop/hive/serde2/thrift_test/CreateSequenceFile.java\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/2375698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sameeragarwal": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/3868174", "body": "Shouldn't this be: \n\n``` java\nmyagg.variance += varianceFieldOI.get(partialVariance)\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/3868174/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "ksumit": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/5973358", "body": "shouldn't this be 0.13.0 for this branch? i tried compiling the current tip, it's broken and changing it to 0.13.0 works just fine.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/5973358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "HZMengYue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/8795892", "body": "hi,i checkout a spark branch today,and after built it,derby-10.11.1.1.jar in hive dir lib,but when i run hive shell ,like  ./hive --auxpath /home/hispark/spark-1.2.0/assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar,it give me follow error:and i think u had fixed this problem.\nLogging initialized using configuration in file:/home/hispark/apache-hive-0.15.0-SNAPSHOT-bin/conf/hive-log4j.properties\nException in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:449)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:634)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:578)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1481)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2674)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2693)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:430)\n        ... 7 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1479)\n        ... 12 more\nCaused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory\nNestedThrowables:\njava.lang.reflect.InvocationTargetException\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n        at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:341)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:370)\n        at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:267)\n        at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:234)\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:572)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:550)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:603)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:441)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5598)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:182)\n        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)\n        ... 17 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)\n        at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:282)\n        at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:240)\n        at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:286)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n        at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n        at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n        ... 46 more\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccess\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/8795892/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sershe": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/9465637", "body": "should be gone\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465637/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/9465752", "body": "this is wrong\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/9465752/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jalajthanaki": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/11626965", "body": "I want to ask you ,Is there full text index kind of functionality available in Apache Hive for searching, matching & ranking keywords of the text?\n\nIf any functionality will be there then let me know.\n\nThanks,\nJalaj T\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/11626965/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "merlin-zaraza": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14273837", "body": "Ok, initialize method is deprecated. How to use GenericUDTF now? (e.g. check for arguments)\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14273837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "walla2sl": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/14312112", "body": "I'm having the same issue. How is GenericUDTF usable now? After trying to use a UTDF I created, I keep getting \"Error while compiling statement: FAILED: IllegalStateException Should not be called directly\". \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/14312367", "body": "I opened https://issues.apache.org/jira/browse/HIVE-12377\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/14312367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "phoenixhadoop": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/15241484", "body": "I am using hive-2.0.0-SNAPSHOT\nwhen starting metastore I meet the ERROR: ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\n\nwhen  I configure :export HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \",the ERROR disappear.\n\nHow can I do?\n\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15241484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hive/comments/15244734", "body": "Hi,prasanthj \nNow we use hive-2.0.0-SNAPSHOT,we meet an error when starting the service metastore .\nThe error is :ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\nIf we configure the variable HADOOP_CLIENT_OPTS  as\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS -Dlog4j.configurationFile=hive-log4j2.xml \" ,the error disappear,but entering hive cli slower.\nPlease help me how can avoid the ERROR.\nThank you very much.\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/15244734/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "prongs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17350037", "body": "This is deleting `target/tmp/`causing all subsequent tests to fail. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17350037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363722", "body": "All of them!\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363722/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hive/comments/17363749", "body": "Check this: https://github.com/apache/hive/blob/master/pom.xml#L914. Then this: https://github.com/apache/hive/blob/master/pom.xml#L991\n\nFirst one sets up target/tmp/conf and second one is adding that to classpath. If the directory gets deleted, subsequent tests that create a `new HiveConf()` don't get to read `target/tmp/conf/hive-site.xml` and there are some test specific properties there. You can't just delete the directory and recreate. This should have been caught in review. \n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17363749/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "goyalr41": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/17575099", "body": "Hi,\n\nJust wanted to know how to access this function now? Should I flatten the map returned by new function getMapRedStats().\n\nThanks,\nRaman\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/17575099/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "djhwx": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20446807", "body": "The build is failing in tests. Can you please revert this back to unblock others and check this in which fixed?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20446807/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nguyenhoan": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/20889577", "body": "Hi @rbalamohan \r\nWe are a team of researchers from Iowa State, The University of Texs at Dallas and Oregon State University, USA. We are investigating common/repeated code changes.\r\nWe have four short questions regarding the change to this statement of the commit.\r\n\r\nQuestions:\r\n\r\nQ1- Is the change at these lines similar to another change from before? (yes, no, not sure)\r\n\r\nQ2- Can you briefly describe the change and why you made it? (for example, checking parameter before calling the method to avoid a Null Pointer Exception)\r\n\r\nQ3- Can you give it a name? (for example, Null Check)\r\n\r\nQ4- Would you like to have this change automated by a tool? (Yes, No, Already automated)\r\n\r\nThe data collected from the answers will never be associated with you or your project. Our questions are about recurring code changes from the developer community, not about personal information. All the data is merged across recurring changes from GitHub repositories. We will publish aggregated data from the trends of the whole community. \r\nWe have a long tradition of developing refactoring tools and contributing them freely to the Eclipse, Netbeans, Android Studio under their respective FLOSS licenses. For example, look at some of our recently released refactoring tools: http://refactoring.info/tools/ \r\n\r\nThank you,\r\nHoan Nguyen https://sites.google.com/site/nguyenanhhoan/\r\nMichael Hilton http://web.engr.oregonstate.edu/~hiltonm/\r\nTien Nguyen http://www.utdallas.edu/~tien.n.nguyen/\r\nDanny Dig http://eecs.oregonstate.edu/people/dig-danny\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/20889577/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "kiddingbaby": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hive/comments/26133483", "body": "The property **hive.server.read.socket.timeout** not found in Wiki: Hive Configuration, can I use it in hive v1.2.1?", "reactions": {"url": "https://api.github.com/repos/apache/hive/comments/26133483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}}}}