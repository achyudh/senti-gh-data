{"_default": {"1": {"StephanEwen": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/1f9c2d9740ffea2b59b8f5f3da287a0dc890ddbf", "message": "[hotfix] [core] Fix checkstyle for 'flink-core' : 'org.apache.flink.configuration'"}, {"url": "https://api.github.com/repos/apache/flink/commits/212ee3d430190e6e771c79a9b94fd8410675a534", "message": "[hotfix] [core] Fix checkstyle for 'flink-core' : 'org.apache.flink.util'"}, {"url": "https://api.github.com/repos/apache/flink/commits/1e6a91a3bf7e734eccdb034ce505b3775b709265", "message": "[hotfix] [core] Move 'ThrowingConsumer' and 'RunnableWithException' to proper package (.util.function)\n\nThis also adds missing stability annotations to the functional interfaces in 'util.function'."}, {"url": "https://api.github.com/repos/apache/flink/commits/edc6f1000704a492629d7bdf8cbfa5ba5c45bb1f", "message": "[FLINK-5823] [checkpoints] State backends now also handle the checkpoint metadata"}, {"url": "https://api.github.com/repos/apache/flink/commits/d19525e90e69ddd257d47371b8ea0319fa4673c8", "message": "[FLINK-5823] [checkpoints] Make RocksDB state backend configurable"}, {"url": "https://api.github.com/repos/apache/flink/commits/1931993bdc1d294a0eb9e1ad727f737cf64fe150", "message": "[hotfix] [rocksdb] Clean up RocksDB state backend code\n\n  - arrange variables to properly express configuration (client side) versus runtime (task manager side)\n  - make all runtime-only fields properly transient\n  - fix confusing variable name for local directories"}, {"url": "https://api.github.com/repos/apache/flink/commits/fa03e78d3a245b40ceb3efffeb3020853e74e48b", "message": "[FLINK-5823] [checkpoints] State backends define checkpoint and savepoint directories, improved configuration"}, {"url": "https://api.github.com/repos/apache/flink/commits/7d820d6fe17341463b2a0f9cd1cea1ef085eed21", "message": "[FLINK-5823] [checkpoints] Pass state backend to checkpoint coordinator"}, {"url": "https://api.github.com/repos/apache/flink/commits/0030d6ab21197077438ba05654a5af353bc1acb7", "message": "[hotfix] [core] Fix broken JavaDoc links in ConfigConstants"}, {"url": "https://api.github.com/repos/apache/flink/commits/e52db8bc411e93c245cc78a278854f2653e5f384", "message": "[FLINK-7925] [checkpoints] Add CheckpointingOptions\n\nThe CheckpointingOptions consolidate all checkpointing and state backend-related\nsettings that were previously split across different classes."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5313", "title": "[FLINK-8455] [core] Make 'org.apache.hadoop.' a 'parent-first' classloading pattern", "body": "## What is the purpose of the change\r\n\r\nThis change avoids duplication of Hadoop classes between the Flink runtime and the user code.\r\nHadoop (and transitively its dependencies) should be part of the application class loader.\r\nThe user code classloader is allowed to duplicate transitive dependencies, but not Hadoop's\r\nclasses directly.\r\n\r\nThis change addresses an issue that various users have reported (mainly using the BucketingSink) where they get ClassCastExceptions related to Hadoop classes.\r\n\r\nIn all cases, users had Hadoop dependencies bundled into their application jar files. To make the experience better, I suggest to let Hadoop always load its classes parent-first.\r\n\r\n## Brief change log\r\n\r\n  - Add `org.apache.hadoop.` to the parent-first patterns.\r\n  - Add some tests for the parent-first patterns.\r\n\r\n## Verifying this change\r\n\r\nThis change added self-contained tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / **no**)\r\n  - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45891773", "body": "Nice one!\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45891773/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45892412", "body": "`taskmanager.io.numInThreads` is a bit ambiguous with respect to disk I/O versus net I/O threads?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45892412/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45919574", "body": "Looks good. I will merge that for the 0.6 release.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45919574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45920114", "body": "Nice. Will merge that for 0.6\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45920114/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45962652", "body": "Merged in 856d5569c62c9a74be24750444c589ee11c4ceb0\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45962652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45963021", "body": "Merged in edf6c4c18303b30e6d0180fd45f78828966511a8\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45963021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997121", "body": "Okay, that looks good. The other flags (max perm gen size, mark sweep GC, mark sweep class unloading) are unaffected by that?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997219", "body": "When we pass the flags, we make sure that Xms and -Xmx are the same, to prevent the JVM from being funny and resizing the heap from time to time. I think we loose that with this option. But I guess its all right, on cluster setups we would expect those to be set anyways.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997480", "body": "Looks good, will merge\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997906", "body": "This is a nice change. Do you have some micro-benchmark numbers on how the methods perform in comparison to the old `writeString()` methods?\n\nAlso, it would be interesting what the benefit of doing binary comparisons is. So, a microbenchmark of sorting the strings with a normalizedKeySorter. And a case where all prefixes are equal.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997906/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998219", "body": "Also, can you add a few more strings where some of the branches get executed? Like collisions on the high byte of a code point, but differences in the low bytes (and vice versa) ?\n\nThis code needs to be absolutely robust, or it will lead to impossible-to-debug situations.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998342", "body": "Let's not do this ourselves. I think it is fine. If you want a robust setup, set the values. Our YARN scripts also set them automatically, based on the container size, afaik.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998342/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998358", "body": "I will merge this...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998358/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998419", "body": "This is merged in f952638360cf8c86e334fb5fc56342272ceb62e5\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45998419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/6755062", "body": "As a general comment: I think such blocks can also catch `Throwable`, if it is crucial that they never fail and return just \"unknown\" in the worst case.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/6755062/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/7998721", "body": "Doesn't this fail the hadoop 2.5.0 builds?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/7998721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8118729", "body": "I agree. Must have been accidentally. Is that the reason for the shutdown hang?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8118729/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8201282", "body": "An iterator in scala allows \"for comprehensions\", correct?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8201282/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8554785", "body": "Why is there a `4` as a parameter here and nowhere else?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8554785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8554795", "body": "Should this be a `4` or no argument at all?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8554795/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9381873", "body": "This seems a bit wrong here. The caller calls the `next()` function, which creates an element (potentially large) to pass it to the `next(T reuse)` function, which in turn ignores the value.\n\nI would move the logic to the `next()` function and have the `next(T reuse)` function simply call the `next()` function, ignoring any reusable element.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9381873/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9381927", "body": "Similar as for the inline comment above, I would switch the logic between `next()` and `next(T reuse)` here to prevent creating unnecessary and ignored objects\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9381927/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9582468", "body": "I think it would actually be fine to throw an error in that case and abort the start. There is obviously something very wrong, and this is a good point to loudly mention it, rather than having people wonder about it later why the JVMs are so small...\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9582468/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/12961159", "body": "Would be good if that one was moved to `flink-core` as well, if the class it tests was moved to `flink-core`.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12961159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/12961618", "body": "If the method is only to draw a serialization copy, we could also add that to the `InstantiationUtil` or use `commons-lang`'s `SerializationUtil` class.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12961618/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "NicoK": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/03c797a1d279a6c09c988755f94566a3d46cbb17", "message": "[FLINK-7518][network] pass our own NetworkBuffer to netty\n\nThis is using a composite buffer to assemble header+content and avoids an\nunnecessary buffer copy from our (Network)Buffer class backed by a MemorySegment\nto Netty's ByteBuf class.\n\nThis closes #4615."}, {"url": "https://api.github.com/repos/apache/flink/commits/905d98e37c015d24d097113b6e6e8194bd88cbd8", "message": "[hotfix][network][tests] remove Mockito mocks in RecordWriterTest"}, {"url": "https://api.github.com/repos/apache/flink/commits/758336553248d2414a88b071ebd5748c1c32d282", "message": "[hotfix][tests] separate tests inside SlotCountExceedingParallelismTest\n\nThis way, in case of failures, we may have better pointers on what test code is\ncurrently executed."}, {"url": "https://api.github.com/repos/apache/flink/commits/665347cf867f0b807fe9d4787b5ef42b0581b510", "message": "[FLINK-8395][network] add a read-only sliced ByteBuf implementation based on NetworkBuffer\n\nThis closes #5288."}, {"url": "https://api.github.com/repos/apache/flink/commits/db440f2434423a23207ba666b33f4ccb55adede5", "message": "[hotfix][network] rename Buffer#retain() and #recycle in preparation for FLINK-8396 and FLINK-8395\n\nSince these two methods also exist in Netty's ByteBuf, we would otherwise get\ninto overloading conflicts.\n\nAlso add Buffer#readableBytes() and Buffer#setAllocator()."}, {"url": "https://api.github.com/repos/apache/flink/commits/9d0dfcba639a206fb7bf06df3b6af48719794d5d", "message": "[hotfix][network] clarify BufferResponse#size() uses (by removing it)\n\nThis field was only used by the code paths on the receiver and was inconsistent\nwith what was added on the sending side. We should use the contained buffer's\nreadableBytes() instead, depending on the actual use case.\n\nThis closes #4613."}, {"url": "https://api.github.com/repos/apache/flink/commits/435930164f32199f345d01d7094647755bc5f455", "message": "[hotfix][io] remove duplicate code between SynchronousBufferFileReader and BufferReadRequest"}, {"url": "https://api.github.com/repos/apache/flink/commits/85bea23ace3133e1b2d239c4ee87270a201b9b6a", "message": "[FLINK-7520][network] let our Buffer class extend from netty's buffer class\n\nFor this, use a common (flink) Buffer interface and an implementation\n(NetworkBuffer) that implements netty's buffer methods as well. In the future,\nwith this, we are able to avoid unnecessary buffer copies when handing buffers\nover to netty while keeping our MemorySegment logic and configuration.\n\nFor the netty-specific part, the NetworkBuffer also requires a ByteBuf allocator\nwhich is otherwise not needed in our use cases, so if the buffer is handed over\nto netty, it requires a byte buffer allocator to be set."}, {"url": "https://api.github.com/repos/apache/flink/commits/1a5a355a873d88d1fe1903503d81140918e0e07e", "message": "[hotfix][tests] replace InputChannelTestUtils#createMockBuffer() with TestBufferFactory#createBuffer()\n\nThis eliminates one more unnecessary buffer mock."}, {"url": "https://api.github.com/repos/apache/flink/commits/b3fc79392343ff1ba364254b194ec70d2bf43dc0", "message": "[hotfix][tests] make SpillableSubpartitionTest use TestBufferFactory.createBuffer\n\n(this simplifies the test setups)"}, {"url": "https://api.github.com/repos/apache/flink/commits/997fab6247a0d0216a69b698ed049656aa358535", "message": "[hotfix][tests] do not use a mocked BufferRecycler for unpooled memory segments\n\nThe mock will actually keep references to the segments instead of freeing them."}, {"url": "https://api.github.com/repos/apache/flink/commits/705ba2e88632c1ca909cd5b8ebd646ba299c994e", "message": "[hotfix][tests] replace DiscardingRecycler with FreeingBufferRecycler"}, {"url": "https://api.github.com/repos/apache/flink/commits/76abcaa55d0d6ab704b7ab8164718e8e2dcae2c4", "message": "[FLINK-8350][config] replace \"taskmanager.tmp.dirs\" with \"io.tmp.dirs\"\n\nThis replaces \"taskmanager.tmp.dirs\" with the new \"io.tmp.dirs\"\nconfiguration parameter to define temporary directories in (cluster)\nenvironments for all components, i.e. JobManager, JobMaster, Dispatcher,...\n\nPlease note that this (kind of internal and thus undocumented) configuration\nparameter is set by our YARN and Mesos integrations.\n\n[FLINK-8350][cluster] initialise \"io.tmp.dirs\" for JobManager as well\n\nIn a YARN and Mesos environment, this initialises Flink's temporary directory\nconfiguration with YARN/Mesos application-specific paths for JobManager,\nJobMaster, Dispatcher, etc. components as well (Mesos integration actually still\nlacks a proper integration of this, but once done, the new hooks fall in place\njust fine)."}, {"url": "https://api.github.com/repos/apache/flink/commits/46ed5e31585499cd0f0b4bb0718460ba47dcb926", "message": "[hotfix] replace misuse of ConfigConstants.DEFAULT_TASK_MANAGER_TMP_PATH as a temporary folder in unit tests\n\n-> use JUnit's TemporaryFolder instead."}, {"url": "https://api.github.com/repos/apache/flink/commits/a7c407ace4f6cbfbde3e247071cee5a755ae66db", "message": "[FLINK-8279][blob] fall back to TaskManager temp directories first\n\nInstead of falling back to java.io.tmpdir directly, the BLOB server and cache\nprocesses fall back to the TaskManager temp directories (given by\nConfigConstants#TASK_MANAGER_TMP_DIR_KEY) directly, before falling back\nto ConfigConstants#DEFAULT_TASK_MANAGER_TMP_PATH (set to java.io.tmpdir).\n\nIn a Mesos/YARN environment, this means that we will use the designated temp\ndirectories for our jobs instead of the system-wide java.io.tmpdir. These\ndirectories may also offer some more space.\n\nThis closes #5176."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zentol": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/b1df177045e24bcb6f063e0b2fdb6dcebad88128", "message": "Revert \"[hotfix][docs] Mention maven dependency for RocksDB state backend\"\n\nThis reverts commit 5623ac66bd145d52f3488ac2fff9dbc762d0bda1."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45841376", "body": "I reworked the serializer/comparator again. It now uses the first bit of every byte to indicate whether there is at least one more byte coming. \n\nThis has the bonus that all letters are serialized as one byte (opposed to the previous version which could only do this for the numbers and a few special characters (which actually made the variable length encoding pointless...))\n\n~~I currently do a selective shift starting on the flag positions to make space for them. I wonder if there is a more efficient way to do that, here's an example how i do it:~~\n\n~~char to send:~~\n~~`0010 0110 1111 1001`~~\n~~1) move the lowest 7 bits into a tmp variable (by doing & with 0000 0000 0111 1111)~~\n~~`0000 0000 0111 1001`~~\n~~2) shift char to the right by 7 positions to omit the lower part~~\n~~`0000 0000 0100 1101`~~\n~~3) shift char to the right by 8 positions (finalizing the shifting of the upper part)~~\n~~`0100 1101 0000 0000`~~\n~~4) char |= tmp~~\n~~`0100 1100 0111 1001`~~\n~~(this would be done resursively for every flag position needed, starting from the right, so up to 3 times)~~\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45841376/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46012075", "body": "the new serialization seems to take longer, but due to faster deserialization in total it breaks even.\n\nsome comparison benchmarks:\n\nstrings of length 90\n1000 repetitions\ncode for New measurement:\n\n```\nlong startTime = System.nanoTime();\ncmp = StringValue.compareUnicode(in1, in2);\nlong endTime = System.nanoTime();\n```\n\ncode for Old measurement:\n\n```\nlong startTime = System.nanoTime();\ncmp = StringValue.readString(in1).compareTo(StringValue.readString(in2));\nlong endTime = System.nanoTime();\n```\n\nresult = SUM(endTime - startTime) / 1000\n\nequality\nNew 26627\nOld 25782\n\naffix (difference in the beginning of the sring)\nNew 4259\nOld 23431\n\ninfix (difference in the middle of the string)\nNew 13560\nOld 30757\n\nsuffix (difference at the end of the string)\nNew 29385\nOld 28293\n\nnot particularly surprising results. no progress on the prefix-issue :/\n\nadding some more tests now.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46012075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "yew1eb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/988bdde17fec8896aae4bc041a10d5e30a4cb702", "message": "[FLINK-7777][build] Bump japicmp to 0.11.0\n\nThis closes #5302."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kl0u": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/7da32d19f9623ca98c8a4ba76e7c406bf9318d4d", "message": "[FLINK-8049] [rest] REST client reports netty exceptions on shutdown.\n\nThis closes #5057."}, {"url": "https://api.github.com/repos/apache/flink/commits/642e11a9cd31c83fbbabe860871c714f3d4ca981", "message": "[FLINK-8050] [rest] REST server reports netty exceptions on shutdown."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bowenli86": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/flink/commits/907361d862c77a70ff60d27e7fcc13647eac0e6d", "message": "[FLINK-8175] remove flink-streaming-contrib and migrate its classes to flink-streaming-java/scala\n\nupdate doc\n\nmove classes to /experimental\n\nupdate license header\n\nreorg scala class level\n\nenforce stylecheck and change API annotation\n\nThis closes #5112."}], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5301", "title": "[FLINK-8267] [Kinesis Connector] update Kinesis Producer example for setting Region key", "body": "## What is the purpose of the change\r\n\r\nUpdate doc to guide users to use KPL's native keys to set regions. \r\n\r\nThis originates from a bug that we forgot to set region keys explicitly for kinesis connector, which has been fixed. According to the previous discussion between @tzulitai and I, we want to remove AWSConfigConstants in 2.0 because it basically copies/translates config keys of KPL/KCL, which doesn't add much value. \r\n\r\nGuide users to use KPL's native keys to set regions can be the first step that facilitates the migration.\r\n\r\n## Brief change log\r\n\r\n- updated doc\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n\r\n\r\ncc @tzulitai ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5300", "title": "[FLINK-8411] [State Backends] HeapListState#add(null) will wipe out entire list state", "body": "## What is the purpose of the change\r\n\r\n`HeapListState#add(null)` will result in the whole state being cleared or wiped out. There's never a unit test for `List#add(null)` in `StateBackendTestBase`\r\n\r\n## Brief change log\r\n\r\n- changed ListState impls such that `add(null)` will be explicitly ignored\r\n- added unit tests to test `add(null)`\r\n- updated javaDoc\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as `StateBackendTestBase`.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (JavaDocs)\r\n\r\n\r\nNote! **This work depends on FLINK-7983 and its PR at https://github.com/apache/flink/pull/5281**\r\n\r\ncc @StefanRRichter ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5281", "title": "[FLINK-7938] [State Backend] support addAll() in ListState", "body": "## What is the purpose of the change\r\n\r\nsupport `addAll()` in `ListState`, so Flink can be more efficient in adding elements to `ListState` in batch. This should give us a much better performance especially for `ListState` backed by RocksDB\r\n\r\n## Brief change log\r\n\r\n- added `addAll()` to ListState\r\n- cleaned and fixed some leftovers comments from the https://github.com/apache/flink/pull/4963\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as `testListStateAPIs()`.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (docs, JavaDocs)\r\n\r\n\r\ncc @StefanRRichter ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "GJL": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5312", "title": "[FLINK-8344][WIP] Add support for HA to RestClusterClient", "body": "WIP", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tillrohrmann": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5311", "title": "[FLINK-8454] [flip6] Remove JobExecutionResultCache from Dispatcher", "body": "## What is the purpose of the change\r\n\r\nWith the introduction of the SerializableExecutionGraphStore to the Dispatcher,\r\nit is no longer necessary to store the JobResult separately. In order to\r\ndecrease complexity and state duplication, this commit removes the\r\nJobExecutionResultCache and instead uses the SerializableExecutionGraphStore\r\nto serve completed job information. A side effect of this change is that the\r\nJobExecutionResult is now available as long as the completed Flink job is stored\r\nin the SerializableExecutionGraphStore.\r\n\r\nThis PR is based on #5310.\r\n\r\n## Brief change log\r\n\r\n- Replace information served from `JobExecutionResultCache` with information from `SerializableExecutionGraphStore`\r\n- Adapt `JobExecutionResultHandler`\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5310", "title": "[FLINK-8453] [flip6] Add SerializableExecutionGraphStore to Dispatcher", "body": "## What is the purpose of the change\r\n\r\nThe SerializableExecutionGraphStore is responsible for storing completed jobs\r\nfor historic job requests (e.g. from the web ui or from the client). The store\r\nis populated by the Dispatcher once a job has terminated.\r\n\r\nThe FileSerializableExecutionGraphStore implementation persists all\r\nSerializableExecutionGraphs on disk in order to avoid OOM problems. It only keeps\r\nsome of the stored graphs in memory until it reaches a configurable size. Once\r\ncoming close to this size, it will evict the elements and only reload them if\r\nrequested again. Additionally, the FileSerializableExecutionGraphStore defines\r\nan expiration time after which the execution graphs will be removed from disk.\r\nThis prevents excessive use of disk resources.\r\n\r\nThis PR is based on #5309.\r\n\r\n## Brief change log\r\n\r\n- Introduce `SerializableExecutionGraphStore` and `FileSerializableExecutionGraphStore`\r\n- Add `FileSerializableExecutionGraphStore` to `Dispatcher`\r\n- Store `SerializableExecutionGraphs` in corresponding `FileSerializableExecutionGraphStore`\r\n- Adapt `Dispatcher` to serve requests for historic jobs\r\n\r\n## Verifying this change\r\n\r\n- Added `FileSerializableExecutionGraphStoreTest`\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n\r\ncc @GJL ", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5309", "title": "[FLINK-8450] [flip6] Make JobMaster/DispatcherGateway#requestJob type safe", "body": "## What is the purpose of the change\r\n\r\nLet JobMasterGateway#requestJob and DispatcherGateway#requestJob return a\r\nCompletableFuture<SerializableExecutionGraph> instead of a\r\nCompletableFuture<AccessExecutionGraph>. In order to support the old code\r\nand the JobManagerGateway implementation we have to keep the return type\r\nin RestfulGateway. Once the old code has been removed, we should change\r\nthis as well.\r\n\r\n## Brief change log\r\n\r\n- Change return type of `RestfulGateway#requestJob` to `CompletableFuture<? extends AccessExecutionGraph>`\r\n- Change return type of `JobMasterGateway#requestJob` and `DispatcherGateway#requestJob` to `CompletableFuture<SerializableExecutionGraph>`\r\n- Introduce `TestingRestfulGateway` as a testing utility\r\n- Adapt `ExecutionGraphCacheTest` to use `TestingRestfulGateway` instead of Mockito mocks\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5308", "title": "[FLINK-8449] [flip6] Extend OnCompletionActions to accept an SerializableExecutionGraph", "body": "## What is the purpose of the change\r\n\r\nThis commit introduces the SerializableExecutionGraph which extends the\r\nAccessExecutionGraph and adds serializability to it. Moreover, this commit\r\nchanges the OnCompletionActions interface such that it accepts a\r\nSerializableExecutionGraph instead of a plain JobResult. This allows to\r\narchive the completed ExecutionGraph for further usage in the container\r\ncomponent of the JobMasterRunner.\r\n\r\n## Brief change log\r\n\r\n- Introduce `SerializableExecutionGraph`\r\n- Introduce `DummyExecutionGraph` for testing purposes\r\n- Change `OnCompletionActions` to accept a `SerializableExecutionGraph`\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as `DispatcherTest`.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "eastcirclek": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5307", "title": "[FLINK-8431] [mesos] Allow to specify # GPUs for TaskManager in Mesos", "body": "## What is the purpose of the change\r\n\r\nThis PR introduces a new configuration property named \"mesos.resourcemanager.tasks.gpus\"  to allow users to specify # of GPUs for each TaskManager process in Mesos. The configuration property is necessary because TaskManagers that do not specify to use GPUs cannot see GPUs at all when Mesos agents are configured to isolate GPUs as shown in [1].\r\n\r\n[1] http://mesos.apache.org/documentation/latest/gpu-support/#agent-flags\r\n\r\n## Brief change log\r\n\r\n* Modify MesosTaskManagerParameters instead of ContaineredTaskManagerParameters to confine this problem to Mesos\r\n* Augment data types: (1) offers from Mesos and (2) task requests of Mesos frameworks \r\n* Add GPU_RESOURCES to the list of framework capabilities if \"mesos.resourcemanager.tasks.gpus\" > 0. Otherwise, LaunchCoordinator gets no offers from Mesos masters that are configured to prevent Mesos frameworks without GPU_RESOURCES from being given resources offers of GPU-equipped agents.\r\n\r\n## Verifying this change\r\n\r\nI tested it by launching a standalone Flink cluster using ./bin/mesos-appmaster.sh. I tested the following scenarios with Mesos configured with --filter_gpu_resources.\r\n\r\n* When mesos.resourcemanager.tasks.gpus is not specified or is set to 0.0\r\nLaunchCoordinator isn't given any offer because MesosFlinkResourceManager does not enable GPU_RESOURCES capability when mesos.resourcemanager.tasks.gpus is not specified or it is set to 0.\r\n* When mesos.resourcemanager.tasks.gpus is smaller than or equal to the available GPUs on a node \r\nGiven offers, LaunchCoordinator aggregates offers of different roles from the same node and puts aggregated offers to Fenzo for scheduling resources over nodes. When notified of the success of scheduling from Fenzo, LaunchCoordinator allocates resources of different roles to tasks and then populate Protos.TaskInfo using the allocated resources which is then wired to the Mesos master.\r\n* When mesos.resourcemanager.tasks.gpus is bigger than the available GPUs on a node \r\nGiven offers, LaunchCoordinator aggregates offers of different roles from the same node and puts aggregated offers to Fenzo. However, Fenzo notifies LaunchCoordinator of the failure of scheduling with the following messages:\r\n    AssignmentFailure {resource=Other, asking=3.0, used=0.0, available=2.0, message=gpus}.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): yes, it includes an upgrade (Fenzo)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: yes (JobManager and TaskManager on Mesos)\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? no\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "suez1224": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5306", "title": "[FLINK-8390][security]remove unused integration test code", "body": "## What is the purpose of the change\r\n\r\nRemove unused integration test code from the main code.\r\n\r\n\r\n## Brief change log\r\n  - Remove unused integration test code\r\n\r\n\r\n## Verifying this change\r\n  - Run RollingSinkSecuredITCase, YARNSessionFIFOSecuredITCase, Kafka09SecuredRunITCase, and all IT cases pass.\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: ( no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)\r\n  - The S3 file system connector: ( no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? ( no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5274", "title": "[FLINK-8401][Cassandra Connector]Refactor CassandraOutputFormat to allow subclass to customize the fai\u2026", "body": "## What is the purpose of the change\r\n\r\nRefactor CassandraOutputFormat to allow subclass to customize the failure handling logic.\r\n\r\n\r\n## Brief change log\r\n  - Added to protected methods for handling cassandra write result, these allow subclass to override them to customize.\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5272", "title": "[Flink-8397][Connectors]Support Row type for Cassandra OutputFormat", "body": "\r\n## Brief change log\r\n  - Add CassandraOutputFormatBase\r\n  - Add CassandraRowOutputFormat\r\n  - Add CassandraTupleOutputFormat\r\n\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified by CassandraConnectorITCase.{testCassandraBatchTupleFormats, testCassandraBatchRowFormats}\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: ( no)\r\n  - The serializers: ( / )\r\n  - The runtime per-record code paths (performance sensitive): (no \r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no \r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes \r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nicktoker": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5305", "title": "Release 1.3", "body": "when asyncIO have timeout all operator fail and break the all stream\r\nneed to add option to control when timeout  happen and choose  what to do in this case\r\nignore this stream element and continue to next one ........ or beak the flow\r\none timeout don't need break the stream\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maqingxiang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5304", "title": "[FLINK-8290]Modify clientId to groupId in flink-connector-kafka-0.8", "body": "Now the Clientid that consumes the all topics are constant(\"flink-kafka-consumer-legacy-\" + broker.id()), and it is not easy for us to look at kafka's log, so I recommend that it be modified to groupid.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elbaulp": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5299", "title": "[Hotfix][Doc][DataStream API] Fix Scala code example in Controlling Latency section", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhangminglei": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5298", "title": "[FLINK-8433] [doc] Remove ununsed CheckpointedRestoring interface", "body": "## What is the purpose of the change\r\n\r\n*Update the document, as the ```CheckpointedRestoring``` interface introduced in the doc is unused since flink 1.4.*\r\n\r\n\r\n## Brief change log\r\n\r\n  - *Update the ```state.md``` file.*\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): ( no )\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: ( no)\r\n  - The S3 file system connector: ( no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (no)\r\n  - If yes, how is the feature documented? (docs / JavaDocs)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shuai-xu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5297", "title": "[FLINK-8434] Take over the running task manager after yarn app master failvoer", "body": "\r\n## What is the purpose of the change\r\n\r\n*This pull request makes the yarn resource manager could take over the running container from previous attempt.*\r\n\r\n## Verifying this change\r\n\r\nThis change is tested manually.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? ( no)\r\n  - If yes, how is the feature documented? (not applicable)\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jelmerk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5296", "title": "[FLINK-8432] [filesystem-connector] Add support for openstack's swift filesystem", "body": "## What is the purpose of the change\r\n\r\nAdd support for OpenStack's cloud storage solution without Hadoop dependencies\r\n\r\n## Brief change log\r\n\r\n- Added new module below flink-filesystems\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n- Added integration tests for simple reading and writing and listing directories\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): no\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: yes\r\n  - The serializers: don't know\r\n  - The runtime per-record code paths (performance sensitive): don't know\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: don't know\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? yes\r\n  - If yes, how is the feature documented? not documented\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dyanarose": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5295", "title": "[FLINK-8384] [streaming] Session Window Assigner with Dynamic Gaps", "body": "## What is the purpose of the change\r\n\r\nThis PR adds the ability for the Session Window assigners to to have dynamic inactivity gaps in addition to the existing static inactivity gaps.\r\n\r\n**Behaviour of dynamic gaps within existing sessions:**\r\n- scenario 1 - the new timeout is prior to the old timeout. The old timeout (the furthest in the future) is respected.\r\n- scenario 2 - the new timeout is after the old timeout. The new timeout is respected.\r\n- scenario 3 - a session is in flight, a new timeout is calculated, however no new events arrive for that session after the new timeout is calculated. This session will not have its timeout changed\r\n\r\n\r\n## Brief change log\r\n\r\n**What's New**\r\n-  SessionWindowTimeGapExtractor\\<T\\> - Generic Interface with one extract method that returns the time gap\r\n- DynamicEventTimeSessionWindows\\<T\\> - Generic event time session window\r\n- DynamicProcessingTimeSessionWindows\\<T\\> - Generic processing time session window\r\n- TypedEventTimeTrigger\\<T\\> - Generic event time trigger\r\n- TypedProcessingTimeTrigger\\<T\\> - Generic processing time trigger\r\n- Tests for all the above\r\n\r\n## Verifying this change\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n - added tests for the typed triggers that duplicate the existing trigger tests to prove parity\r\n - added unit tests for the dynamic session window assigners that mimic the existing static session window assigner tests to prove parity in the static case\r\n - added tests to the WindowOperatorTest class to prove the behaviour of changing inactivity gaps\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no, though the two typed trigger classes are marked `@Public(Evolving)`)\r\n  - The serializers: (no)\r\n  - The runtime per-record code paths (performance sensitive): (no)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)\r\n  - The S3 file system connector: (no)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes)\r\n  - If yes, how is the feature documented? (docs && JavaDocs)\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "greghogan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5292", "title": "[FLINK-8422] [core] Checkstyle for org.apache.flink.api.java.tuple", "body": "## What is the purpose of the change\r\n\r\nUpdate TupleGenerator for Flink's checkstyle and rebuild Tuple and TupleBuilder classes.\r\n\r\n## Brief change log\r\n\r\n`TupleGenerator` has been updated to write Flink-checkstyle compliant code.\r\n\r\nThe following non-generated files were manually updated: `Tuple`, `Tuple0`, `Tuple0Builder`, `Tuple2Test`\r\n\r\n`org.apache.flink.api.java.tuple` was removed from the checkstyle suppressions for `flink-core`.\r\n\r\n## Verifying this change\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage. Re-running `TupleGenerator` should replicate the newly updated files.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5291", "title": "[FLINK-8361] [build] Remove create_release_files.sh", "body": "## What is the purpose of the change\r\n\r\nThe monolithic create_release_files.sh does not support building without Hadoop and has been superseded by the scripts in tools/releasing.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5279", "title": "[hotfix] [build] Print cache info", "body": "Print the size of the Maven cache copied for each TravisCI job.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "eskabetxe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5290", "title": "[Flink-8424][Cassandra Connector] upgrade of cassandra version and driver to latest", "body": "## What is the purpose of the change\r\n\r\nUpdate Cassandra version and driver to something new,\r\nthe driver is current in 3 minor versions behind\r\nthe version is 1 major version behind\r\n\r\n## Verifying this change\r\n\r\nThis change is already covered by existing tests, such as *(please describe tests)*.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): yes\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no\r\n  - The serializers: no\r\n  - The runtime per-record code paths (performance sensitive): no\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no\r\n  - The S3 file system connector: no\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature?  no\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tzulitai": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5282", "title": "[FLINK-6352] [kafka] Timestamp-based offset configuration for FlinkKafkaConsumer", "body": "## What is the purpose of the change\r\n\r\nThis PR is based on @zjureel's initial efforts on the feature in #3915.\r\n\r\nThis version mainly differs in that:\r\n- When using timestamps to define the offset, the actual offset is eagerly determined in the `FlinkKafkaConsumerBase` class.\r\n- The `setStartFromTimestamp` configuration method is defined in the `FlinkKafkaConsumerBase` class, with `protected` access. Kafka versions which support the functionality should override the method with `public` access.\r\n- Timestamp is configured simply as a long value, and not a Java `Date`.\r\n\r\n**Overall, the usage of the feature is as follows:**\r\n```\r\nFlinkKafkaConsumer011<String> consumer = new FlinkKafkaConsumer011<>(...);\r\nconsumer.setStartFromTimestamp(1515671654453L);\r\n\r\nDataStream<String> stream = env.addSource(consumer);\r\n...\r\n```\r\n\r\nOnly versions 0.10 and 0.11 supports this feature.\r\n\r\n**Semantics:**\r\n- The provided timestamp cannot be larger than the current timestamp.\r\n- For a partition, the earliest record which `record timestamp >= provided timestamp` is used as the starting offset.\r\n- If the provided timestamp is larger than the latest record in a partition, that partition will simply be read from the head.\r\n- For all new partitions that are discovered after the initial startup (due to scaling out Kafka), they are all read from the earliest possible record and the provided timestamp is not used.\r\n\r\n## Brief change log\r\n\r\n- d012826 @zjureel's initial efforts on the feature.\r\n- 7ac07e8 Instead of lazily determining exact offsets for timestamp-based startup, the offsets are determined eagerly in `FlinkKafkaConsumerBase`. This commit also refactors the `setStartFromTimestamp` method to live in the base class.\r\n- 32d46ef Change to just use long values to define timestamps, instead of using Java `Date`\r\n- 7bb44a8 General improvement for the `runStartFromTimestamp` integration test.\r\n\r\n## Verifying this change\r\n\r\nNew integration tests `Kafka010ITCase::testStartFromTimestamp` and `Kafka011ITCase::testStartFromTimestamp` verifies this new feature.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (**yes** / no)\r\n  - If yes, how is the feature documented? (not applicable / **docs** / **JavaDocs** / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5269", "title": "[FLINK-6004] [kinesis] Allow FlinkKinesisConsumer to skip non-deserializable records", "body": "## What is the purpose of the change\r\n\r\nThis PR is based on #5268, which includes fixes to harden Kinesis unit tests. Only the last commit is relevant.\r\n\r\nIn the past, we allowed the Flink Kafka Consumer to skip corrupted Kafka records which cannot be deserialized. In reality pipelines, it is entirely normal that this could happen.\r\n\r\nThis PR adds this functionality to the Flink Kinesis Consumer also.\r\n\r\n## Brief change log\r\n\r\n- Clarify in Javadoc of `KinesisDeserializationSchema` that `null` can be returned if a message cannot be deserialized.\r\n- If `record` is `null` in `KinesisDataFetcher::emitRecordAndUpdateState(...)`, do not collect any output for the record.\r\n- Add `KinesisDataFetcherTest::testSkipCorruptedRecord()` to verify feature.\r\n\r\n## Verifying this change\r\n\r\nAdditional `KinesisDataFetcherTest::testSkipCorruptedRecord()` test verifies this change.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (**yes** / no / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (**yes** / no)\r\n  - If yes, how is the feature documented? (not applicable / docs / **JavaDocs** / not documented)\r\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/5268", "title": "[FLINK-8398] [kinesis, tests] Harden KinesisDataFetcherTest unit tests", "body": "## What is the purpose of the change\r\n\r\nPrior to this PR, many of the `KinesisDataFetcherTest` unit tests relied on thread sleeps to wait until a certain operation occurs to allow the test to pass. This test behaviour is very flaky, and should be replaced with `OneShotLatch`.\r\n\r\n## Brief change log\r\n\r\n- 94b4591: Several minor cleanups of confusing implementations / code smells in the `KinesisDataFetcherTest` and related test classes. The commit message explains what exactly was changed.\r\n- 547d19f: Remove thread sleeps in unit tests, and replace them with `OneShotLatch`.\r\n\r\n\r\n## Verifying this change\r\n\r\nNo test coverage should have been affected by this change.\r\nThe existing tests in `KinesisDataFetcherTest` verifies this.\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / **no**)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / **no**)\r\n  - The serializers: (yes / **no** / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / **no** / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / **no** / don't know)\r\n  - The S3 file system connector: (yes / **no** / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / **no**)\r\n  - If yes, how is the feature documented? (**not applicable** / docs / JavaDocs / not documented)\r\n\r\n  ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChrisChinchilla": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5277", "title": "[hotfix][docs]Review to reduce passive voice, improve grammar and formatting", "body": "Small review of the runtime concept doc to reduce passive voice, reduce future tense, improve grammar and formatting. Let me know if it needs backporting to any other branch or if there are any other issues", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/152426709", "body": "@greghogan Not sure, I have found a lot of inconsistent headings in the docs, so it seemed worthwhile adding, doesn't mean anyone will follow it of course :)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/152426709/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/152426833", "body": "@greghogan It's something my Markdown linter 'fixes', it doesn't matter too much, but is apparently more correct.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/152426833/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/152427259", "body": "@greghogan No, this was an attempt to make the whole passage flow better as it says\u2026\r\n\r\n> xxx and yyy or zzz\r\n\r\nSo I was attempting to seperate out the two parts of the paragraph, better may be\u2026\r\n\r\n> **Programming Guides**: You can read our guides about [basic API concepts](dev/api_concepts.html), the [DataStream API](dev/datastream_api.html), and the [DataSet API](dev/batch/index.html) to learn how to write your first Flink programs.\r\n\r\nThen it's an oxford comma, but that's stylistic.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/152427259/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/153033430", "body": "@StefanRRichter I'm struggling to understand this sentence.\r\n\r\n> As long as we have the previous checkpoint and the state changes for the current checkpoint, we can restore the full, current state for the job.\r\n\r\nDo you mean\u2026\r\n\r\n> As long as we have the previous checkpoint, if the state changes for the current checkpoint, we can restore the full, current state for the job.\r\n\r\nOr something different? Explain to me what you're trying to say :)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/153033430/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/153034462", "body": "@StefanRRichter Is the whole section\u2026\r\n\r\n> We present more detail about this in the next section\u2026\r\n\r\nUntil the end of the paragraph needed? If you're about to cover it below anyway, why mention it? Or is there anything here that isn't mentioned in the \r\n\r\n> ### Incremental Checkpoints in Flink\r\n\r\nsection?", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/153034462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/153374312", "body": "@greghogan I'm personally not a fan of hard line breaks, but happy to stick to them, but I'm struggling to figure out what the character limit for the project is, I can't see anything in any style files and it's somewhat varied throughout the docs. If there's a solid number then I'll add to the contributors guide.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/153374312/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/153374829", "body": "@greghogan With the full stop I don't think it's needed anymore.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/153374829/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "paulzwu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5247", "title": "[FLINK-8356] Need to add the commit in flush() in JDBCOutputFormat", "body": "*Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.*\r\n\r\n*Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.*\r\n\r\n## Contribution Checklist\r\n\r\n  - Make sure that the pull request corresponds to a [JIRA issue](https://issues.apache.org/jira/projects/FLINK/issues). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.\r\n  \r\n  - Name the pull request in the form \"[FLINK-XXXX] [component] Title of the pull request\", where *FLINK-XXXX* should be replaced by the actual issue number. Skip *component* if you are unsure about which is the best component.\r\n  Typo fixes that have no associated JIRA issue should be named following this pattern: `[hotfix] [docs] Fix typo in event time introduction` or `[hotfix] [javadocs] Expand JavaDoc for PuncuatedWatermarkGenerator`.\r\n\r\n  - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.\r\n  \r\n  - Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following [this guide](http://flink.apache.org/contribute-code.html#best-practices).\r\n\r\n  - Each pull request should address only one issue, not mix up code from multiple issues.\r\n  \r\n  - Each commit in the pull request has a meaningful commit message (including the JIRA id)\r\n\r\n  - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.\r\n\r\n\r\n**(The sections below can be removed for hotfixes of typos)**\r\n\r\n## What is the purpose of the change\r\n\r\n*(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)*\r\n\r\n\r\n## Brief change log\r\n\r\n*(for example:)*\r\n  - *The TaskInfo is stored in the blob store on job creation time as a persistent artifact*\r\n  - *Deployments RPC transmits only the blob storage reference*\r\n  - *TaskManagers retrieve the TaskInfo from the blob cache*\r\n\r\n\r\n## Verifying this change\r\n\r\n*(Please pick either of the following options)*\r\n\r\nThis change is a trivial rework / code cleanup without any test coverage.\r\n\r\n*(or)*\r\n\r\nThis change is already covered by existing tests, such as *(please describe tests)*.\r\n\r\n*(or)*\r\n\r\nThis change added tests and can be verified as follows:\r\n\r\n*(example:)*\r\n  - *Added integration tests for end-to-end deployment with large payloads (100MB)*\r\n  - *Extended integration test for recovery after master (JobManager) failure*\r\n  - *Added test that validates that TaskInfo is transferred only once across recoveries*\r\n  - *Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.*\r\n\r\n## Does this pull request potentially affect one of the following parts:\r\n\r\n  - Dependencies (does it add or upgrade a dependency): (yes / no)\r\n  - The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)\r\n  - The serializers: (yes / no / don't know)\r\n  - The runtime per-record code paths (performance sensitive): (yes / no / don't know)\r\n  - Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don't know)\r\n  - The S3 file system connector: (yes / no / don't know)\r\n\r\n## Documentation\r\n\r\n  - Does this pull request introduce a new feature? (yes / no)\r\n  - If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hequn8128": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/flink/pulls/5244", "title": "[FLINK-8366] [table] Use Row instead of String as key when process upsert results", "body": "\r\n## What is the purpose of the change\r\n\r\nThis pr fix the bug in `TableSinkITCase.upsertResults()`. In `upsertResults()` function, we use String as key to upsert results. This will make (1,11) and (11,1) have the same key (i.e., 111).\r\n\r\n\r\n## Brief change log\r\n\r\n  - Use Row instead of String to avoid the String problem.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rmetzger": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45640561", "body": "Travis build (in my account) https://travis-ci.org/rmetzger/incubator-flink/builds/27237521\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45640561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45661714", "body": "merged in c0e76bc0cc296b23df98491ea730a73b43577ddf\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45661714/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949247", "body": "Merged into master and release-0.5.2.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949247/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949335", "body": "Merged into master and 0.5.1.\n\nPlease close the pull request (only the author of the PR can close it)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949389", "body": "Merged into master and 0.5.1.\nPlease close the pull request.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949389/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949498", "body": "Thank you.\nMerged into master and 0.5.1.\n\nPlease close the pull request (I can not do it)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45949498/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46041318", "body": "Thank you for the pull request.\nI spend quite some time today to debug it and to get it to run. Everything seems fine, except for one thing: I can not get a plan preview for the Java K-Means example.\nI don't know if I'm doing anything wrong, and I spend quite some time with the debugger. Our system is able to generate a JSON plan, its also sent it to the browser, but still, the plan does not show up.\n![jonathan](https://cloud.githubusercontent.com/assets/89049/3273402/e23855d4-f323-11e3-9df8-b6abaf7ad599.png)\nThere is no JavaScript error. The standalone tool is able to visualize it (the file that is also located in the file system).\nCan you try and see if you can reproduce it?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46041318/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086997", "body": "Cool. Good catch.\nI think we have already a JSON parser in the Maven dependencies (\"jackson-core-asl\"), can you add a test case for the `PlanJSONDumpGenerator` that verifies that the JSON is valid (without parse errors).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086997/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/6757204", "body": "You are right. I'll change this with my next pull request.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/6757204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8117401", "body": "@StephanEwen : Is there a reason why you removed this code-block?\nI think its a good idea to stop the web interface's Jetty-server?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8117401/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8118793", "body": "Ok. My latest push contains a fix for the accidential removal.\n\nThe shutdown hang is independent of this.\n\nSent from my iPhone\n\n> On 10.10.2014, at 19:31, Stephan Ewen notifications@github.com wrote:\n> \n> I agree. Must have been accidentally. Is that the reason for the shutdown hang?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8118793/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9297423", "body": "Yes. But we can discuss it ;)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9297423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "asfbot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45938921", "body": "Robert Metzger  on dev@flink.incubator.apache.org replies:\nTesting Github <--> ML integration.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45938921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46052624", "body": "Ufuk Celebi  on dev@flink.incubator.apache.org replies:\n+1\n\nBut imo this will soon be subsumed by the upcoming RPC rework.\n\nSent from my iPhone\nl/14 we should not limit the heap space of the job submission client to 512M=\nB.\nsize(DataOutputSerializer.java:243)\nite(DataOutputSerializer.java:87)\nJobGraph.java:706)\n1)\nava:469)\nent.java:258)\ntEnvironment.java:50)\njava:79)\nImpl.java:62)\nAccessorImpl.java:43)\nckagedProgram.java:384)\nModeForExecution(PackagedProgram.java:302)\nva:327)\nava:927)\nent_fix\nr\ne\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46052624/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46053094", "body": "Robert Metzger  on dev@flink.incubator.apache.org replies:\nThats good. We have the same problem on the receiver side. I assume the new\nRPC service is not transferring the user-jar by first allocating it as a\nwhole in memory.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46053094/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "uce": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/45997673", "body": "Yes, the other flags are set as before. The `Xms` and `Xmx` flags are only set, when they are found in `stratosphere-conf.yaml`.\n\nWhile doing the PR, I came to the same conclusion as your second comment. But we can actually also do something more fancy and figure out the available memory ourselves and set it (thereby circumventing the defaults).\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/45997673/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/10656403", "body": "Just skimmed over the scaladoc and found a typo in `join`: t**w**o is missing the w.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/10656403/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "MoeweX": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/flink/issues/comments/46086799", "body": "Hi, thank you for your time ... . \nI checked it and found out that the json file created by the system is corrupted\n![bildschirmfoto 2014-06-14 um 14 33 55](https://cloud.githubusercontent.com/assets/5738978/3278453/5d1245f2-f3c0-11e3-80b6-65f773df18ce.png)\nThe comma under the step-function line is wrong. That is why jquery does not recognize this file as a json and the ajax request ends with an error. I will try to fix it, but I wonder how the old webclient is able to handle this, because the error occures in code I did not changed.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086799/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086894", "body": "By the way, the tool is able to work with the json, when you don't use the ajax request (where jquery checks whether it is a json):\n![bildschirmfoto 2014-06-14 um 14 40 35](https://cloud.githubusercontent.com/assets/5738978/3278469/3d3bb5d2-f3c1-11e3-9149-2be4082b05af.png)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/issues/comments/46086894/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "physikerwelt": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/7689854", "body": "for DRIVERNAME = \"org.mariadb.jdbc.Driver\" it worked with FLOAT_TYPE_INFO instead of DOUBLE_TYPE_INFO\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/7689854/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/7689856", "body": "test with org.mariadb.jdbc.Driver see inline comment\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/7689856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aljoscha": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/8201320", "body": "Yes, they are not like Java Iterators. They allow all the usual collection operations. In addition, you can get a buffered iterator and access the first element, as in:\n\n```\nval it: Iterator = // ...\nval buffered = it.buffered\nval first = buffered.head\nfor (v <- buffered) {\n  // iterate over all element, including first\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8201320/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/10661408", "body": "Dammit, I fixed it in my janino PR\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/10661408/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mbalassi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/8215458", "body": "Thanks, @hsaputra. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8215458/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/8599203", "body": "Thanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/8599203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/9725249", "body": "This way you use the generate pom that is in the outside repo and might be from a different branch. Not a big difference, but might lead to unexpected results. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9725249/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aalexandrov": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/9297124", "body": "Is removing the .gitignore a standard procedure for the release branches?\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/9297124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mjsax": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/11303380", "body": "Sorry for the long diff. It's due to inconsistent line-breaks within the file... now all line-breaks are UNIX format.\n\nThe actuall change is from line 144 to 153. (144 to 147 is new; 153 is changed -> added programClass to response String)\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11303380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "thvasilo": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/11408683", "body": "Just noticed: The example still uses CoCoA() instead of SVM()\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11408683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/11537562", "body": "Example should be program, cannot currently perform in the shell\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11537562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/11537565", "body": "TODO: Need to transform the tuples to LabeledVector\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11537565/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/comments/11537784", "body": "This way of reading in CSVs can get unwieldy fast. We need a more concise way to do this,\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/11537784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mxm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/flink/comments/12961579", "body": "Thanks for noticing. This requires some refactoring because the test depends on a method in `CommonTestUtils`. Is it desired to have a CommonTestUtils clases in each runtime and core? As far as I can see the methods are not runtime or core-specific, so the two can be combined into one class residing in core.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/comments/12961579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "fs111": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/53786826", "body": "nope, good catch. I'll give it another spin later today\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/53786826/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "nellboy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/102725620", "body": "Fixed with the latest push", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102725620/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102725706", "body": "I'm not sure to what we're referring to here. Can you clarify?", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102725706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102725785", "body": "Fixed with latest push", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102725785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726198", "body": "no, because initially there are no watermarks, so we must check if they exist or not.  Nonetheless I have refactored this function and moved it to jobs.ctrl.coffee", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726198/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726419", "body": "I updated it because I had the same problem with sublime :) ... It was easier for my workflow to indent it like this.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726454", "body": "Fixed.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726588", "body": "I have not fixed this, will try to see if I can find a more elegant solution.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726602", "body": "Done", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726645", "body": "Fixed.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726645/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726782", "body": "This controller is no longer empty, but we still needed to register it as a controller, so yes, we needed it.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/102726782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mtanski": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/63755827", "body": "I was following the (Deflate|Gzip)DeflateInflaterInputStreamFactory examples. I can change it if it's needed.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/63755827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/76121574", "body": "That's true, although calling it InputStreamFactory also does not seam right.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/76121574/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/81589240", "body": "This causes a build error due to binary backwards compatibility.\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/81589240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "alpinegizmo": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/flink/pulls/comments/92689908", "body": "Yes, this fixes the -p argument.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/92689908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96612039", "body": "This link can go to dev/stream/state.html", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96612039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96612338", "body": "Perhaps it would be clearer to say \"from a savepoint generated by its Flink-1.1 predecessor\"", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96612338/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96613305", "body": "This would be better as \"namely **keyed** and **non-keyed** state\" -- but also, do we want to agree on terminology? Stephan used \"operator state\" in the intro doc.", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96613305/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96613974", "body": "inconsistent indentation?", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96613974/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96614280", "body": "I suggest either\r\n\r\n> a grouped-by-key input stream\r\n\r\nor\r\n\r\n> a keyed input stream\r\n", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96614280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96628280", "body": "... the rest of this section focuses on ...", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96628280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96628469", "body": "interface, or the `ListCheckpointed<T extends Serializable>` interface,\r\n\r\n(add the word interface)", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96628469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96628699", "body": "snapshotState has a typo", "reactions": {"url": "https://api.github.com/repos/apache/flink/pulls/comments/96628699/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}}}