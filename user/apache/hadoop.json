{"_default": {"1": {"sunilgovind": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/06cceba1cb07340c412c4467439c16ea6812e685", "message": "YARN-7738. CapacityScheduler: Support refresh maximum allocation for multiple resource types. Contributed by Wangda Tan."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/8e5472b1e63f1c50e253e64702468da2bb38e476", "message": "YARN-7750. [UI2] Render time related fields in all pages to the browser timezone. Contributed by Vasudevan Skm."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "steveloughran": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/1093a73689912f78547e6d23023be2fd1c7ddc85", "message": "HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\nContributed by Aaron Fabbri"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/de630708d1912b3e4fa31e00f5d84a08a580e763", "message": "HADOOP-15123. KDiag tries to load krb5.conf from KRB5CCNAME instead of KRB5_CONFIG.\nContributed by Vipin Rathor.\n\n(cherry picked from commit 1ef906e29e0989aafcb35c51ad2acbb262b3c8e7)\n(cherry picked from commit f61edab1d0ea08b6d752ecdfb6068103822012ec)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f274fe33ea359d26a31efec42a856320a0dbb5f4", "message": "Revert \"HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\"\n\nThis reverts commit 35ad9b1dd279b769381ea1625d9bf776c309c5cb."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a0c71dcc33ca7c5539d0ab61c4a276c4f39e5744", "message": "HADOOP-15079. ITestS3AFileOperationCost#testFakeDirectoryDeletion failing\nafter OutputCommitter patch.\nContributed by Steve Loughran"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900", "body": "wrong JIRA\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477", "body": "Please can you file a separate JIRA for these two changes, link to HADOOP-9991 and include justification. Version updates are a traumatic issue in Hadoop and done fairly reluctantly.\n\nFWIW, updating netty-all from beta to final makes sense just from a release perspective; updating the other jetty less so \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270", "body": "this is three chained conditions which could be merged through `&&`\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678", "body": "could you move the preconditions checks into `S3ObjectAttributes` and invoke them from both output streams? That'd reduce duplicate code and perhaps aid future maintenance\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920", "body": "we're ok with .\\* on static imports here, so you can just skip this bit of the patch\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746", "body": "why was this cut? This is directly referred to in {{TestS3AEncryption}}, a file which this patch doesn't touch. I don't think a clean build of this patch is going to work. Have you tried it?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452", "body": "looks like an accidental paste in of a bit of HADOOP-13224's doc changes\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629", "body": "Well it's being covered in HADOOP-13224, so it's best to pull it here and review that patch instead\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002", "body": "sorry, wrong JIRA. https://issues.apache.org/jira/browse/HADOOP-13324\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957", "body": "might be good to add an example here. e.g what the final options of distcp are going to be. Will there be two -Xmx commands? if so, which wins? Because I suspect that's a JVM decision\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brahmareddybattula": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/08332e12d055d85472f0c9371fefe9b56bfea1ed", "message": "HADOOP-15150. in FsShell, UGI params should be overidden through env vars(-D arg). Contributed by Brahma Reddy Battula."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/880b9d24ff7b5f350ec99bac9b0862009460b486", "message": "HDFS-8693. refreshNamenodes does not support adding a new standby to a running DN. Contributed by Ajith S."}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/14279774", "body": "Hi Vinod,\n\nCan I get the Jira Id for this commit..?\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/14279774/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19446926", "body": "again \"import javax.servlet.ServletContext\" is missed which brokes branch-2 compliation and MetricsServlet changes are not present.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19446926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19461785", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19461785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19461789", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19461789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19462196", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19462196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/22364971", "body": "looks jjira id is missed in commit message", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/22364971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aajisaka": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/cdaf92c9f57560219b8f915a19ad8603ddf2a505", "message": "HADOOP-15177. Update the release year to 2018. Contributed by Bharat Viswanadham."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687", "body": "I've committed the patch in https://issues.apache.org/jira/browse/HADOOP-12081 to trunk and branch-2. Would you close this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562", "body": "Thank you for the pull request. I reviewed the patch (A) and the another patch in YARN-4434 jira (B) and decided to commit the patch (B) because the patch (B) replaces \"i.e. the entire disk\" with \"i.e. 90% of the disk\" as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634", "body": "I've committed the patch (B), so would you close this pull request?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727", "body": "I'm thinking the string concatenation by `+` is unnecessarily.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524", "body": "- The code creates String by `File.getAbsolutePath()` and then creates File by `new File(String)`. It can be simplified by the following and `TEST_ROOT_DIR` can be removed.\n\n```\n private static final File TEST_DIR = new File(GenericTestUtils.getTestDir(), \"fu\");\n```\n- `cacheDir` is unused.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222", "body": "I found some other occurrences of `/tmp/xxx`, which is used as the root dir of `FileContextTestHelper`. Would you replace them? I'm thinking the following is fine:\n\n```\nreturn new FileContextTestHelper(GenericTestUtils.getTempPath(\"TestWebHdfsFileContextMainOperations\"));\n```\n\nI'm okay if the replace is done in separate jira(s).\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827", "body": "It looks to me that the javac warning is related because the patch removes `@SuppressWarnings(\"unchecked\")`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731", "body": "Thank you for updating the pull request! Mostly looks good to me.\n(nit) Unused argument `resourceManager` can be removed. I'm +1 if that is addressed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "szegedim": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/a68e445dc682f4a123cdf016ce1aa46e550c7fdf", "message": "YARN-7717. Add configuration consistency for module.enabled and docker.privileged-containers.enabled. Contributed by Eric Badger."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/41049ba5d129f0fd0953ed8fdeb12635f7546bb2", "message": "YARN-7758. Add an additional check to the validity of container and application ids passed to container-executor. Contributed by Yufei Gu."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2dcfc1876e4d73cf85a6b1b7de694b1b4cc54494", "message": "YARN-7705. Create the container log directory with correct sticky bit in C code. Contributed by Yufei Gu."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vinayakumarb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/09efdfe9e13c9695867ce4034aa6ec970c2032f1", "message": "HDFS-9049. Make Datanode Netty reverse proxy port to be configurable. Contributed by Vinayakumar B."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ajfabbri": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/268ab4e0279b3e40f4a627d3dfe91e2a3523a8cc", "message": "HADOOP-15141 Support IAM Assumed roles in S3A. Contributed by Steve Loughran."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChenSammi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9195a6e302028ed3921d1016ac2fa5754f06ebf0", "message": "HADOOP-15027. AliyunOSS: Support multi-thread pre-read to improve sequential read from Hadoop to Aliyun OSS performance. (Contributed by Jinhu Wu)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flyrain": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/370f1c6283813dc1c7d001f44930e3c79c140c54", "message": "YARN-6486. FairScheduler: Deprecate continuous scheduling. (Contributed by Wilfred Spiegelenburg)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b2029353537fc8da9ab67834568cb2e24924cf5a", "message": "HADOOP-15157. Zookeeper authentication related properties to support CredentialProviders. (Contributed by Gergo Repas)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rkanter": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d716084f4503bf826ef10424d7025ea1ff4ee104", "message": "MAPREDUCE-7032. Add the ability to specify a delayed replication count (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5ac109909a29fab30363b752b5215be7f5dc616b", "message": "YARN-7479. TestContainerManagerSecurity.testContainerManager[Simple] flaky in trunk (ajisakaa via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e404650f489727d2df9a8813fddc4e0d682fbbee", "message": "MAPREDUCE-7030. Uploader tool should ignore symlinks to the same directory (miklos.szegedi@cloudera.com via rkanter)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohithsharmaks": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d09058b2fd18803d12f0835fdf78aef5e0b99c90", "message": "YARN-6736. Consider writing to both ats v1 & v2 from RM for smoother upgrades. Contributed by Aaron Gresch."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mukulhorton": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/2e1e9017aae7d88b443f7efb7217cf3b56ab0075", "message": "HADOOP-15172. Fix the javadoc warning in WriteOperationHelper.java\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tasanuma": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/1a9c5d479e2259ac024da392b020967112c5af55", "message": "MAPREDUCE-7034. Moving logging APIs over to slf4j the rest of all in hadoop-mapreduce\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arp7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/7016dd44e0975274856dc19f19815123c4b2a352", "message": "HDFS-13016. globStatus javadoc refers to glob pattern as \"regular expression\". Contributed by Mukul Kumar Singh."}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/20178778", "body": "Done, thanks a lot again @xiaoyuyao!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/20178778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373", "body": "Consider returning a List.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818", "body": "See HDFS-9478 which is fixing exception handling when constructing callqueue instances. We could use a similar fix for createScheduler.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863", "body": "The dn.getConf() object is not referenced outside the constructor so you can just pass a reference to that object. Also DNConf need not keep a reference to the dn. I think you can just revert all changes to this file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889", "body": "If we revert changes to DNConf we can just replace this with `this.dnConf = new DNConf(getConf())`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "linyiqun": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9afb8025d6549f0ade0ae7d36f5e67cd20c500f4", "message": "HDFS-12972. RBF: Display mount table quota info in Web UI and admin command. Contributed by Yiqun Lin."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangdatan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/edcc3a95d5248883492f2960f4fd22e09612ee9c", "message": "YARN-7468. Provide means for container network policy control. (Xuan Gong via wangda)\n\nChange-Id: I73678c343f663412917758feef35d8308c216e76"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "billierinaldi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/53f2768926700d2a27ce6223f1ccbfd3be49fc29", "message": "YARN-7724. yarn application status should support application name. Contributed by Jian He"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Yiran-wu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/331", "title": "YARN-7773. Fix YARN Federation used Mysql as state store throw except\u2026", "body": "YARN-7773 YARN Federation used Mysql as state store throw exception, Unknown column 'homeSubCluster' in 'field list'\r\n\r\n\r\n#331 \r\n\r\nsubmitApplication appIdapplication_1516277664083_0014 try #0 on SubCluster cluster1 , queue: root.bdp_federation\r\n[2018-01-18T23:25:29.325+08:00] [ERROR] store.impl.SQLFederationStateStore.logAndThrowRetriableException(FederationStateStoreUtils.java 158) [IPC Server handler 44 on 8050] : Unable to insert the newly generated application application_1516277664083_0014\r\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\n[2018-01-18T23:25:29.326+08:00] [ERROR] server.router.RouterServerUtil.logAndThrowException(RouterServerUtil.java 55) [IPC Server handler 44 on 8050] : Unable to insert the ApplicationId application_1516277664083_0014 into the FederationStateStore\r\norg.apache.hadoop.yarn.server.federation.store.exception.FederationStateStoreRetriableException: Unable to insert the newly generated application application_1516277664083_0014\r\nat org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils.logAndThrowRetriableException(FederationStateStoreUtils.java:159)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:593)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\nCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\n... 20 more \r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wuzhilon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/330", "title": "YARN PROXYSERVER throw IOEXCEPTION", "body": "When we more than ten users simultaneously submitted to view the proxyserver, there will be stuck, and then it will throw IO exception", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "skmvasu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/329", "title": "YARN-7760. precompute master node URL", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/328", "title": "YARN-7742 - Remove duplicate entries", "body": "[YARN-7742]", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/327", "title": "YARN-7749. Fix GPU sidebar", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/323", "title": "YARN-7750. Use local time to render dates", "body": "Use local time to render dates", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/315", "title": "YARN-7648. Fix attempts UI when app run fails", "body": "Fixes applications tab rendering when the app attempts have failed", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gerashegalov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/326", "title": "YARN-7747 use injected GuiceFilter instances", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/325", "title": "HADOOP-15166: simplify minicluster start", "body": "add minicluster subcommand to simplify its usage", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xshaun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/324", "title": "Fix NullPointerException caused by null-builder", "body": "Sometimes occurs java.lang.NullPointerException leading to app failed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gehaijiang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/321", "title": "Branch 3.0   operation shell script   ERROR", "body": "run  ./stop-dfs.sh  \r\n\r\ntext:\r\n\r\nStopping namenodes on [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping datanodes\r\n10.50.132.147: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.151: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.146: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.150: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.154: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.145: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.152: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.148: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.149: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.153: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\nStopping journal nodes [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping ZK Failover Controllers on NN hosts [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mmolimar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/319", "title": "HADOOP-15142. Register FTP and SFTP as FS services", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "medb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/318", "title": "[HADOOP-11032] Migrate Guava's Stopwatch to Hadoop's StopWatch", "body": "After this change Hadoop could build against Guava 21.0\r\n\r\nJustification for migration is the same as previous migrations here:\r\nhttps://issues.apache.org/jira/browse/HADOOP-11032 ", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/316", "title": "HADOOP-15124. Improve FileSystem.Statistics performance", "body": "This is PR for https://issues.apache.org/jira/browse/HADOOP-15124", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dwshmilyss": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/317", "title": "Branch 2.8.3", "body": "test\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "denis-zhdanov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/314", "title": "POC: replace explicit method parameters null-checks by a declarative approach", "body": "This *PR* shows an approach when explicit *null*-checks (*Preconditions.checkNotNull()*) are generated automatically by the [Traute](http://traute.oss.harmonysoft.tech/) *javac* plugin for method parameters marked by *Nonnull* annotation.  \r\n\r\nExample: consider the [FSDataOutputStreamBuilder.permission()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java#L151) - its bytecode looks like if it's compiled from the source below:  \r\n\r\n```java\r\npublic B permission(@Nonnull final FsPermission perm) {\r\n    if (perm == null) {\r\n        throw new NullPointerException(\"String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\");\r\n    }\r\n    Preconditions.checkNotNull(perm);\r\n    permission = perm;\r\n    return getThisBuilder();\r\n}\r\n```  \r\n\r\nDetails:  \r\n\r\n```\r\njavap -c ./hadoop-common-project/hadoop-common/target/classes/org/apache/hadoop/fs/FSDataOutputStreamBuilder.class\r\n...\r\n  public B permission(org.apache.hadoop.fs.permission.FsPermission);\r\n    Code:\r\n       0: aload_1\r\n       1: ifnonnull     14\r\n       4: new           #16                 // class java/lang/NullPointerException\r\n       7: dup\r\n       8: ldc           #31                 // String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\r\n      10: invokespecial #18                 // Method java/lang/NullPointerException.\"<init>\":(Ljava/lang/String;)V\r\n      13: athrow\r\n      14: aload_0\r\n      15: aload_1\r\n      16: putfield      #3                  // Field permission:Lorg/apache/hadoop/fs/permission/FsPermission;\r\n      19: aload_0\r\n      20: invokevirtual #32                 // Method getThisBuilder:()Lorg/apache/hadoop/fs/FSDataOutputStreamBuilder;\r\n      23: areturn\r\n```  \r\n\r\nSo, the idea is to do the following:  \r\n1. Go through the project's codebase and find all places where *Preconditions.checkNotNull()* is called for method parameter\r\n2. Ensure that target method parameter is marked by the *Nonnull* annotation (e.g. [ActiveStandbyElector.isStaleClient()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java#L1124) is not marked by it, so, we need to add the annotation)\r\n3. Remove *Preconditions.checkNotNull()* call  \r\n\r\nBenefits:  \r\n* the code becomes cleaner without that explicit checks\r\n* the code is better documented as it's immediately clear what method parameters must be not-*null*\r\n* IDEs highlight possible *NPE* for method parameters marked by *Nonnull* annotations\r\n\r\nPlease let me know if you like the idea, I'm fine with providing a *PR* which applies the solution to the whole project's codebase then.\r\n\r\nTESTED: mvn clean package & ensured that resulting\r\n        bytecode has the checks", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "addisonj": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/311", "title": "[HADOOP-15096] Don't create user with lastlog", "body": "This fixes a problem where in certain cases, the useradd command can create a very large diff that can blow up the host disk size.\r\n\r\nThe reason for this is that lastlog is a sparse file, but AUFS under docker apparently doesn't deal with those well and creates a very large file.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jiayuhan-it": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/309", "title": "MAPREDUCE-7017:Too many times of meaningless invocation in TaskAttemptImpl#resolveHosts", "body": "MRAppMaster uses TaskAttemptImpl::resolveHosts to determine the dataLocalHosts for each task when the location of data split is IP, which will call a lot of times ( taskNum * dfsReplication) of function InetAddress::getByName and most of the funcition calls are redundant. When the job has a great number of tasks and the speed of DNS resolution is not fast enough, it will take a lot of time at this stage before the job running.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vetriselvan1187": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/304", "title": "[YARN-7578] waitForDiskHealthCheck sleep time is extended from 1000ms\u2026", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "animenon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/300", "title": "[HADOOP-15099] YARN Federation Link fix", "body": "The fix is for YARN Federation link on [Apache Hadoop YARN page](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).\r\n\r\nFederation Link was `.Federation.html`, removed the `.`, hence fixing the 404 Error I saw on the site.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rgoers": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/299", "title": "Support Log4j 2 and Logback", "body": "This patch relates to https://issues.apache.org/jira/browse/HADOOP-12956. It makes the logging implementation in hadoop common optional and provides support for event counters in Log4j 2 and Logback in addition to Log4j 1.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maobaolong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/297", "title": "Fix Checkstyle error, rename a argument", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/296", "title": "Fix constants variable name", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/295", "title": "Fix checkstyle problem", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mchataigner": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/290", "title": "HADOOP-14128. fix renameInternal in ChecksumFs", "body": "AbstractFs.rename(source, destination, options) calls\r\nrenameInternal(source, destination, overwrite)\r\n\r\nThis patch adds this method to ChecksumFs to rename the crc file in\r\naddition to the file itself to avoid crc missmatch when use for example\r\nin LocalFs.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuxintan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/288", "title": "HDFS-12749. Catch IOException to schedule BlockReport correctly when \u2026", "body": "\u2026DN re-register. (Contributed by TanYuxin)", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wenxinhe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/287", "title": "HDFS-10323. transient deleteOnExit failure in ViewFileSystem due to close() ordering", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "retroverse": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/286", "title": "Update and rename README.txt to README.md", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ggribeler": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/285", "title": "MAPREDUCE-6752: Bad logging practices in mapreduce", "body": "Changed the log level of the method that is only used for debugging purpose as discussed here:\r\n\r\nhttps://issues.apache.org/jira/browse/MAPREDUCE-6752\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bberton-ciandt": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/284", "title": "HADOOP-14157 fix parseUrl to replace '\\' for '/'", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Markiry": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738", "body": "I'm very sorry, I have some mistakes.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "resouer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298", "body": "I'm really interested in this feature, but why it is not merged yet? \nBut why I can see this patch has been added to 2.6.0?\nhttp://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n\nI'm really confused ....\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169", "body": "So is it possible for me to launch Dockers in Yarn to run Spark jobs now? Is there a guide for me to do so?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203", "body": "Thanks! I'd like to.  \n\nBTW, what's the current status of this great yarn-docker work. \n1. Can I use it now?\n2. Do I need to use customized docker?\n3. What I can do if I want to contribute to make it better?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ashahab-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472", "body": "It has been merged to trunk and branch-2.6\n\nOn Wed, Dec 10, 2014 at 12:01 AM, Harry Zhang notifications@github.com\nwrote:\n\n> I'm really interested in this feature, but why it is not merged yet?\n> But why I can see this patch has been added to 2.6.0?\n> \n> http://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n> \n> I'm really confused ....\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66416298.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694", "body": "No, but feel free to contribute that guide to hadoop.\n\nOn Wed, Dec 10, 2014 at 1:06 AM, Harry Zhang notifications@github.com\nwrote:\n\n> So is it possible for me to launch Dockers in Yarn to run Spark jobs now?\n> Is there a guide for me to do so?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66422169.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "modeyang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055", "body": "hi, ashahab-altiscale,\nhas any way or plan  apply docker container to  mapreduce workers within hadoop ? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "oza": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438", "body": "@gliptak please report this issue to JIRA: https://issues.apache.org/jira/browse/YARN\n\nPlease see the for more detail: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731", "body": "@s-bolz thank you for your contribution. Please use Hadoop's JIRA to contribute Hadoop project.\n\nhttps://issues.apache.org/jira/browse/HADOOP\nhttp://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021", "body": "@gliptak yes, these patches have been submitted successfully. Now committers reviewed your patches, so please check them.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717", "body": "@Guchige  thanks for your contribution. We use JIRA instead of github and don't accept any patches via github for now. Could you attach diff file based on the PR? \n\nThe documenation, How To Contribe in Hadoop Wiki, is useful.  http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735", "body": "@kemiya-yx  thanks for your PR. Currently, we accept all contributions via JIRA.\n\nPlease read how to contribute page on wiki: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448", "body": "This PR is for a review.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706", "body": "I found one incompatible change about Jersey - after upgrading from 1.12 to 1.13, the root element whose content is empty collection is changed from null to empty object({}). Related to following change: https://java.net/jira/browse/JERSEY-1168\n\nThis change of lines fixes tests and assertions about JSONObject.NULL to address the change above. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639", "body": "@aajisaka thank you for the comment. The fix I made is just for avoiding warning.  \nCan we do this on another jira? Your comment is a just minor refactoring, so we can do it on separate issue.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158", "body": "@oza could you validate if these are submitted correctly? thanks\n\nhttps://issues.apache.org/jira/browse/YARN-3444\nhttps://issues.apache.org/jira/browse/HADOOP-11801\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "MjAbuz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630", "body": "Learning\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471", "body": "Testing comments on github pull requests.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "yxkemiya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267", "body": "@oza Thanks, I will close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "vesense": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789", "body": "Reported the issue to JIRA: https://issues.apache.org/jira/browse/YARN-4387\nSo, close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "zhe-thoughts": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122", "body": "Thanks for reporting this. Even though the fix is minor, please created a JIRA at https://issues.apache.org/jira/browse/HADOOP\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "bwtakacy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815", "body": "OK.\nI will close this PR.\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "raviprak-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287", "body": "Thanks for your contribution emopers. Could you please follow these steps? https://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "rainforc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169", "body": "Balancer will throw NullPointerException when datanode is down.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "2899722744": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122", "body": "sync\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "h4ck3rm1k3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944", "body": "I am having some problems with further testing. Closing this PR for now until I fix and test more. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034", "body": "https://github.com/h4ck3rm1k3/hadoop-archive-org-bucket-fs/issues/1 this is the problem that I found. Needs more work. Need to create test cases inside of hadoop for this. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tonyzeng20151": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/9905533", "body": "Hi\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/9905533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zhangminglei": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/11830060", "body": "Good\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/11830060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "svnpenn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/13832916", "body": "This line is insane because GnuWin32 doesn\u2019t even provide a shell:\n\nhttp://gnuwin32.sourceforge.net/faq.html#How_do_I_run_shell_scripts\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/13832916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "JunpingDu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/14097788", "body": "Hi @rkanter, do we have a public JIRA to track this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/14097788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "msemelman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/15730983", "body": "It would be nice to see an example.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/15730983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "DasiyShang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/17624303", "body": "just to study\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/17624303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "yuj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/18033277", "body": "Could someone explain why 'others' are not allowed to even read the logs inside App dirs?  Would 0775 work?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/18033277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "tgravescs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/18041857", "body": "others aren't allowed to see logs because the application could be logging something that is secure/sensitive.  ie financial data, passwords, etc.  Ideally they aren't but we need to protect other applications from reading this without explicit authorization.  \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/18041857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "ybank": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/19199596", "body": "Just came across this when I am doing code analysis. Very minor issue, though.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19199596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "xiaoyuyao": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/20178230", "body": "Looks good to me. +1. Can you update the patch on Apache?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/20178230/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429", "body": "Good point. I will address that in the next patch.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "nfouka": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/21319790", "body": ".", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/21319790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "donnyw88": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/25191011", "body": "[license.txt](https://github.com/apache/hadoop/files/1415870/license.txt)\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/25191011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jebat9999": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26379606", "body": "Detail...please...about my payment", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26379606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "1004770753": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26564582", "body": "baga", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26564582/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "PrinceKK300188": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26697240", "body": "Why", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26697240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/26697248", "body": "Fuck", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26697248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adsontag": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26788523", "body": "Kill", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26788523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "cnauroth": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996", "body": "I think you'll still need to incorporate the fixes I suggested earlier in a JIRA comment: remove the extra space character at the end of the `\"target \"` string literal, and switch from `File.pathSeparator` to `File.separator`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "fedecz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407", "body": "As you said, that constant is only used in that test which I did change. I changed it to abstract and created 3 different implementations: one for SSE-S3, SSE-KMS and SSE-C. Basically I'm running all the tests in TestS3AEncryption, but with different encryption algorithms depending on the concrete class.\nYes, it builds and all test pass.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908", "body": "Nope, this is the section _Other Issues_ in the documentation, so I wanted to document if the user was having that warning, he/she should specify the endpoint in the config. I guess I could set a title to describe it better instead of just pasting the warning.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238", "body": "true, but not all of them can be merged. I'm relying in else clauses as well depending on some of the conditions being false. I'll try to rewrite it though and will see how it looks.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364", "body": "will do\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085", "body": "I don't see anything related to this patch in that ticket's patch, are you sure that's the one? I'm looking at the attached patch in HADOOP-13224\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "aw-was-here": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457", "body": "If there is an Xmx in HADOOP_CLIENT_OPTS and an Xmx in MAPRED_DISTCP_OPTS, then the mapred distcp final HADOOP_OPTS will definitely have two Xmx flags.  After HADOOP-13365, we'll be in a position to potentially de-dupe user provided settings like we do for other things.  But until de-dupe, you're correct that it's a JVM decision.  In the past, that decision has been last one wins and I doubt Oracle could change it if they wanted to at this point without major ramifications.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "templedf": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935", "body": "Can we please have one exit point?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007", "body": "Seems like this should be more defensive, i.e. check for the type before casting.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635", "body": "Maybe put the scheduler into the context?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}, "2": {"sunilgovind": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/06cceba1cb07340c412c4467439c16ea6812e685", "message": "YARN-7738. CapacityScheduler: Support refresh maximum allocation for multiple resource types. Contributed by Wangda Tan."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/8e5472b1e63f1c50e253e64702468da2bb38e476", "message": "YARN-7750. [UI2] Render time related fields in all pages to the browser timezone. Contributed by Vasudevan Skm."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "steveloughran": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/1093a73689912f78547e6d23023be2fd1c7ddc85", "message": "HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\nContributed by Aaron Fabbri"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/de630708d1912b3e4fa31e00f5d84a08a580e763", "message": "HADOOP-15123. KDiag tries to load krb5.conf from KRB5CCNAME instead of KRB5_CONFIG.\nContributed by Vipin Rathor.\n\n(cherry picked from commit 1ef906e29e0989aafcb35c51ad2acbb262b3c8e7)\n(cherry picked from commit f61edab1d0ea08b6d752ecdfb6068103822012ec)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f274fe33ea359d26a31efec42a856320a0dbb5f4", "message": "Revert \"HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\"\n\nThis reverts commit 35ad9b1dd279b769381ea1625d9bf776c309c5cb."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a0c71dcc33ca7c5539d0ab61c4a276c4f39e5744", "message": "HADOOP-15079. ITestS3AFileOperationCost#testFakeDirectoryDeletion failing\nafter OutputCommitter patch.\nContributed by Steve Loughran"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900", "body": "wrong JIRA\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477", "body": "Please can you file a separate JIRA for these two changes, link to HADOOP-9991 and include justification. Version updates are a traumatic issue in Hadoop and done fairly reluctantly.\n\nFWIW, updating netty-all from beta to final makes sense just from a release perspective; updating the other jetty less so \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270", "body": "this is three chained conditions which could be merged through `&&`\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678", "body": "could you move the preconditions checks into `S3ObjectAttributes` and invoke them from both output streams? That'd reduce duplicate code and perhaps aid future maintenance\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920", "body": "we're ok with .\\* on static imports here, so you can just skip this bit of the patch\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746", "body": "why was this cut? This is directly referred to in {{TestS3AEncryption}}, a file which this patch doesn't touch. I don't think a clean build of this patch is going to work. Have you tried it?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452", "body": "looks like an accidental paste in of a bit of HADOOP-13224's doc changes\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629", "body": "Well it's being covered in HADOOP-13224, so it's best to pull it here and review that patch instead\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002", "body": "sorry, wrong JIRA. https://issues.apache.org/jira/browse/HADOOP-13324\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957", "body": "might be good to add an example here. e.g what the final options of distcp are going to be. Will there be two -Xmx commands? if so, which wins? Because I suspect that's a JVM decision\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brahmareddybattula": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/08332e12d055d85472f0c9371fefe9b56bfea1ed", "message": "HADOOP-15150. in FsShell, UGI params should be overidden through env vars(-D arg). Contributed by Brahma Reddy Battula."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/880b9d24ff7b5f350ec99bac9b0862009460b486", "message": "HDFS-8693. refreshNamenodes does not support adding a new standby to a running DN. Contributed by Ajith S."}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/14279774", "body": "Hi Vinod,\n\nCan I get the Jira Id for this commit..?\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/14279774/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19446926", "body": "again \"import javax.servlet.ServletContext\" is missed which brokes branch-2 compliation and MetricsServlet changes are not present.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19446926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19461785", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19461785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19461789", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19461789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19462196", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19462196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/22364971", "body": "looks jjira id is missed in commit message", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/22364971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aajisaka": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/cdaf92c9f57560219b8f915a19ad8603ddf2a505", "message": "HADOOP-15177. Update the release year to 2018. Contributed by Bharat Viswanadham."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687", "body": "I've committed the patch in https://issues.apache.org/jira/browse/HADOOP-12081 to trunk and branch-2. Would you close this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562", "body": "Thank you for the pull request. I reviewed the patch (A) and the another patch in YARN-4434 jira (B) and decided to commit the patch (B) because the patch (B) replaces \"i.e. the entire disk\" with \"i.e. 90% of the disk\" as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634", "body": "I've committed the patch (B), so would you close this pull request?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727", "body": "I'm thinking the string concatenation by `+` is unnecessarily.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524", "body": "- The code creates String by `File.getAbsolutePath()` and then creates File by `new File(String)`. It can be simplified by the following and `TEST_ROOT_DIR` can be removed.\n\n```\n private static final File TEST_DIR = new File(GenericTestUtils.getTestDir(), \"fu\");\n```\n- `cacheDir` is unused.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222", "body": "I found some other occurrences of `/tmp/xxx`, which is used as the root dir of `FileContextTestHelper`. Would you replace them? I'm thinking the following is fine:\n\n```\nreturn new FileContextTestHelper(GenericTestUtils.getTempPath(\"TestWebHdfsFileContextMainOperations\"));\n```\n\nI'm okay if the replace is done in separate jira(s).\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827", "body": "It looks to me that the javac warning is related because the patch removes `@SuppressWarnings(\"unchecked\")`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731", "body": "Thank you for updating the pull request! Mostly looks good to me.\n(nit) Unused argument `resourceManager` can be removed. I'm +1 if that is addressed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "szegedim": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/a68e445dc682f4a123cdf016ce1aa46e550c7fdf", "message": "YARN-7717. Add configuration consistency for module.enabled and docker.privileged-containers.enabled. Contributed by Eric Badger."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/41049ba5d129f0fd0953ed8fdeb12635f7546bb2", "message": "YARN-7758. Add an additional check to the validity of container and application ids passed to container-executor. Contributed by Yufei Gu."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2dcfc1876e4d73cf85a6b1b7de694b1b4cc54494", "message": "YARN-7705. Create the container log directory with correct sticky bit in C code. Contributed by Yufei Gu."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vinayakumarb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/09efdfe9e13c9695867ce4034aa6ec970c2032f1", "message": "HDFS-9049. Make Datanode Netty reverse proxy port to be configurable. Contributed by Vinayakumar B."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ajfabbri": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/268ab4e0279b3e40f4a627d3dfe91e2a3523a8cc", "message": "HADOOP-15141 Support IAM Assumed roles in S3A. Contributed by Steve Loughran."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChenSammi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9195a6e302028ed3921d1016ac2fa5754f06ebf0", "message": "HADOOP-15027. AliyunOSS: Support multi-thread pre-read to improve sequential read from Hadoop to Aliyun OSS performance. (Contributed by Jinhu Wu)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flyrain": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/370f1c6283813dc1c7d001f44930e3c79c140c54", "message": "YARN-6486. FairScheduler: Deprecate continuous scheduling. (Contributed by Wilfred Spiegelenburg)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b2029353537fc8da9ab67834568cb2e24924cf5a", "message": "HADOOP-15157. Zookeeper authentication related properties to support CredentialProviders. (Contributed by Gergo Repas)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rkanter": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d716084f4503bf826ef10424d7025ea1ff4ee104", "message": "MAPREDUCE-7032. Add the ability to specify a delayed replication count (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5ac109909a29fab30363b752b5215be7f5dc616b", "message": "YARN-7479. TestContainerManagerSecurity.testContainerManager[Simple] flaky in trunk (ajisakaa via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e404650f489727d2df9a8813fddc4e0d682fbbee", "message": "MAPREDUCE-7030. Uploader tool should ignore symlinks to the same directory (miklos.szegedi@cloudera.com via rkanter)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohithsharmaks": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d09058b2fd18803d12f0835fdf78aef5e0b99c90", "message": "YARN-6736. Consider writing to both ats v1 & v2 from RM for smoother upgrades. Contributed by Aaron Gresch."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mukulhorton": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/2e1e9017aae7d88b443f7efb7217cf3b56ab0075", "message": "HADOOP-15172. Fix the javadoc warning in WriteOperationHelper.java\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tasanuma": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/1a9c5d479e2259ac024da392b020967112c5af55", "message": "MAPREDUCE-7034. Moving logging APIs over to slf4j the rest of all in hadoop-mapreduce\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arp7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/7016dd44e0975274856dc19f19815123c4b2a352", "message": "HDFS-13016. globStatus javadoc refers to glob pattern as \"regular expression\". Contributed by Mukul Kumar Singh."}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/20178778", "body": "Done, thanks a lot again @xiaoyuyao!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/20178778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373", "body": "Consider returning a List.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818", "body": "See HDFS-9478 which is fixing exception handling when constructing callqueue instances. We could use a similar fix for createScheduler.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863", "body": "The dn.getConf() object is not referenced outside the constructor so you can just pass a reference to that object. Also DNConf need not keep a reference to the dn. I think you can just revert all changes to this file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889", "body": "If we revert changes to DNConf we can just replace this with `this.dnConf = new DNConf(getConf())`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "linyiqun": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9afb8025d6549f0ade0ae7d36f5e67cd20c500f4", "message": "HDFS-12972. RBF: Display mount table quota info in Web UI and admin command. Contributed by Yiqun Lin."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangdatan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/edcc3a95d5248883492f2960f4fd22e09612ee9c", "message": "YARN-7468. Provide means for container network policy control. (Xuan Gong via wangda)\n\nChange-Id: I73678c343f663412917758feef35d8308c216e76"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "billierinaldi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/53f2768926700d2a27ce6223f1ccbfd3be49fc29", "message": "YARN-7724. yarn application status should support application name. Contributed by Jian He"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Yiran-wu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/331", "title": "YARN-7773. Fix YARN Federation used Mysql as state store throw except\u2026", "body": "YARN-7773 YARN Federation used Mysql as state store throw exception, Unknown column 'homeSubCluster' in 'field list'\r\n\r\n\r\n#331 \r\n\r\nsubmitApplication appIdapplication_1516277664083_0014 try #0 on SubCluster cluster1 , queue: root.bdp_federation\r\n[2018-01-18T23:25:29.325+08:00] [ERROR] store.impl.SQLFederationStateStore.logAndThrowRetriableException(FederationStateStoreUtils.java 158) [IPC Server handler 44 on 8050] : Unable to insert the newly generated application application_1516277664083_0014\r\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\n[2018-01-18T23:25:29.326+08:00] [ERROR] server.router.RouterServerUtil.logAndThrowException(RouterServerUtil.java 55) [IPC Server handler 44 on 8050] : Unable to insert the ApplicationId application_1516277664083_0014 into the FederationStateStore\r\norg.apache.hadoop.yarn.server.federation.store.exception.FederationStateStoreRetriableException: Unable to insert the newly generated application application_1516277664083_0014\r\nat org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils.logAndThrowRetriableException(FederationStateStoreUtils.java:159)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:593)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\nCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\n... 20 more \r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wuzhilon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/330", "title": "YARN PROXYSERVER throw IOEXCEPTION", "body": "When we more than ten users simultaneously submitted to view the proxyserver, there will be stuck, and then it will throw IO exception", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "skmvasu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/329", "title": "YARN-7760. precompute master node URL", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/328", "title": "YARN-7742 - Remove duplicate entries", "body": "[YARN-7742]", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/327", "title": "YARN-7749. Fix GPU sidebar", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/323", "title": "YARN-7750. Use local time to render dates", "body": "Use local time to render dates", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/315", "title": "YARN-7648. Fix attempts UI when app run fails", "body": "Fixes applications tab rendering when the app attempts have failed", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gerashegalov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/326", "title": "YARN-7747 use injected GuiceFilter instances", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/325", "title": "HADOOP-15166: simplify minicluster start", "body": "add minicluster subcommand to simplify its usage", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xshaun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/324", "title": "Fix NullPointerException caused by null-builder", "body": "Sometimes occurs java.lang.NullPointerException leading to app failed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gehaijiang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/321", "title": "Branch 3.0   operation shell script   ERROR", "body": "run  ./stop-dfs.sh  \r\n\r\ntext:\r\n\r\nStopping namenodes on [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping datanodes\r\n10.50.132.147: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.151: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.146: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.150: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.154: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.145: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.152: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.148: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.149: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.153: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\nStopping journal nodes [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping ZK Failover Controllers on NN hosts [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mmolimar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/319", "title": "HADOOP-15142. Register FTP and SFTP as FS services", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "medb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/318", "title": "[HADOOP-11032] Migrate Guava's Stopwatch to Hadoop's StopWatch", "body": "After this change Hadoop could build against Guava 21.0\r\n\r\nJustification for migration is the same as previous migrations here:\r\nhttps://issues.apache.org/jira/browse/HADOOP-11032 ", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/316", "title": "HADOOP-15124. Improve FileSystem.Statistics performance", "body": "This is PR for https://issues.apache.org/jira/browse/HADOOP-15124", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dwshmilyss": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/317", "title": "Branch 2.8.3", "body": "test\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "denis-zhdanov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/314", "title": "POC: replace explicit method parameters null-checks by a declarative approach", "body": "This *PR* shows an approach when explicit *null*-checks (*Preconditions.checkNotNull()*) are generated automatically by the [Traute](http://traute.oss.harmonysoft.tech/) *javac* plugin for method parameters marked by *Nonnull* annotation.  \r\n\r\nExample: consider the [FSDataOutputStreamBuilder.permission()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java#L151) - its bytecode looks like if it's compiled from the source below:  \r\n\r\n```java\r\npublic B permission(@Nonnull final FsPermission perm) {\r\n    if (perm == null) {\r\n        throw new NullPointerException(\"String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\");\r\n    }\r\n    Preconditions.checkNotNull(perm);\r\n    permission = perm;\r\n    return getThisBuilder();\r\n}\r\n```  \r\n\r\nDetails:  \r\n\r\n```\r\njavap -c ./hadoop-common-project/hadoop-common/target/classes/org/apache/hadoop/fs/FSDataOutputStreamBuilder.class\r\n...\r\n  public B permission(org.apache.hadoop.fs.permission.FsPermission);\r\n    Code:\r\n       0: aload_1\r\n       1: ifnonnull     14\r\n       4: new           #16                 // class java/lang/NullPointerException\r\n       7: dup\r\n       8: ldc           #31                 // String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\r\n      10: invokespecial #18                 // Method java/lang/NullPointerException.\"<init>\":(Ljava/lang/String;)V\r\n      13: athrow\r\n      14: aload_0\r\n      15: aload_1\r\n      16: putfield      #3                  // Field permission:Lorg/apache/hadoop/fs/permission/FsPermission;\r\n      19: aload_0\r\n      20: invokevirtual #32                 // Method getThisBuilder:()Lorg/apache/hadoop/fs/FSDataOutputStreamBuilder;\r\n      23: areturn\r\n```  \r\n\r\nSo, the idea is to do the following:  \r\n1. Go through the project's codebase and find all places where *Preconditions.checkNotNull()* is called for method parameter\r\n2. Ensure that target method parameter is marked by the *Nonnull* annotation (e.g. [ActiveStandbyElector.isStaleClient()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java#L1124) is not marked by it, so, we need to add the annotation)\r\n3. Remove *Preconditions.checkNotNull()* call  \r\n\r\nBenefits:  \r\n* the code becomes cleaner without that explicit checks\r\n* the code is better documented as it's immediately clear what method parameters must be not-*null*\r\n* IDEs highlight possible *NPE* for method parameters marked by *Nonnull* annotations\r\n\r\nPlease let me know if you like the idea, I'm fine with providing a *PR* which applies the solution to the whole project's codebase then.\r\n\r\nTESTED: mvn clean package & ensured that resulting\r\n        bytecode has the checks", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "addisonj": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/311", "title": "[HADOOP-15096] Don't create user with lastlog", "body": "This fixes a problem where in certain cases, the useradd command can create a very large diff that can blow up the host disk size.\r\n\r\nThe reason for this is that lastlog is a sparse file, but AUFS under docker apparently doesn't deal with those well and creates a very large file.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jiayuhan-it": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/309", "title": "MAPREDUCE-7017:Too many times of meaningless invocation in TaskAttemptImpl#resolveHosts", "body": "MRAppMaster uses TaskAttemptImpl::resolveHosts to determine the dataLocalHosts for each task when the location of data split is IP, which will call a lot of times ( taskNum * dfsReplication) of function InetAddress::getByName and most of the funcition calls are redundant. When the job has a great number of tasks and the speed of DNS resolution is not fast enough, it will take a lot of time at this stage before the job running.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vetriselvan1187": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/304", "title": "[YARN-7578] waitForDiskHealthCheck sleep time is extended from 1000ms\u2026", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "animenon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/300", "title": "[HADOOP-15099] YARN Federation Link fix", "body": "The fix is for YARN Federation link on [Apache Hadoop YARN page](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).\r\n\r\nFederation Link was `.Federation.html`, removed the `.`, hence fixing the 404 Error I saw on the site.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rgoers": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/299", "title": "Support Log4j 2 and Logback", "body": "This patch relates to https://issues.apache.org/jira/browse/HADOOP-12956. It makes the logging implementation in hadoop common optional and provides support for event counters in Log4j 2 and Logback in addition to Log4j 1.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maobaolong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/297", "title": "Fix Checkstyle error, rename a argument", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/296", "title": "Fix constants variable name", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/295", "title": "Fix checkstyle problem", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mchataigner": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/290", "title": "HADOOP-14128. fix renameInternal in ChecksumFs", "body": "AbstractFs.rename(source, destination, options) calls\r\nrenameInternal(source, destination, overwrite)\r\n\r\nThis patch adds this method to ChecksumFs to rename the crc file in\r\naddition to the file itself to avoid crc missmatch when use for example\r\nin LocalFs.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuxintan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/288", "title": "HDFS-12749. Catch IOException to schedule BlockReport correctly when \u2026", "body": "\u2026DN re-register. (Contributed by TanYuxin)", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wenxinhe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/287", "title": "HDFS-10323. transient deleteOnExit failure in ViewFileSystem due to close() ordering", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "retroverse": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/286", "title": "Update and rename README.txt to README.md", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ggribeler": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/285", "title": "MAPREDUCE-6752: Bad logging practices in mapreduce", "body": "Changed the log level of the method that is only used for debugging purpose as discussed here:\r\n\r\nhttps://issues.apache.org/jira/browse/MAPREDUCE-6752\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bberton-ciandt": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/284", "title": "HADOOP-14157 fix parseUrl to replace '\\' for '/'", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Markiry": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738", "body": "I'm very sorry, I have some mistakes.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "resouer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298", "body": "I'm really interested in this feature, but why it is not merged yet? \nBut why I can see this patch has been added to 2.6.0?\nhttp://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n\nI'm really confused ....\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169", "body": "So is it possible for me to launch Dockers in Yarn to run Spark jobs now? Is there a guide for me to do so?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203", "body": "Thanks! I'd like to.  \n\nBTW, what's the current status of this great yarn-docker work. \n1. Can I use it now?\n2. Do I need to use customized docker?\n3. What I can do if I want to contribute to make it better?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ashahab-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472", "body": "It has been merged to trunk and branch-2.6\n\nOn Wed, Dec 10, 2014 at 12:01 AM, Harry Zhang notifications@github.com\nwrote:\n\n> I'm really interested in this feature, but why it is not merged yet?\n> But why I can see this patch has been added to 2.6.0?\n> \n> http://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n> \n> I'm really confused ....\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66416298.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694", "body": "No, but feel free to contribute that guide to hadoop.\n\nOn Wed, Dec 10, 2014 at 1:06 AM, Harry Zhang notifications@github.com\nwrote:\n\n> So is it possible for me to launch Dockers in Yarn to run Spark jobs now?\n> Is there a guide for me to do so?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66422169.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "modeyang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055", "body": "hi, ashahab-altiscale,\nhas any way or plan  apply docker container to  mapreduce workers within hadoop ? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "oza": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438", "body": "@gliptak please report this issue to JIRA: https://issues.apache.org/jira/browse/YARN\n\nPlease see the for more detail: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731", "body": "@s-bolz thank you for your contribution. Please use Hadoop's JIRA to contribute Hadoop project.\n\nhttps://issues.apache.org/jira/browse/HADOOP\nhttp://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021", "body": "@gliptak yes, these patches have been submitted successfully. Now committers reviewed your patches, so please check them.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717", "body": "@Guchige  thanks for your contribution. We use JIRA instead of github and don't accept any patches via github for now. Could you attach diff file based on the PR? \n\nThe documenation, How To Contribe in Hadoop Wiki, is useful.  http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735", "body": "@kemiya-yx  thanks for your PR. Currently, we accept all contributions via JIRA.\n\nPlease read how to contribute page on wiki: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448", "body": "This PR is for a review.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706", "body": "I found one incompatible change about Jersey - after upgrading from 1.12 to 1.13, the root element whose content is empty collection is changed from null to empty object({}). Related to following change: https://java.net/jira/browse/JERSEY-1168\n\nThis change of lines fixes tests and assertions about JSONObject.NULL to address the change above. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639", "body": "@aajisaka thank you for the comment. The fix I made is just for avoiding warning.  \nCan we do this on another jira? Your comment is a just minor refactoring, so we can do it on separate issue.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158", "body": "@oza could you validate if these are submitted correctly? thanks\n\nhttps://issues.apache.org/jira/browse/YARN-3444\nhttps://issues.apache.org/jira/browse/HADOOP-11801\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "MjAbuz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630", "body": "Learning\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471", "body": "Testing comments on github pull requests.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "yxkemiya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267", "body": "@oza Thanks, I will close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "vesense": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789", "body": "Reported the issue to JIRA: https://issues.apache.org/jira/browse/YARN-4387\nSo, close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "zhe-thoughts": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122", "body": "Thanks for reporting this. Even though the fix is minor, please created a JIRA at https://issues.apache.org/jira/browse/HADOOP\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "bwtakacy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815", "body": "OK.\nI will close this PR.\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "raviprak-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287", "body": "Thanks for your contribution emopers. Could you please follow these steps? https://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "rainforc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169", "body": "Balancer will throw NullPointerException when datanode is down.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "2899722744": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122", "body": "sync\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "h4ck3rm1k3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944", "body": "I am having some problems with further testing. Closing this PR for now until I fix and test more. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034", "body": "https://github.com/h4ck3rm1k3/hadoop-archive-org-bucket-fs/issues/1 this is the problem that I found. Needs more work. Need to create test cases inside of hadoop for this. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tonyzeng20151": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/9905533", "body": "Hi\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/9905533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zhangminglei": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/11830060", "body": "Good\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/11830060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "svnpenn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/13832916", "body": "This line is insane because GnuWin32 doesn\u2019t even provide a shell:\n\nhttp://gnuwin32.sourceforge.net/faq.html#How_do_I_run_shell_scripts\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/13832916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "JunpingDu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/14097788", "body": "Hi @rkanter, do we have a public JIRA to track this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/14097788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "msemelman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/15730983", "body": "It would be nice to see an example.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/15730983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "DasiyShang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/17624303", "body": "just to study\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/17624303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "yuj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/18033277", "body": "Could someone explain why 'others' are not allowed to even read the logs inside App dirs?  Would 0775 work?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/18033277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "tgravescs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/18041857", "body": "others aren't allowed to see logs because the application could be logging something that is secure/sensitive.  ie financial data, passwords, etc.  Ideally they aren't but we need to protect other applications from reading this without explicit authorization.  \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/18041857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "ybank": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/19199596", "body": "Just came across this when I am doing code analysis. Very minor issue, though.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19199596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "xiaoyuyao": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/20178230", "body": "Looks good to me. +1. Can you update the patch on Apache?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/20178230/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429", "body": "Good point. I will address that in the next patch.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "nfouka": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/21319790", "body": ".", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/21319790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "donnyw88": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/25191011", "body": "[license.txt](https://github.com/apache/hadoop/files/1415870/license.txt)\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/25191011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jebat9999": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26379606", "body": "Detail...please...about my payment", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26379606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "1004770753": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26564582", "body": "baga", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26564582/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "PrinceKK300188": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26697240", "body": "Why", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26697240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/26697248", "body": "Fuck", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26697248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adsontag": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26788523", "body": "Kill", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26788523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "cnauroth": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996", "body": "I think you'll still need to incorporate the fixes I suggested earlier in a JIRA comment: remove the extra space character at the end of the `\"target \"` string literal, and switch from `File.pathSeparator` to `File.separator`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "fedecz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407", "body": "As you said, that constant is only used in that test which I did change. I changed it to abstract and created 3 different implementations: one for SSE-S3, SSE-KMS and SSE-C. Basically I'm running all the tests in TestS3AEncryption, but with different encryption algorithms depending on the concrete class.\nYes, it builds and all test pass.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908", "body": "Nope, this is the section _Other Issues_ in the documentation, so I wanted to document if the user was having that warning, he/she should specify the endpoint in the config. I guess I could set a title to describe it better instead of just pasting the warning.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238", "body": "true, but not all of them can be merged. I'm relying in else clauses as well depending on some of the conditions being false. I'll try to rewrite it though and will see how it looks.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364", "body": "will do\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085", "body": "I don't see anything related to this patch in that ticket's patch, are you sure that's the one? I'm looking at the attached patch in HADOOP-13224\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "aw-was-here": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457", "body": "If there is an Xmx in HADOOP_CLIENT_OPTS and an Xmx in MAPRED_DISTCP_OPTS, then the mapred distcp final HADOOP_OPTS will definitely have two Xmx flags.  After HADOOP-13365, we'll be in a position to potentially de-dupe user provided settings like we do for other things.  But until de-dupe, you're correct that it's a JVM decision.  In the past, that decision has been last one wins and I doubt Oracle could change it if they wanted to at this point without major ramifications.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "templedf": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935", "body": "Can we please have one exit point?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007", "body": "Seems like this should be more defensive, i.e. check for the type before casting.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635", "body": "Maybe put the scheduler into the context?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}, "3": {"sunilgovind": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/06cceba1cb07340c412c4467439c16ea6812e685", "message": "YARN-7738. CapacityScheduler: Support refresh maximum allocation for multiple resource types. Contributed by Wangda Tan."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/8e5472b1e63f1c50e253e64702468da2bb38e476", "message": "YARN-7750. [UI2] Render time related fields in all pages to the browser timezone. Contributed by Vasudevan Skm."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "steveloughran": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/1093a73689912f78547e6d23023be2fd1c7ddc85", "message": "HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\nContributed by Aaron Fabbri"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/de630708d1912b3e4fa31e00f5d84a08a580e763", "message": "HADOOP-15123. KDiag tries to load krb5.conf from KRB5CCNAME instead of KRB5_CONFIG.\nContributed by Vipin Rathor.\n\n(cherry picked from commit 1ef906e29e0989aafcb35c51ad2acbb262b3c8e7)\n(cherry picked from commit f61edab1d0ea08b6d752ecdfb6068103822012ec)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f274fe33ea359d26a31efec42a856320a0dbb5f4", "message": "Revert \"HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\"\n\nThis reverts commit 35ad9b1dd279b769381ea1625d9bf776c309c5cb."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a0c71dcc33ca7c5539d0ab61c4a276c4f39e5744", "message": "HADOOP-15079. ITestS3AFileOperationCost#testFakeDirectoryDeletion failing\nafter OutputCommitter patch.\nContributed by Steve Loughran"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900", "body": "wrong JIRA\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477", "body": "Please can you file a separate JIRA for these two changes, link to HADOOP-9991 and include justification. Version updates are a traumatic issue in Hadoop and done fairly reluctantly.\n\nFWIW, updating netty-all from beta to final makes sense just from a release perspective; updating the other jetty less so \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270", "body": "this is three chained conditions which could be merged through `&&`\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678", "body": "could you move the preconditions checks into `S3ObjectAttributes` and invoke them from both output streams? That'd reduce duplicate code and perhaps aid future maintenance\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920", "body": "we're ok with .\\* on static imports here, so you can just skip this bit of the patch\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746", "body": "why was this cut? This is directly referred to in {{TestS3AEncryption}}, a file which this patch doesn't touch. I don't think a clean build of this patch is going to work. Have you tried it?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452", "body": "looks like an accidental paste in of a bit of HADOOP-13224's doc changes\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629", "body": "Well it's being covered in HADOOP-13224, so it's best to pull it here and review that patch instead\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002", "body": "sorry, wrong JIRA. https://issues.apache.org/jira/browse/HADOOP-13324\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957", "body": "might be good to add an example here. e.g what the final options of distcp are going to be. Will there be two -Xmx commands? if so, which wins? Because I suspect that's a JVM decision\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brahmareddybattula": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/08332e12d055d85472f0c9371fefe9b56bfea1ed", "message": "HADOOP-15150. in FsShell, UGI params should be overidden through env vars(-D arg). Contributed by Brahma Reddy Battula."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/880b9d24ff7b5f350ec99bac9b0862009460b486", "message": "HDFS-8693. refreshNamenodes does not support adding a new standby to a running DN. Contributed by Ajith S."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aajisaka": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/cdaf92c9f57560219b8f915a19ad8603ddf2a505", "message": "HADOOP-15177. Update the release year to 2018. Contributed by Bharat Viswanadham."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687", "body": "I've committed the patch in https://issues.apache.org/jira/browse/HADOOP-12081 to trunk and branch-2. Would you close this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562", "body": "Thank you for the pull request. I reviewed the patch (A) and the another patch in YARN-4434 jira (B) and decided to commit the patch (B) because the patch (B) replaces \"i.e. the entire disk\" with \"i.e. 90% of the disk\" as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634", "body": "I've committed the patch (B), so would you close this pull request?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727", "body": "I'm thinking the string concatenation by `+` is unnecessarily.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524", "body": "- The code creates String by `File.getAbsolutePath()` and then creates File by `new File(String)`. It can be simplified by the following and `TEST_ROOT_DIR` can be removed.\n\n```\n private static final File TEST_DIR = new File(GenericTestUtils.getTestDir(), \"fu\");\n```\n- `cacheDir` is unused.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222", "body": "I found some other occurrences of `/tmp/xxx`, which is used as the root dir of `FileContextTestHelper`. Would you replace them? I'm thinking the following is fine:\n\n```\nreturn new FileContextTestHelper(GenericTestUtils.getTempPath(\"TestWebHdfsFileContextMainOperations\"));\n```\n\nI'm okay if the replace is done in separate jira(s).\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827", "body": "It looks to me that the javac warning is related because the patch removes `@SuppressWarnings(\"unchecked\")`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731", "body": "Thank you for updating the pull request! Mostly looks good to me.\n(nit) Unused argument `resourceManager` can be removed. I'm +1 if that is addressed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "szegedim": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/a68e445dc682f4a123cdf016ce1aa46e550c7fdf", "message": "YARN-7717. Add configuration consistency for module.enabled and docker.privileged-containers.enabled. Contributed by Eric Badger."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/41049ba5d129f0fd0953ed8fdeb12635f7546bb2", "message": "YARN-7758. Add an additional check to the validity of container and application ids passed to container-executor. Contributed by Yufei Gu."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2dcfc1876e4d73cf85a6b1b7de694b1b4cc54494", "message": "YARN-7705. Create the container log directory with correct sticky bit in C code. Contributed by Yufei Gu."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vinayakumarb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/09efdfe9e13c9695867ce4034aa6ec970c2032f1", "message": "HDFS-9049. Make Datanode Netty reverse proxy port to be configurable. Contributed by Vinayakumar B."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ajfabbri": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/268ab4e0279b3e40f4a627d3dfe91e2a3523a8cc", "message": "HADOOP-15141 Support IAM Assumed roles in S3A. Contributed by Steve Loughran."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChenSammi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9195a6e302028ed3921d1016ac2fa5754f06ebf0", "message": "HADOOP-15027. AliyunOSS: Support multi-thread pre-read to improve sequential read from Hadoop to Aliyun OSS performance. (Contributed by Jinhu Wu)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flyrain": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/370f1c6283813dc1c7d001f44930e3c79c140c54", "message": "YARN-6486. FairScheduler: Deprecate continuous scheduling. (Contributed by Wilfred Spiegelenburg)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b2029353537fc8da9ab67834568cb2e24924cf5a", "message": "HADOOP-15157. Zookeeper authentication related properties to support CredentialProviders. (Contributed by Gergo Repas)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rkanter": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d716084f4503bf826ef10424d7025ea1ff4ee104", "message": "MAPREDUCE-7032. Add the ability to specify a delayed replication count (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5ac109909a29fab30363b752b5215be7f5dc616b", "message": "YARN-7479. TestContainerManagerSecurity.testContainerManager[Simple] flaky in trunk (ajisakaa via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e404650f489727d2df9a8813fddc4e0d682fbbee", "message": "MAPREDUCE-7030. Uploader tool should ignore symlinks to the same directory (miklos.szegedi@cloudera.com via rkanter)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohithsharmaks": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d09058b2fd18803d12f0835fdf78aef5e0b99c90", "message": "YARN-6736. Consider writing to both ats v1 & v2 from RM for smoother upgrades. Contributed by Aaron Gresch."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mukulhorton": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/2e1e9017aae7d88b443f7efb7217cf3b56ab0075", "message": "HADOOP-15172. Fix the javadoc warning in WriteOperationHelper.java\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tasanuma": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/1a9c5d479e2259ac024da392b020967112c5af55", "message": "MAPREDUCE-7034. Moving logging APIs over to slf4j the rest of all in hadoop-mapreduce\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arp7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/7016dd44e0975274856dc19f19815123c4b2a352", "message": "HDFS-13016. globStatus javadoc refers to glob pattern as \"regular expression\". Contributed by Mukul Kumar Singh."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373", "body": "Consider returning a List.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818", "body": "See HDFS-9478 which is fixing exception handling when constructing callqueue instances. We could use a similar fix for createScheduler.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863", "body": "The dn.getConf() object is not referenced outside the constructor so you can just pass a reference to that object. Also DNConf need not keep a reference to the dn. I think you can just revert all changes to this file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889", "body": "If we revert changes to DNConf we can just replace this with `this.dnConf = new DNConf(getConf())`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "linyiqun": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9afb8025d6549f0ade0ae7d36f5e67cd20c500f4", "message": "HDFS-12972. RBF: Display mount table quota info in Web UI and admin command. Contributed by Yiqun Lin."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangdatan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/edcc3a95d5248883492f2960f4fd22e09612ee9c", "message": "YARN-7468. Provide means for container network policy control. (Xuan Gong via wangda)\n\nChange-Id: I73678c343f663412917758feef35d8308c216e76"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "billierinaldi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/53f2768926700d2a27ce6223f1ccbfd3be49fc29", "message": "YARN-7724. yarn application status should support application name. Contributed by Jian He"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Yiran-wu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/331", "title": "YARN-7773. Fix YARN Federation used Mysql as state store throw except\u2026", "body": "YARN-7773 YARN Federation used Mysql as state store throw exception, Unknown column 'homeSubCluster' in 'field list'\r\n\r\n\r\n#331 \r\n\r\nsubmitApplication appIdapplication_1516277664083_0014 try #0 on SubCluster cluster1 , queue: root.bdp_federation\r\n[2018-01-18T23:25:29.325+08:00] [ERROR] store.impl.SQLFederationStateStore.logAndThrowRetriableException(FederationStateStoreUtils.java 158) [IPC Server handler 44 on 8050] : Unable to insert the newly generated application application_1516277664083_0014\r\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\n[2018-01-18T23:25:29.326+08:00] [ERROR] server.router.RouterServerUtil.logAndThrowException(RouterServerUtil.java 55) [IPC Server handler 44 on 8050] : Unable to insert the ApplicationId application_1516277664083_0014 into the FederationStateStore\r\norg.apache.hadoop.yarn.server.federation.store.exception.FederationStateStoreRetriableException: Unable to insert the newly generated application application_1516277664083_0014\r\nat org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils.logAndThrowRetriableException(FederationStateStoreUtils.java:159)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:593)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\nCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\n... 20 more \r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wuzhilon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/330", "title": "YARN PROXYSERVER throw IOEXCEPTION", "body": "When we more than ten users simultaneously submitted to view the proxyserver, there will be stuck, and then it will throw IO exception", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "skmvasu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/329", "title": "YARN-7760. precompute master node URL", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/328", "title": "YARN-7742 - Remove duplicate entries", "body": "[YARN-7742]", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/327", "title": "YARN-7749. Fix GPU sidebar", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/323", "title": "YARN-7750. Use local time to render dates", "body": "Use local time to render dates", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/315", "title": "YARN-7648. Fix attempts UI when app run fails", "body": "Fixes applications tab rendering when the app attempts have failed", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gerashegalov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/326", "title": "YARN-7747 use injected GuiceFilter instances", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/325", "title": "HADOOP-15166: simplify minicluster start", "body": "add minicluster subcommand to simplify its usage", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xshaun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/324", "title": "Fix NullPointerException caused by null-builder", "body": "Sometimes occurs java.lang.NullPointerException leading to app failed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gehaijiang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/321", "title": "Branch 3.0   operation shell script   ERROR", "body": "run  ./stop-dfs.sh  \r\n\r\ntext:\r\n\r\nStopping namenodes on [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping datanodes\r\n10.50.132.147: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.151: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.146: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.150: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.154: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.145: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.152: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.148: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.149: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.153: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\nStopping journal nodes [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping ZK Failover Controllers on NN hosts [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mmolimar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/319", "title": "HADOOP-15142. Register FTP and SFTP as FS services", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "medb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/318", "title": "[HADOOP-11032] Migrate Guava's Stopwatch to Hadoop's StopWatch", "body": "After this change Hadoop could build against Guava 21.0\r\n\r\nJustification for migration is the same as previous migrations here:\r\nhttps://issues.apache.org/jira/browse/HADOOP-11032 ", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/316", "title": "HADOOP-15124. Improve FileSystem.Statistics performance", "body": "This is PR for https://issues.apache.org/jira/browse/HADOOP-15124", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dwshmilyss": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/317", "title": "Branch 2.8.3", "body": "test\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "denis-zhdanov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/314", "title": "POC: replace explicit method parameters null-checks by a declarative approach", "body": "This *PR* shows an approach when explicit *null*-checks (*Preconditions.checkNotNull()*) are generated automatically by the [Traute](http://traute.oss.harmonysoft.tech/) *javac* plugin for method parameters marked by *Nonnull* annotation.  \r\n\r\nExample: consider the [FSDataOutputStreamBuilder.permission()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java#L151) - its bytecode looks like if it's compiled from the source below:  \r\n\r\n```java\r\npublic B permission(@Nonnull final FsPermission perm) {\r\n    if (perm == null) {\r\n        throw new NullPointerException(\"String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\");\r\n    }\r\n    Preconditions.checkNotNull(perm);\r\n    permission = perm;\r\n    return getThisBuilder();\r\n}\r\n```  \r\n\r\nDetails:  \r\n\r\n```\r\njavap -c ./hadoop-common-project/hadoop-common/target/classes/org/apache/hadoop/fs/FSDataOutputStreamBuilder.class\r\n...\r\n  public B permission(org.apache.hadoop.fs.permission.FsPermission);\r\n    Code:\r\n       0: aload_1\r\n       1: ifnonnull     14\r\n       4: new           #16                 // class java/lang/NullPointerException\r\n       7: dup\r\n       8: ldc           #31                 // String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\r\n      10: invokespecial #18                 // Method java/lang/NullPointerException.\"<init>\":(Ljava/lang/String;)V\r\n      13: athrow\r\n      14: aload_0\r\n      15: aload_1\r\n      16: putfield      #3                  // Field permission:Lorg/apache/hadoop/fs/permission/FsPermission;\r\n      19: aload_0\r\n      20: invokevirtual #32                 // Method getThisBuilder:()Lorg/apache/hadoop/fs/FSDataOutputStreamBuilder;\r\n      23: areturn\r\n```  \r\n\r\nSo, the idea is to do the following:  \r\n1. Go through the project's codebase and find all places where *Preconditions.checkNotNull()* is called for method parameter\r\n2. Ensure that target method parameter is marked by the *Nonnull* annotation (e.g. [ActiveStandbyElector.isStaleClient()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java#L1124) is not marked by it, so, we need to add the annotation)\r\n3. Remove *Preconditions.checkNotNull()* call  \r\n\r\nBenefits:  \r\n* the code becomes cleaner without that explicit checks\r\n* the code is better documented as it's immediately clear what method parameters must be not-*null*\r\n* IDEs highlight possible *NPE* for method parameters marked by *Nonnull* annotations\r\n\r\nPlease let me know if you like the idea, I'm fine with providing a *PR* which applies the solution to the whole project's codebase then.\r\n\r\nTESTED: mvn clean package & ensured that resulting\r\n        bytecode has the checks", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "addisonj": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/311", "title": "[HADOOP-15096] Don't create user with lastlog", "body": "This fixes a problem where in certain cases, the useradd command can create a very large diff that can blow up the host disk size.\r\n\r\nThe reason for this is that lastlog is a sparse file, but AUFS under docker apparently doesn't deal with those well and creates a very large file.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jiayuhan-it": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/309", "title": "MAPREDUCE-7017:Too many times of meaningless invocation in TaskAttemptImpl#resolveHosts", "body": "MRAppMaster uses TaskAttemptImpl::resolveHosts to determine the dataLocalHosts for each task when the location of data split is IP, which will call a lot of times ( taskNum * dfsReplication) of function InetAddress::getByName and most of the funcition calls are redundant. When the job has a great number of tasks and the speed of DNS resolution is not fast enough, it will take a lot of time at this stage before the job running.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vetriselvan1187": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/304", "title": "[YARN-7578] waitForDiskHealthCheck sleep time is extended from 1000ms\u2026", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "animenon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/300", "title": "[HADOOP-15099] YARN Federation Link fix", "body": "The fix is for YARN Federation link on [Apache Hadoop YARN page](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).\r\n\r\nFederation Link was `.Federation.html`, removed the `.`, hence fixing the 404 Error I saw on the site.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rgoers": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/299", "title": "Support Log4j 2 and Logback", "body": "This patch relates to https://issues.apache.org/jira/browse/HADOOP-12956. It makes the logging implementation in hadoop common optional and provides support for event counters in Log4j 2 and Logback in addition to Log4j 1.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maobaolong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/297", "title": "Fix Checkstyle error, rename a argument", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/296", "title": "Fix constants variable name", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/295", "title": "Fix checkstyle problem", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mchataigner": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/290", "title": "HADOOP-14128. fix renameInternal in ChecksumFs", "body": "AbstractFs.rename(source, destination, options) calls\r\nrenameInternal(source, destination, overwrite)\r\n\r\nThis patch adds this method to ChecksumFs to rename the crc file in\r\naddition to the file itself to avoid crc missmatch when use for example\r\nin LocalFs.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuxintan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/288", "title": "HDFS-12749. Catch IOException to schedule BlockReport correctly when \u2026", "body": "\u2026DN re-register. (Contributed by TanYuxin)", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wenxinhe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/287", "title": "HDFS-10323. transient deleteOnExit failure in ViewFileSystem due to close() ordering", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "retroverse": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/286", "title": "Update and rename README.txt to README.md", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ggribeler": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/285", "title": "MAPREDUCE-6752: Bad logging practices in mapreduce", "body": "Changed the log level of the method that is only used for debugging purpose as discussed here:\r\n\r\nhttps://issues.apache.org/jira/browse/MAPREDUCE-6752\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bberton-ciandt": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/284", "title": "HADOOP-14157 fix parseUrl to replace '\\' for '/'", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Markiry": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738", "body": "I'm very sorry, I have some mistakes.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "resouer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298", "body": "I'm really interested in this feature, but why it is not merged yet? \nBut why I can see this patch has been added to 2.6.0?\nhttp://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n\nI'm really confused ....\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169", "body": "So is it possible for me to launch Dockers in Yarn to run Spark jobs now? Is there a guide for me to do so?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203", "body": "Thanks! I'd like to.  \n\nBTW, what's the current status of this great yarn-docker work. \n1. Can I use it now?\n2. Do I need to use customized docker?\n3. What I can do if I want to contribute to make it better?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ashahab-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472", "body": "It has been merged to trunk and branch-2.6\n\nOn Wed, Dec 10, 2014 at 12:01 AM, Harry Zhang notifications@github.com\nwrote:\n\n> I'm really interested in this feature, but why it is not merged yet?\n> But why I can see this patch has been added to 2.6.0?\n> \n> http://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n> \n> I'm really confused ....\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66416298.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694", "body": "No, but feel free to contribute that guide to hadoop.\n\nOn Wed, Dec 10, 2014 at 1:06 AM, Harry Zhang notifications@github.com\nwrote:\n\n> So is it possible for me to launch Dockers in Yarn to run Spark jobs now?\n> Is there a guide for me to do so?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66422169.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "modeyang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055", "body": "hi, ashahab-altiscale,\nhas any way or plan  apply docker container to  mapreduce workers within hadoop ? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "oza": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438", "body": "@gliptak please report this issue to JIRA: https://issues.apache.org/jira/browse/YARN\n\nPlease see the for more detail: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731", "body": "@s-bolz thank you for your contribution. Please use Hadoop's JIRA to contribute Hadoop project.\n\nhttps://issues.apache.org/jira/browse/HADOOP\nhttp://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021", "body": "@gliptak yes, these patches have been submitted successfully. Now committers reviewed your patches, so please check them.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717", "body": "@Guchige  thanks for your contribution. We use JIRA instead of github and don't accept any patches via github for now. Could you attach diff file based on the PR? \n\nThe documenation, How To Contribe in Hadoop Wiki, is useful.  http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735", "body": "@kemiya-yx  thanks for your PR. Currently, we accept all contributions via JIRA.\n\nPlease read how to contribute page on wiki: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448", "body": "This PR is for a review.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706", "body": "I found one incompatible change about Jersey - after upgrading from 1.12 to 1.13, the root element whose content is empty collection is changed from null to empty object({}). Related to following change: https://java.net/jira/browse/JERSEY-1168\n\nThis change of lines fixes tests and assertions about JSONObject.NULL to address the change above. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639", "body": "@aajisaka thank you for the comment. The fix I made is just for avoiding warning.  \nCan we do this on another jira? Your comment is a just minor refactoring, so we can do it on separate issue.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158", "body": "@oza could you validate if these are submitted correctly? thanks\n\nhttps://issues.apache.org/jira/browse/YARN-3444\nhttps://issues.apache.org/jira/browse/HADOOP-11801\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "MjAbuz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630", "body": "Learning\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471", "body": "Testing comments on github pull requests.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "yxkemiya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267", "body": "@oza Thanks, I will close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "vesense": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789", "body": "Reported the issue to JIRA: https://issues.apache.org/jira/browse/YARN-4387\nSo, close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "zhe-thoughts": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122", "body": "Thanks for reporting this. Even though the fix is minor, please created a JIRA at https://issues.apache.org/jira/browse/HADOOP\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "bwtakacy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815", "body": "OK.\nI will close this PR.\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "raviprak-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287", "body": "Thanks for your contribution emopers. Could you please follow these steps? https://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "rainforc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169", "body": "Balancer will throw NullPointerException when datanode is down.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "2899722744": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122", "body": "sync\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "h4ck3rm1k3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944", "body": "I am having some problems with further testing. Closing this PR for now until I fix and test more. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034", "body": "https://github.com/h4ck3rm1k3/hadoop-archive-org-bucket-fs/issues/1 this is the problem that I found. Needs more work. Need to create test cases inside of hadoop for this. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "cnauroth": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996", "body": "I think you'll still need to incorporate the fixes I suggested earlier in a JIRA comment: remove the extra space character at the end of the `\"target \"` string literal, and switch from `File.pathSeparator` to `File.separator`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "xiaoyuyao": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429", "body": "Good point. I will address that in the next patch.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "fedecz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407", "body": "As you said, that constant is only used in that test which I did change. I changed it to abstract and created 3 different implementations: one for SSE-S3, SSE-KMS and SSE-C. Basically I'm running all the tests in TestS3AEncryption, but with different encryption algorithms depending on the concrete class.\nYes, it builds and all test pass.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908", "body": "Nope, this is the section _Other Issues_ in the documentation, so I wanted to document if the user was having that warning, he/she should specify the endpoint in the config. I guess I could set a title to describe it better instead of just pasting the warning.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238", "body": "true, but not all of them can be merged. I'm relying in else clauses as well depending on some of the conditions being false. I'll try to rewrite it though and will see how it looks.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364", "body": "will do\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085", "body": "I don't see anything related to this patch in that ticket's patch, are you sure that's the one? I'm looking at the attached patch in HADOOP-13224\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "aw-was-here": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457", "body": "If there is an Xmx in HADOOP_CLIENT_OPTS and an Xmx in MAPRED_DISTCP_OPTS, then the mapred distcp final HADOOP_OPTS will definitely have two Xmx flags.  After HADOOP-13365, we'll be in a position to potentially de-dupe user provided settings like we do for other things.  But until de-dupe, you're correct that it's a JVM decision.  In the past, that decision has been last one wins and I doubt Oracle could change it if they wanted to at this point without major ramifications.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "templedf": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935", "body": "Can we please have one exit point?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007", "body": "Seems like this should be more defensive, i.e. check for the type before casting.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635", "body": "Maybe put the scheduler into the context?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}, "4": {"sunilgovind": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/06cceba1cb07340c412c4467439c16ea6812e685", "message": "YARN-7738. CapacityScheduler: Support refresh maximum allocation for multiple resource types. Contributed by Wangda Tan."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/8e5472b1e63f1c50e253e64702468da2bb38e476", "message": "YARN-7750. [UI2] Render time related fields in all pages to the browser timezone. Contributed by Vasudevan Skm."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "steveloughran": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/1093a73689912f78547e6d23023be2fd1c7ddc85", "message": "HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\nContributed by Aaron Fabbri"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/de630708d1912b3e4fa31e00f5d84a08a580e763", "message": "HADOOP-15123. KDiag tries to load krb5.conf from KRB5CCNAME instead of KRB5_CONFIG.\nContributed by Vipin Rathor.\n\n(cherry picked from commit 1ef906e29e0989aafcb35c51ad2acbb262b3c8e7)\n(cherry picked from commit f61edab1d0ea08b6d752ecdfb6068103822012ec)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/f274fe33ea359d26a31efec42a856320a0dbb5f4", "message": "Revert \"HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\"\n\nThis reverts commit 35ad9b1dd279b769381ea1625d9bf776c309c5cb."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/a0c71dcc33ca7c5539d0ab61c4a276c4f39e5744", "message": "HADOOP-15079. ITestS3AFileOperationCost#testFakeDirectoryDeletion failing\nafter OutputCommitter patch.\nContributed by Steve Loughran"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900", "body": "wrong JIRA\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/158895900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477", "body": "Please can you file a separate JIRA for these two changes, link to HADOOP-9991 and include justification. Version updates are a traumatic issue in Hadoop and done fairly reluctantly.\n\nFWIW, updating netty-all from beta to final makes sense just from a release perspective; updating the other jetty less so \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/197274477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270", "body": "this is three chained conditions which could be merged through `&&`\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274270/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678", "body": "could you move the preconditions checks into `S3ObjectAttributes` and invoke them from both output streams? That'd reduce duplicate code and perhaps aid future maintenance\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274678/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920", "body": "we're ok with .\\* on static imports here, so you can just skip this bit of the patch\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70274920/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746", "body": "why was this cut? This is directly referred to in {{TestS3AEncryption}}, a file which this patch doesn't touch. I don't think a clean build of this patch is going to work. Have you tried it?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70273746/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452", "body": "looks like an accidental paste in of a bit of HADOOP-13224's doc changes\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70278452/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629", "body": "Well it's being covered in HADOOP-13224, so it's best to pull it here and review that patch instead\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70433629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002", "body": "sorry, wrong JIRA. https://issues.apache.org/jira/browse/HADOOP-13324\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70463002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957", "body": "might be good to add an example here. e.g what the final options of distcp are going to be. Will there be two -Xmx commands? if so, which wins? Because I suspect that's a JVM decision\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78158957/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brahmareddybattula": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/08332e12d055d85472f0c9371fefe9b56bfea1ed", "message": "HADOOP-15150. in FsShell, UGI params should be overidden through env vars(-D arg). Contributed by Brahma Reddy Battula."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/880b9d24ff7b5f350ec99bac9b0862009460b486", "message": "HDFS-8693. refreshNamenodes does not support adding a new standby to a running DN. Contributed by Ajith S."}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/14279774", "body": "Hi Vinod,\n\nCan I get the Jira Id for this commit..?\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/14279774/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19446926", "body": "again \"import javax.servlet.ServletContext\" is missed which brokes branch-2 compliation and MetricsServlet changes are not present.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19446926/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19461785", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19461785/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19461789", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19461789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/19462196", "body": "Please update the CHANGES.txt\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19462196/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/22364971", "body": "looks jjira id is missed in commit message", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/22364971/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aajisaka": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/cdaf92c9f57560219b8f915a19ad8603ddf2a505", "message": "HADOOP-15177. Update the release year to 2018. Contributed by Bharat Viswanadham."}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687", "body": "I've committed the patch in https://issues.apache.org/jira/browse/HADOOP-12081 to trunk and branch-2. Would you close this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/123165687/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562", "body": "Thank you for the pull request. I reviewed the patch (A) and the another patch in YARN-4434 jira (B) and decided to commit the patch (B) because the patch (B) replaces \"i.e. the entire disk\" with \"i.e. 90% of the disk\" as well.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142562/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634", "body": "I've committed the patch (B), so would you close this pull request?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163142634/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727", "body": "I'm thinking the string concatenation by `+` is unnecessarily.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58334727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524", "body": "- The code creates String by `File.getAbsolutePath()` and then creates File by `new File(String)`. It can be simplified by the following and `TEST_ROOT_DIR` can be removed.\n\n```\n private static final File TEST_DIR = new File(GenericTestUtils.getTestDir(), \"fu\");\n```\n- `cacheDir` is unused.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/58492524/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222", "body": "I found some other occurrences of `/tmp/xxx`, which is used as the root dir of `FileContextTestHelper`. Would you replace them? I'm thinking the following is fine:\n\n```\nreturn new FileContextTestHelper(GenericTestUtils.getTempPath(\"TestWebHdfsFileContextMainOperations\"));\n```\n\nI'm okay if the replace is done in separate jira(s).\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/59335222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827", "body": "It looks to me that the javac warning is related because the patch removes `@SuppressWarnings(\"unchecked\")`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/62836827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731", "body": "Thank you for updating the pull request! Mostly looks good to me.\n(nit) Unused argument `resourceManager` can be removed. I'm +1 if that is addressed.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67744731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}, "szegedim": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/a68e445dc682f4a123cdf016ce1aa46e550c7fdf", "message": "YARN-7717. Add configuration consistency for module.enabled and docker.privileged-containers.enabled. Contributed by Eric Badger."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/41049ba5d129f0fd0953ed8fdeb12635f7546bb2", "message": "YARN-7758. Add an additional check to the validity of container and application ids passed to container-executor. Contributed by Yufei Gu."}, {"url": "https://api.github.com/repos/apache/hadoop/commits/2dcfc1876e4d73cf85a6b1b7de694b1b4cc54494", "message": "YARN-7705. Create the container log directory with correct sticky bit in C code. Contributed by Yufei Gu."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vinayakumarb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/09efdfe9e13c9695867ce4034aa6ec970c2032f1", "message": "HDFS-9049. Make Datanode Netty reverse proxy port to be configurable. Contributed by Vinayakumar B."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ajfabbri": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/268ab4e0279b3e40f4a627d3dfe91e2a3523a8cc", "message": "HADOOP-15141 Support IAM Assumed roles in S3A. Contributed by Steve Loughran."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChenSammi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9195a6e302028ed3921d1016ac2fa5754f06ebf0", "message": "HADOOP-15027. AliyunOSS: Support multi-thread pre-read to improve sequential read from Hadoop to Aliyun OSS performance. (Contributed by Jinhu Wu)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flyrain": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/370f1c6283813dc1c7d001f44930e3c79c140c54", "message": "YARN-6486. FairScheduler: Deprecate continuous scheduling. (Contributed by Wilfred Spiegelenburg)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/b2029353537fc8da9ab67834568cb2e24924cf5a", "message": "HADOOP-15157. Zookeeper authentication related properties to support CredentialProviders. (Contributed by Gergo Repas)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rkanter": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d716084f4503bf826ef10424d7025ea1ff4ee104", "message": "MAPREDUCE-7032. Add the ability to specify a delayed replication count (miklos.szegedi@cloudera.com via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/5ac109909a29fab30363b752b5215be7f5dc616b", "message": "YARN-7479. TestContainerManagerSecurity.testContainerManager[Simple] flaky in trunk (ajisakaa via rkanter)"}, {"url": "https://api.github.com/repos/apache/hadoop/commits/e404650f489727d2df9a8813fddc4e0d682fbbee", "message": "MAPREDUCE-7030. Uploader tool should ignore symlinks to the same directory (miklos.szegedi@cloudera.com via rkanter)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohithsharmaks": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/d09058b2fd18803d12f0835fdf78aef5e0b99c90", "message": "YARN-6736. Consider writing to both ats v1 & v2 from RM for smoother upgrades. Contributed by Aaron Gresch."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mukulhorton": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/2e1e9017aae7d88b443f7efb7217cf3b56ab0075", "message": "HADOOP-15172. Fix the javadoc warning in WriteOperationHelper.java\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tasanuma": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/1a9c5d479e2259ac024da392b020967112c5af55", "message": "MAPREDUCE-7034. Moving logging APIs over to slf4j the rest of all in hadoop-mapreduce\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "arp7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/7016dd44e0975274856dc19f19815123c4b2a352", "message": "HDFS-13016. globStatus javadoc refers to glob pattern as \"regular expression\". Contributed by Mukul Kumar Singh."}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/20178778", "body": "Done, thanks a lot again @xiaoyuyao!", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/20178778/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373", "body": "Consider returning a List.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606373/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818", "body": "See HDFS-9478 which is fixing exception handling when constructing callqueue instances. We could use a similar fix for createScheduler.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57606818/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 1}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863", "body": "The dn.getConf() object is not referenced outside the constructor so you can just pass a reference to that object. Also DNConf need not keep a reference to the dn. I think you can just revert all changes to this file.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889", "body": "If we revert changes to DNConf we can just replace this with `this.dnConf = new DNConf(getConf())`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/60831889/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "linyiqun": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/9afb8025d6549f0ade0ae7d36f5e67cd20c500f4", "message": "HDFS-12972. RBF: Display mount table quota info in Web UI and admin command. Contributed by Yiqun Lin."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangdatan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/edcc3a95d5248883492f2960f4fd22e09612ee9c", "message": "YARN-7468. Provide means for container network policy control. (Xuan Gong via wangda)\n\nChange-Id: I73678c343f663412917758feef35d8308c216e76"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "billierinaldi": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/hadoop/commits/53f2768926700d2a27ce6223f1ccbfd3be49fc29", "message": "YARN-7724. yarn application status should support application name. Contributed by Jian He"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Yiran-wu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/331", "title": "YARN-7773. Fix YARN Federation used Mysql as state store throw except\u2026", "body": "YARN-7773 YARN Federation used Mysql as state store throw exception, Unknown column 'homeSubCluster' in 'field list'\r\n\r\n\r\n#331 \r\n\r\nsubmitApplication appIdapplication_1516277664083_0014 try #0 on SubCluster cluster1 , queue: root.bdp_federation\r\n[2018-01-18T23:25:29.325+08:00] [ERROR] store.impl.SQLFederationStateStore.logAndThrowRetriableException(FederationStateStoreUtils.java 158) [IPC Server handler 44 on 8050] : Unable to insert the newly generated application application_1516277664083_0014\r\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\n[2018-01-18T23:25:29.326+08:00] [ERROR] server.router.RouterServerUtil.logAndThrowException(RouterServerUtil.java 55) [IPC Server handler 44 on 8050] : Unable to insert the ApplicationId application_1516277664083_0014 into the FederationStateStore\r\norg.apache.hadoop.yarn.server.federation.store.exception.FederationStateStoreRetriableException: Unable to insert the newly generated application application_1516277664083_0014\r\nat org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils.logAndThrowRetriableException(FederationStateStoreUtils.java:159)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:593)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\nat com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source)\r\nat org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345)\r\nat org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334)\r\nat org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196)\r\nat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218)\r\nat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076)\r\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070)\r\nCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list'\r\nat sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source)\r\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:425)\r\nat com.mysql.jdbc.Util.getInstance(Util.java:408)\r\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)\r\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909)\r\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)\r\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680)\r\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)\r\nat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079)\r\nat com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013)\r\nat com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104)\r\nat com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418)\r\nat com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887)\r\nat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\nat com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java)\r\nat org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547)\r\n... 20 more \r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wuzhilon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/330", "title": "YARN PROXYSERVER throw IOEXCEPTION", "body": "When we more than ten users simultaneously submitted to view the proxyserver, there will be stuck, and then it will throw IO exception", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "skmvasu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/329", "title": "YARN-7760. precompute master node URL", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/328", "title": "YARN-7742 - Remove duplicate entries", "body": "[YARN-7742]", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/327", "title": "YARN-7749. Fix GPU sidebar", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/323", "title": "YARN-7750. Use local time to render dates", "body": "Use local time to render dates", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/315", "title": "YARN-7648. Fix attempts UI when app run fails", "body": "Fixes applications tab rendering when the app attempts have failed", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gerashegalov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/326", "title": "YARN-7747 use injected GuiceFilter instances", "body": "", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/325", "title": "HADOOP-15166: simplify minicluster start", "body": "add minicluster subcommand to simplify its usage", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xshaun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/324", "title": "Fix NullPointerException caused by null-builder", "body": "Sometimes occurs java.lang.NullPointerException leading to app failed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gehaijiang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/321", "title": "Branch 3.0   operation shell script   ERROR", "body": "run  ./stop-dfs.sh  \r\n\r\ntext:\r\n\r\nStopping namenodes on [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping datanodes\r\n10.50.132.147: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.151: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.146: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.150: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.154: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.145: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.152: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.148: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.149: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\n10.50.132.153: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.\r\nStopping journal nodes [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.\r\nStopping ZK Failover Controllers on NN hosts [10.50.132.145 10.50.132.146 10.50.132.147]\r\nERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mmolimar": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/319", "title": "HADOOP-15142. Register FTP and SFTP as FS services", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "medb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/318", "title": "[HADOOP-11032] Migrate Guava's Stopwatch to Hadoop's StopWatch", "body": "After this change Hadoop could build against Guava 21.0\r\n\r\nJustification for migration is the same as previous migrations here:\r\nhttps://issues.apache.org/jira/browse/HADOOP-11032 ", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/316", "title": "HADOOP-15124. Improve FileSystem.Statistics performance", "body": "This is PR for https://issues.apache.org/jira/browse/HADOOP-15124", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dwshmilyss": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/317", "title": "Branch 2.8.3", "body": "test\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "denis-zhdanov": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/314", "title": "POC: replace explicit method parameters null-checks by a declarative approach", "body": "This *PR* shows an approach when explicit *null*-checks (*Preconditions.checkNotNull()*) are generated automatically by the [Traute](http://traute.oss.harmonysoft.tech/) *javac* plugin for method parameters marked by *Nonnull* annotation.  \r\n\r\nExample: consider the [FSDataOutputStreamBuilder.permission()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java#L151) - its bytecode looks like if it's compiled from the source below:  \r\n\r\n```java\r\npublic B permission(@Nonnull final FsPermission perm) {\r\n    if (perm == null) {\r\n        throw new NullPointerException(\"String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\");\r\n    }\r\n    Preconditions.checkNotNull(perm);\r\n    permission = perm;\r\n    return getThisBuilder();\r\n}\r\n```  \r\n\r\nDetails:  \r\n\r\n```\r\njavap -c ./hadoop-common-project/hadoop-common/target/classes/org/apache/hadoop/fs/FSDataOutputStreamBuilder.class\r\n...\r\n  public B permission(org.apache.hadoop.fs.permission.FsPermission);\r\n    Code:\r\n       0: aload_1\r\n       1: ifnonnull     14\r\n       4: new           #16                 // class java/lang/NullPointerException\r\n       7: dup\r\n       8: ldc           #31                 // String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it\r\n      10: invokespecial #18                 // Method java/lang/NullPointerException.\"<init>\":(Ljava/lang/String;)V\r\n      13: athrow\r\n      14: aload_0\r\n      15: aload_1\r\n      16: putfield      #3                  // Field permission:Lorg/apache/hadoop/fs/permission/FsPermission;\r\n      19: aload_0\r\n      20: invokevirtual #32                 // Method getThisBuilder:()Lorg/apache/hadoop/fs/FSDataOutputStreamBuilder;\r\n      23: areturn\r\n```  \r\n\r\nSo, the idea is to do the following:  \r\n1. Go through the project's codebase and find all places where *Preconditions.checkNotNull()* is called for method parameter\r\n2. Ensure that target method parameter is marked by the *Nonnull* annotation (e.g. [ActiveStandbyElector.isStaleClient()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java#L1124) is not marked by it, so, we need to add the annotation)\r\n3. Remove *Preconditions.checkNotNull()* call  \r\n\r\nBenefits:  \r\n* the code becomes cleaner without that explicit checks\r\n* the code is better documented as it's immediately clear what method parameters must be not-*null*\r\n* IDEs highlight possible *NPE* for method parameters marked by *Nonnull* annotations\r\n\r\nPlease let me know if you like the idea, I'm fine with providing a *PR* which applies the solution to the whole project's codebase then.\r\n\r\nTESTED: mvn clean package & ensured that resulting\r\n        bytecode has the checks", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "addisonj": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/311", "title": "[HADOOP-15096] Don't create user with lastlog", "body": "This fixes a problem where in certain cases, the useradd command can create a very large diff that can blow up the host disk size.\r\n\r\nThe reason for this is that lastlog is a sparse file, but AUFS under docker apparently doesn't deal with those well and creates a very large file.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jiayuhan-it": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/309", "title": "MAPREDUCE-7017:Too many times of meaningless invocation in TaskAttemptImpl#resolveHosts", "body": "MRAppMaster uses TaskAttemptImpl::resolveHosts to determine the dataLocalHosts for each task when the location of data split is IP, which will call a lot of times ( taskNum * dfsReplication) of function InetAddress::getByName and most of the funcition calls are redundant. When the job has a great number of tasks and the speed of DNS resolution is not fast enough, it will take a lot of time at this stage before the job running.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vetriselvan1187": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/304", "title": "[YARN-7578] waitForDiskHealthCheck sleep time is extended from 1000ms\u2026", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "animenon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/300", "title": "[HADOOP-15099] YARN Federation Link fix", "body": "The fix is for YARN Federation link on [Apache Hadoop YARN page](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).\r\n\r\nFederation Link was `.Federation.html`, removed the `.`, hence fixing the 404 Error I saw on the site.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rgoers": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/299", "title": "Support Log4j 2 and Logback", "body": "This patch relates to https://issues.apache.org/jira/browse/HADOOP-12956. It makes the logging implementation in hadoop common optional and provides support for event counters in Log4j 2 and Logback in addition to Log4j 1.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maobaolong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/297", "title": "Fix Checkstyle error, rename a argument", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/296", "title": "Fix constants variable name", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/295", "title": "Fix checkstyle problem", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mchataigner": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/290", "title": "HADOOP-14128. fix renameInternal in ChecksumFs", "body": "AbstractFs.rename(source, destination, options) calls\r\nrenameInternal(source, destination, overwrite)\r\n\r\nThis patch adds this method to ChecksumFs to rename the crc file in\r\naddition to the file itself to avoid crc missmatch when use for example\r\nin LocalFs.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuxintan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/288", "title": "HDFS-12749. Catch IOException to schedule BlockReport correctly when \u2026", "body": "\u2026DN re-register. (Contributed by TanYuxin)", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wenxinhe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/287", "title": "HDFS-10323. transient deleteOnExit failure in ViewFileSystem due to close() ordering", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "retroverse": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/286", "title": "Update and rename README.txt to README.md", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ggribeler": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/285", "title": "MAPREDUCE-6752: Bad logging practices in mapreduce", "body": "Changed the log level of the method that is only used for debugging purpose as discussed here:\r\n\r\nhttps://issues.apache.org/jira/browse/MAPREDUCE-6752\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bberton-ciandt": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/284", "title": "HADOOP-14157 fix parseUrl to replace '\\' for '/'", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Markiry": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738", "body": "I'm very sorry, I have some mistakes.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/62241738/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "resouer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298", "body": "I'm really interested in this feature, but why it is not merged yet? \nBut why I can see this patch has been added to 2.6.0?\nhttp://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n\nI'm really confused ....\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416298/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169", "body": "So is it possible for me to launch Dockers in Yarn to run Spark jobs now? Is there a guide for me to do so?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66422169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203", "body": "Thanks! I'd like to.  \n\nBTW, what's the current status of this great yarn-docker work. \n1. Can I use it now?\n2. Do I need to use customized docker?\n3. What I can do if I want to contribute to make it better?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66567203/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ashahab-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472", "body": "It has been merged to trunk and branch-2.6\n\nOn Wed, Dec 10, 2014 at 12:01 AM, Harry Zhang notifications@github.com\nwrote:\n\n> I'm really interested in this feature, but why it is not merged yet?\n> But why I can see this patch has been added to 2.6.0?\n> \n> http://hadoop.apache.org/releases.html#18+November%2C+2014%3A+Release+2.6.0+available\n> \n> I'm really confused ....\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66416298.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66416472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694", "body": "No, but feel free to contribute that guide to hadoop.\n\nOn Wed, Dec 10, 2014 at 1:06 AM, Harry Zhang notifications@github.com\nwrote:\n\n> So is it possible for me to launch Dockers in Yarn to run Spark jobs now?\n> Is there a guide for me to do so?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/hadoop/pull/7#issuecomment-66422169.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/66424694/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "modeyang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055", "body": "hi, ashahab-altiscale,\nhas any way or plan  apply docker container to  mapreduce workers within hadoop ? \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/67595055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "oza": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438", "body": "@gliptak please report this issue to JIRA: https://issues.apache.org/jira/browse/YARN\n\nPlease see the for more detail: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/84055438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731", "body": "@s-bolz thank you for your contribution. Please use Hadoop's JIRA to contribute Hadoop project.\n\nhttps://issues.apache.org/jira/browse/HADOOP\nhttp://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/87997731/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021", "body": "@gliptak yes, these patches have been submitted successfully. Now committers reviewed your patches, so please check them.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/90585021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717", "body": "@Guchige  thanks for your contribution. We use JIRA instead of github and don't accept any patches via github for now. Could you attach diff file based on the PR? \n\nThe documenation, How To Contribe in Hadoop Wiki, is useful.  http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/107206717/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735", "body": "@kemiya-yx  thanks for your PR. Currently, we accept all contributions via JIRA.\n\nPlease read how to contribute page on wiki: http://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/155797735/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448", "body": "This PR is for a review.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/184275448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706", "body": "I found one incompatible change about Jersey - after upgrading from 1.12 to 1.13, the root element whose content is empty collection is changed from null to empty object({}). Related to following change: https://java.net/jira/browse/JERSEY-1168\n\nThis change of lines fixes tests and assertions about JSONObject.NULL to address the change above. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/52916706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639", "body": "@aajisaka thank you for the comment. The fix I made is just for avoiding warning.  \nCan we do this on another jira? Your comment is a just minor refactoring, so we can do it on separate issue.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/67776639/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "gliptak": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158", "body": "@oza could you validate if these are submitted correctly? thanks\n\nhttps://issues.apache.org/jira/browse/YARN-3444\nhttps://issues.apache.org/jira/browse/HADOOP-11801\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/89670158/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "MjAbuz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630", "body": "Learning\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/137352630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "omalley": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471", "body": "Testing comments on github pull requests.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/152589471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "yxkemiya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267", "body": "@oza Thanks, I will close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/156015267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "vesense": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789", "body": "Reported the issue to JIRA: https://issues.apache.org/jira/browse/YARN-4387\nSo, close this PR.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/159192789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commit_comments": [], "review_comments": []}, "zhe-thoughts": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122", "body": "Thanks for reporting this. Even though the fix is minor, please created a JIRA at https://issues.apache.org/jira/browse/HADOOP\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163342122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "bwtakacy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815", "body": "OK.\nI will close this PR.\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/163488815/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "raviprak-altiscale": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287", "body": "Thanks for your contribution emopers. Could you please follow these steps? https://wiki.apache.org/hadoop/HowToContribute\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/168770287/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "rainforc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169", "body": "Balancer will throw NullPointerException when datanode is down.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/177856169/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "2899722744": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122", "body": "sync\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/182214122/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "h4ck3rm1k3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944", "body": "I am having some problems with further testing. Closing this PR for now until I fix and test more. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187164944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034", "body": "https://github.com/h4ck3rm1k3/hadoop-archive-org-bucket-fs/issues/1 this is the problem that I found. Needs more work. Need to create test cases inside of hadoop for this. \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/issues/comments/187167034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tonyzeng20151": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/9905533", "body": "Hi\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/9905533/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zhangminglei": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/11830060", "body": "Good\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/11830060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "svnpenn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/13832916", "body": "This line is insane because GnuWin32 doesn\u2019t even provide a shell:\n\nhttp://gnuwin32.sourceforge.net/faq.html#How_do_I_run_shell_scripts\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/13832916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "JunpingDu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/14097788", "body": "Hi @rkanter, do we have a public JIRA to track this issue?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/14097788/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "msemelman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/15730983", "body": "It would be nice to see an example.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/15730983/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "DasiyShang": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/17624303", "body": "just to study\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/17624303/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "yuj": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/18033277", "body": "Could someone explain why 'others' are not allowed to even read the logs inside App dirs?  Would 0775 work?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/18033277/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "tgravescs": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/18041857", "body": "others aren't allowed to see logs because the application could be logging something that is secure/sensitive.  ie financial data, passwords, etc.  Ideally they aren't but we need to protect other applications from reading this without explicit authorization.  \n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/18041857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "ybank": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/19199596", "body": "Just came across this when I am doing code analysis. Very minor issue, though.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/19199596/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "xiaoyuyao": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/20178230", "body": "Looks good to me. +1. Can you update the patch on Apache?", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/20178230/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429", "body": "Good point. I will address that in the next patch.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/57619429/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "nfouka": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/21319790", "body": ".", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/21319790/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "donnyw88": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/25191011", "body": "[license.txt](https://github.com/apache/hadoop/files/1415870/license.txt)\r\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/25191011/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jebat9999": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26379606", "body": "Detail...please...about my payment", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26379606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "1004770753": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26564582", "body": "baga", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26564582/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "PrinceKK300188": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26697240", "body": "Why", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26697240/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/comments/26697248", "body": "Fuck", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26697248/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adsontag": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/hadoop/comments/26788523", "body": "Kill", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/comments/26788523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "cnauroth": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996", "body": "I think you'll still need to incorporate the fixes I suggested earlier in a JIRA comment: remove the extra space character at the end of the `\"target \"` string literal, and switch from `File.pathSeparator` to `File.separator`.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/45787996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "fedecz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407", "body": "As you said, that constant is only used in that test which I did change. I changed it to abstract and created 3 different implementations: one for SSE-S3, SSE-KMS and SSE-C. Basically I'm running all the tests in TestS3AEncryption, but with different encryption algorithms depending on the concrete class.\nYes, it builds and all test pass.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70430407/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908", "body": "Nope, this is the section _Other Issues_ in the documentation, so I wanted to document if the user was having that warning, he/she should specify the endpoint in the config. I guess I could set a title to describe it better instead of just pasting the warning.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70431908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238", "body": "true, but not all of them can be merged. I'm relying in else clauses as well depending on some of the conditions being false. I'll try to rewrite it though and will see how it looks.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432238/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364", "body": "will do\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70432364/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085", "body": "I don't see anything related to this patch in that ticket's patch, are you sure that's the one? I'm looking at the attached patch in HADOOP-13224\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/70439085/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "aw-was-here": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457", "body": "If there is an Xmx in HADOOP_CLIENT_OPTS and an Xmx in MAPRED_DISTCP_OPTS, then the mapred distcp final HADOOP_OPTS will definitely have two Xmx flags.  After HADOOP-13365, we'll be in a position to potentially de-dupe user provided settings like we do for other things.  But until de-dupe, you're correct that it's a JVM decision.  In the past, that decision has been last one wins and I doubt Oracle could change it if they wanted to at this point without major ramifications.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/78162457/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "templedf": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935", "body": "Can we please have one exit point?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77861935/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007", "body": "Seems like this should be more defensive, i.e. check for the type before casting.\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77863007/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635", "body": "Maybe put the scheduler into the context?\n", "reactions": {"url": "https://api.github.com/repos/apache/hadoop/pulls/comments/77864635/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}}}