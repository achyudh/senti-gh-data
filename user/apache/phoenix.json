{"_default": {"1": {"karanmehta93": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/e3faa954952fbe6f9ea5a9e792a5d275d9193a53", "message": "PHOENIX-4528 PhoenixAccessController checks permissions only at table level when creating views"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c075a17879309fe4def5a621951d72929fb10c3a", "message": "PHOENIX-4424 Allow users to create \"DEFAULT\" and \"HBASE\" Schema (Uppercase Schema Names)"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/289", "title": "PHOENIX-4528 PhoenixAccessController checks permissions only at table\u2026", "body": "\u2026 level when creating views\r\n\r\n@ankitsinghal @twdsilva Please review.\r\n\r\n@ankitsinghal Please suggest new tests that can be added to verify this patch. The test that I added only verifies that create views would succeed. The change that I have made is generic, however it will be good to add tests that cover scenarios that include creation or dropping of index tables.", "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/284", "title": "PHOENIX-4424 Allow users to create DEFAULT and HBASE Schema (Uppercas\u2026", "body": "\u2026e Schema Names)\r\n\r\n@twdsilva Please review.", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lhofhansl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/2d655cdc723c1d69ed9ff556438727e378ac1ed5", "message": "PHOENIX-4076 Move master branch up to HBase 1.4.0. (Andrew Purtell and Lars Hofhansl)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jtaylor-sfdc": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/27d6582827b9306e66d3bfd430c6186ac165fb08", "message": "PHOENIX-4523 phoenix.schema.isNamespaceMappingEnabled problem (Karan Mehta)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/83adf0d1a6e75c97f8ed696340c7af167834f7e9", "message": "PHOENIX-4514 A incorrect key object is used in SequenceManager#validateSequences (Chia-Ping Tsai)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/5f733b38978c7caaab3428fa5680eab614f56cb2", "message": "PHOENIX-4522 Fail to remove the schema from client-side cache (Chia-Ping Tsai)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/2759727e444ef8ae7475d25979be84bfe89895f5", "message": "PHOENIX-4487 Missing SYSTEM.MUTEX table upgrading from 4.7 to 4.13"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/1636f6182ae89e9c4b97c877aa919a6edac5bbc2", "message": "PHOENIX-4488 Cache config parameters for MetaDataEndPointImpl during initialization"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/d6e61af807f7a4e605c61217bac556ffe00ea237", "message": "PHOENIX-4415 Ignore CURRENT_SCN property if set in Pig Storer"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ss77892": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/3035fb11b8523c68b70e55d9a0fd1646eb6d15cf", "message": "PHOENIX-4525 Integer overflow in GroupBy execution"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/90c7241611667e3cd3689ce6a72762c6315231ef", "message": "PHOENIX-4456 queryserver script doesn't perform as expected."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ee728a4d19c004ad456b24cd228fb2351362472d", "message": "PHOENIX-4439 QueryServer pid file name doesn't comply the usual schema we are using in hadoop ecosystem"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "maryannxue": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/b3854d2c16fa102e087ecdfba6f366ba4d174dc6", "message": "PHOENIX-4508 Order-by not optimized in sort-merge-join on salted tables"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/412329a7415302831954891285d291055328c28b", "message": "PHOENIX-4437 Make QueryPlan.getEstimatedBytesToScan() independent of getExplainPlan() and pull optimize() out of getExplainPlan()"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/281", "title": "PHOENIX-4288 Indexes not used when ordering by primary key", "body": "1. Add class Cost.\r\n2. Add method getCost() in QueryPlan.\r\n3. Let QueryOptimizer choose the best plan based on Cost; meanwhile if stats are not available the QueryOptimizer will keep the original behavior.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/278", "title": "PHOENIX-4322 DESC primary key column with variable length does not work in SkipScanFilter", "body": "Changes:\r\nAvoid adding an extra trailing separator to the key", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "joshelser": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/2136b002c37db478ffea11233f9ebb80276d2594", "message": "PHOENIX-4466 Do not relocate hadoop code (addendum)\n\nTurns out relocating hadoop-common (most obviously) breaks\nsome security-related classes in hadoop-common around Kerberos logins."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/93306e9e28a0f13cbac87055c30fb9a781ae3345", "message": "PHOENIX-4510 Fix performance.py issue in not finding tests jar (Artem Ervits)\n\nSigned-off-by: Josh Elser <elserj@apache.org>"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/0584d3cb2c2e65c56381f33d93360e64cf79f993", "message": "PHOENIX-4509 Fix performance.py usage text (Artem Ervits)\n\nSigned-off-by: Josh Elser <elserj@apache.org>"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/34693843abe4490b54fbd30512bf7d98d0f59c0d", "message": "PHOENIX-4466 Relocate Avatica and hadoop-common in thin-client jar (Toshihiro Suzuki)\n\nWhen using the thin-client in Spark, we encounter problems in that Spark\nis placing its own version of avatica on the classpath as well. We can\nrelocate most of Avatica (all but the protobuf generated messages as\ntheir classnames are required to be 'org.apache.calcite.avatica.proto'\npresently) and hadoop-common to avoid future problems."}, {"url": "https://api.github.com/repos/apache/phoenix/commits/5cb02da74c15b0ae7c0fb4c880d60a2d1b6d18aa", "message": "PHOENIX-4449 Bundle a copy of Argparse-1.4.0 for installations that need it"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/18831881", "body": "> UserGroupInformation.getCurrentUser() will not be thread safe.\n\n```\n  @InterfaceAudience.Public\n  @InterfaceStability.Evolving\n  public synchronized\n  static UserGroupInformation getCurrentUser() throws IOException {\n    AccessControlContext context = AccessController.getContext();\n    Subject subject = Subject.getSubject(context);\n    if (subject == null || subject.getPrincipals(User.class).isEmpty()) {\n      return getLoginUser();\n    } else {\n      return new UserGroupInformation(subject);\n    }\n  }\n```\n\nAm I missing something, @dbahir?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18831881/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "twdsilva": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/f7142879f33cae236e0530a8ed4eeaad1542d66a", "message": "PHOENIX-4473 Exception when Adding new columns to base table and view diverge (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/ae21f87338de5c80efd8e89b256415faad0e00a3", "message": "PHOENIX-4473 Exception when Adding new columns to base table and view diverge (Ankit Singhal)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/9355a4d262d31d8d65e1467bcc351bb99760e11d", "message": "PHOENIX-4468 Looking up a parent index table of a child view from a different client fails (addendum)"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/c935f57f5e7afb42ff8b62a4712ad7d8ffed17cc", "message": "PHOENIX-4468 Looking up a parent index table of a child view from a different client fails"}, {"url": "https://api.github.com/repos/apache/phoenix/commits/9d8be0e9214ba3680a81c399c5da316c1b91c99b", "message": "PHOENIX-4460 High GC / RS shutdown when we use select query with IN clause using 4.10 phoenix client on 4.13 phoenix server"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/20281656", "body": "After the rebase the following assert in AlterTableIT.testAddingColumnsToTablesAndViewsWithEncodedColumns was failing.\r\n\r\n```\r\nassertNull(\"A view should always have the column qualifier counter as null\", view.getEncodedCQCounter().getNextQualifier(DEFAULT_COLUMN_FAMILY));\r\n```\r\n\r\n This was because after the rebase when you add a column or update a table property, the new PTable is sent from the server to the client and this is added to the client cache. Previously we just used to modify the PTable on the client side. \r\n\r\nThe createFromProto code to deserialize the PTable was setting the encodedColumnQualifierCounter to  new EncodedCQCounter() (even if the table is a view) which caused the assert to fail.", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/20281656/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "vincentpoon": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/edc9d12dc056640526e388d7ccb1a6e2c6d3c51c", "message": "Immutable table SINGLE_CELL_ARRAY_WITH_OFFSETS values starting with separator byte return null in query results"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/269", "title": "PHOENIX-2460 Implement scrutiny command to validate whether or not an\u2026", "body": "\u2026 index is in sync with the data table", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "samarthjain": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/72bc8190272480fae17475398ef492a3071a3a44", "message": "PHOENIX-4397 Incorrect query results when with stats are disabled on a salted table"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/20281443", "body": "hey Thomas, can you tell me more about the test failure that this fixes. My local test run was successful. Did it fail after the rebase? If so, which test/s? ", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/20281443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aertoria": {"issues": [], "commits": [{"url": "https://api.github.com/repos/apache/phoenix/commits/1c3387d0eb2fcec2423dda029aa65ca66f547416", "message": "PHOENIX-3837 Feature enabling to set property on an index with Alter statement\n\nSigned-off-by: aertoria <castives@gmail.com>"}], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/287", "title": "PHOENIX-4370 Surface hbase metrics from perconnection to global metrics", "body": "PHOENIX-4370 Surface hbase metrics from perconnection to global metrics\r\n\r\nOpening this p.r. for the connivence of discussion", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pboado": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/286", "title": "Sync 4.x-HBase-1.2 to master", "body": "This PR syncs 4.x-HBase-1.2 to master branch", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChinmaySKulkarni": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/282", "title": "PHOENIX-4361: Remove redundant argument in separateAndValidateProperties in CQSI", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xsq0718": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/280", "title": "indextool inedxTable is not an index table for dataTable", "body": "Phoenix;phoenix-4.8.0-cdh5.8.0\r\nHbase;1.2.0\r\nCreate phoenixTable ;\r\nCREATE Table \"everAp\"(pk VARCHAR PRIMARY KEY,\"ba\".\"ap\" varchar,\"ba\".\"ft\" varchar,\"ba\".\"et\" varchar,\"ba\".\"n\" varchar);\r\n\r\nCreate Index;\r\ncreate local index EVERAP_INDEX_AP on \"everAp\"(\"ba\".\"ap\") async;\r\nUse indexTool\uff1b\r\n./hbase org.apache.phoenix.mapreduce.index.IndexTool -dt \\\"\\\"everAp\\\"\\\" -it EVERAP_INDEX_AP -op hdfs:/hbase/data/default/everApIndc\r\n\r\n\r\n/cloudera/parcels/CDH-5.8.2-1.cdh5.8.2.p0.3/lib/hbase/bin/../lib/native/Linux-amd64-64\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:os.version=2.6.32-504.el6.x86_64\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:user.name=root\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:user.home=/root\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Client environment:user.dir=/opt/cloudera/parcels/CDH-5.8.2-1.cdh5.8.2.p0.3/bin\r\n17/11/02 15:08:09 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=slave1:2181,slave2:2181,master:2181 sessionTimeout=60000 watcher=hconnection-0x4470f8a60x0, quorum=slave1:2181,slave2:2181,master:2181, baseZNode=/hbase\r\n17/11/02 15:08:09 INFO zookeeper.ClientCnxn: Opening socket connection to server master/192.168.0.250:2181. Will not attempt to authenticate using SASL (unknown error)\r\n17/11/02 15:08:09 INFO zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.0.250:53140, server: master/192.168.0.250:2181\r\n17/11/02 15:08:09 INFO zookeeper.ClientCnxn: Session establishment complete on server master/192.168.0.250:2181, sessionid = 0x35f518ca651786a, negotiated timeout = 60000\r\n17/11/02 15:08:10 INFO metrics.Metrics: Initializing metrics system: phoenix\r\n17/11/02 15:08:10 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-phoenix.properties,hadoop-metrics2.properties\r\n17/11/02 15:08:10 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\r\n17/11/02 15:08:10 INFO impl.MetricsSystemImpl: phoenix metrics system started\r\n17/11/02 15:08:11 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available\r\n17/11/02 15:08:12 ERROR index.IndexTool: An exception occurred while performing the indexing job: IllegalArgumentException:  EVERAP_INDEX_AP is not an index table for everAp  at:\r\njava.lang.IllegalArgumentException:  EVERAP_INDEX_AP is not an index table for everAp \r\n\tat org.apache.phoenix.mapreduce.index.IndexTool.run(IndexTool.java:190)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\r\n\tat org.apache.phoenix.mapreduce.index.IndexTool.main(IndexTool.java:394)\r\n\r\nYou have mail in /var/spool/mail/root\r\n\r\n**help!!!**", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shehzaadn": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/275", "title": "PHOENIX-4237: Add function to calculate Java collation keys", "body": "Here we implement a generalized solution for calculating Java collation keys by creating Java collators based on a user locale. These collation keys can then be used in an ORDER BY clause to sort strings in a natural-language-appropriate way. We add a new Phoenix function COLLKEY. In general usage for this function will be:\r\n\r\nselect name from my_table order by COLLKEY(name, 'zh_TW')\r\n\r\nWe use artifacts from the ICU4J project and recently open-sourced grammaticus project (by Maven dependency). We were forced to include some code from ICU4J because some jars produced by that project aren't published in Maven. We also include code from Salesforce that has been licensed for open-source release but not yet published as artifacts in maven.\r\n\r\nThere are three commits that split the changes into three logical pieces:\r\n\r\n1) f8cb121: Add the external source code described above\r\n2) fdbb5e0: Make changes needed to the Phoenix license due to the above (and fix to what seems to be an existing bug) \r\n3) 98cfc10: The actual function implementation of COLLKEY - new code that uses the code introduced above and newly introduced dependencies via maven.\r\n\r\nThanks in advance to the Phoenix community for your feedback on this.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "daiamo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/274", "title": "4.8 h base 1.2 cdh5.8", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TheRealHaui": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/273", "title": "Add equals verification test", "body": "Adds an equals verification Unit Test.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/272", "title": "avoid-equals-null-problem", "body": "Avoids NullPointerException in any case.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rhshriva": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/271", "title": "PHOENIX-4180 - Modify tests to generate unique table names and not us\u2026", "body": "The following tests was changed. \r\n\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/ArrayIT.java\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/ClientTimeArithmeticQueryIT.java\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/ColumnProjectionOptimizationIT.java\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/ConcurrentMutationsIT.java\r\n./phoenix-core/src/it/java/org/apache/phoenix/end2end/CursorWithRowValueConstructorIT.java\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "szq80140": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/270", "title": " `DEFAULT_TIME_ZONE_ID`  should take value from `TimeZone.getDefault().getID()` instead of constant `GMT`", "body": " `DEFAULT_TIME_ZONE_ID`  should take value from `TimeZone.getDefault().getID()` instead of constant `GMT`, so that all `DATE`  and  `TIMESTAMP`  column values are formatted  using timezone configured in JVM.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ankitsinghal": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/268", "title": "PHOENIX-4010 Hash Join cache may not be send to all regionservers whe\u2026", "body": "\u2026n we have stale HBase meta cache", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/22672204", "body": "Do we really need to catch exception here. It can be thrown from the caller right.\r\n@Override\r\n      public Tuple next() throws SQLException {", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22672204/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22672267", "body": "can't we use the same statement . something like this?\r\nFETCH p=NEXT|p=PRIOR (a=NUMBER)? (ROW|ROWS)? FROM c=cursor_name {ret = factory.fetch(c,p!=null, a == null ? 1 :  Integer.parseInt( a.getText() )); }", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22672267/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22672273", "body": "read config in constructor ", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22672273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22672590", "body": "please remove sys.out ", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22672590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22674761", "body": "Add a test case when first call itself is prior", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22674761/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/22674824", "body": "please update all the test cases with smaller result cache size (like 2)", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/22674824/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "kaisenc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/267", "title": "4.10 h base 1.2", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "asdf2014": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/264", "title": "PHOENIX-3961 Should delete `tableOutputPath` in `completebulkload`", "body": "For solving the situation that too many `.tmp` files in /tmp directory when processing a huge `bulkload` job, we should consider deleting `tableOutputPath` in `completebulkload`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aaraujo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/263", "title": "PHOENIX-3817 Verify replication tool", "body": "MapReduce tool that compares data accross tables. SQL conditions may be\r\nused to optionally compare only a subset of the tables.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "akshita-malhotra": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/260", "title": "PHOENIX-3812: Use HBase snapshots in async index building M/R job", "body": "- Index tool creates a snapshot and uses it as a configuration parameter to run index M/R job using HBase snapshot.\r\n- Add option to configure use of snapshots in IndexTool", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/256", "title": "PHOENIX-3477 patch for 4.x-HBase-1.1", "body": "PHOENIX-3477 patch for 4.x-HBase-1.1", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/255", "title": "PHOENIX-3744 for 4.x-HBase-0.98", "body": "PHOENIX-3744 patch for 4.x-HBase-0.98 branch", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dizzy2": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/259", "title": "fixed setting of jetty resource base directory in phoenix-tracing-web\u2026", "body": "fixed obtaining of resource base for embedded jetty in tracing web appp - original implementation of obtaining webapp root from ProtectionDomain is not reliable and doesn't work on some JVMs. New implementation gets the correct webapp location from class loader.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bijugs": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/258", "title": "PHOENIX-3921 Change the condition checking in ScanUtil#isReversed", "body": "The current logic will return ``isReversed`` as ``true`` whether the ``BaseScannerRegionObserver.REVERSE_SCAN`` attribute is set to ``PDataType.TRUE_BYTES`` or ``PDataType.FALSE_BYTES``. The PR is to change it to return ``true`` only if  ``BaseScannerRegionObserver.REVERSE_SCAN`` attribute is set to ``PDataType.TRUE_BYTES``.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/249", "title": "PHOENIX-3878 Add license headers missed in PHOENIX-3572", "body": "Add Apache license headers missed in ``PHOENIX-3572``.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ssanthanam-sfdc": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/254", "title": "PHOENIX-3903 Generate empty javadoc jar to comply with Maven central", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/253", "title": "PHOENIX-3903 Generate empty javadoc jar to comply with Maven central", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/252", "title": "PHOENIX-3903 Generate empty javadoc jar to comply with Maven central", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/251", "title": "Generate empty javadoc jar to comply with Maven Central", "body": "", "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/250", "title": "Generate empty javadoc jar to comply with Maven Central", "body": "Im guessing I need to add same change to the following repositories \r\n\r\n4.10-HBase-1.1, 4.10-HBase-1.2, 4.x-HBase-0.98, 4.x-HBase-1.1, 4.x-HBase-1.2 ?", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JamesRTaylor": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48488374", "body": "Thanks for the pull, @chrajeshbabu. Nice work! Looks like you need to rebase as there are merge conflicts currently.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48488374/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48496677", "body": "Excellent job, @chrajeshbabu! A few minor issues, but overall this is great! Looking forward to seeing how it performs!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48496677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48825943", "body": "Related to the revert of the ScanRanges change, you'll need to make this change to prevent the SkipRangeParallelIteratorRegionSplitter from being used (as you always need to scan all regions for a local index). Cleanest might be to just implement a simple ParallelIteratorRegionSplitter for use when a local index is used that just returns all regions:\n\n```\npublic class ParallelIteratorRegionSplitterFactory {\n\n    public static ParallelIteratorRegionSplitter getSplitter(StatementContext context, TableRef table, HintNode hintNode) throws SQLException {\n        if (!isLocalIndex && context.getScanRanges().useSkipScanFilter()) {\n            return SkipRangeParallelIteratorRegionSplitter.getInstance(context, table, hintNode);\n        }\n        return DefaultParallelIteratorRegionSplitter.getInstance(context, table, hintNode);\n    }\n}\n```\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48825943/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48839791", "body": "Sounds good, @chrajeshbabu. Thanks!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48839791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48853884", "body": "Just a few very minor items. Just add the check to disable creating local indexes on a table with immutable rows and then let's check this in. Great work, @chrajeshbabu!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48853884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48887938", "body": "@ramkrish86 - would you have time to check this in to master? We can add this minor check in a follow up commit. @chrajeshbabu - will an alternate patch be required for the 4.0 branch? 4.0 should match master, but it seems to be ever so slightly different.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48887938/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48941536", "body": "@ramkrish86 - just add a .patch to the url for the pull request and you'll have the patch file to apply against the normal/updatable repo. The github repo is read-only for everyone.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48941536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/49374984", "body": "Nice work, @jfernandosf. Would you mind attaching a .patch file to the JIRA for 3.0, 4.0, and master (if they're different)?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/49374984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/50102170", "body": "The code looks very clean, @jyates. Would love to get a demo and understand the overall flow and any limitations a bit better. What's the typical way a Phoenix user will interact with this to get their metrics? Got any time tomorrow?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/50102170/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/50422772", "body": "Looking very good, @jeffreyz88.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/50422772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53378107", "body": "Nice start, @ramkrish86. I went through your pull and gave you some feedback. Please let me know if you have comments/questions. Thanks - this is going to be a big improvement!\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53378107/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53386020", "body": "One thing I didn't see is how you're handling multiple column families. Is that still left to be done? Think a bit on when you think is the best time to \"merge\" the guideposts. Maybe when you read the stats table in MetaDataEndPointImpl you can execute this logic and then the PTableStats that get passed into PTable are already the \"merged\" ones? Probably don't want to do this in ParallelIterators, as you'd do it again and again for every query execution.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53386020/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53618543", "body": "Looking close, @ramkrish86. Nice work. I think you need to just code up the consumption of the guideposts in ParallelIterators (see my comment there), add the logic for a \"min\" elapsed time for stats collection, and then do some basic local perf testing to see the impact. Then we can follow up with the issue that handles views correctly (just passing through the start/stop row through your coprocessor call) and add some more tests.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53618543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54002950", "body": "@ramkrish86 - just wanted to make sure you're not waiting on me for anything. I think this will be the \"big\" feature for 3.2/4.2 release. I think we should shoot for a monthly release schedule (like HBase does). If you can make the above changes and do 3-5M row local perf test, I think we'll be just about ready to pull this in. Thoughts?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54002950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54014952", "body": "Sounds good, Ram. Thanks for all your effort on this one. Looking forward to see the perf numbers.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54014952/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54078575", "body": "Yes, the DefaultParallelIteratorsRegionSplitterIT should be modified to test the new behavior.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54078575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54418091", "body": "-> Test cases are not running and all the test cases are failing at the deleteTable sequence.\nPost a stack trace - not sure offhand why this would be. \n-> The checking of the timestamp stored in the table and avoiding concurrent update stats does not work. I am checking on it.\n-> The clearing of the metadata cache should happen in a lock? If so then we need to see every where (even while reading from the cache) we should introduce a new lock.\nWe shouldn't need a new lock. What's wrong with the existing lock mechanism?\n-> Estimated byte size() is yet to be added to Ptable\nThis should be easy to add - just have your PTableStats implement a getEstimatedSize() method and estimate based on the size of the guidepost array. \n-> Multi CF is not yet tested.\nNo problem - you can do this after your initial perf testing.\n\nBut this would help in getting the review done on the latest changes.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54418091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/15361268", "body": "This should pass through the same arguments as below:\n    ResultIterator scanner = new TableResultIterator(mutationState, tableRef, scan, scanMetrics);\n\nThe mutationState  was cloned here because the one in context may change. Otherwise, we end up not passing through the transaction. We have a test, but it's ignored and we're waiting for a Tephra fix to enable it. I'll modify this and confirm it fixes the issue (as I'm testing with a Tephra snapshot).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/15361268/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/16795354", "body": "There were no symptoms, but only a warning in the logs. In this case, Phoenix is purely tracking memory usage, but wasn't issuing a close under some circumstances on the client and thus not freeing memory on the server (until a GC occurred). I don't think it's related to what you're seeing. Are you seeing this in our 4.7.0 release and if so can you reliably reproduce it? If you wouldn't mind filing a JIRA with the steps necessary to reproduce the issue, that be much appreciated. We have a lot of regression tests in place, but it's always possible something slipped through the cracks.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16795354/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/16855409", "body": "When a group by is being performed, unless the group by key is made up of the leading PK columns of a table or index, Phoenix holds in memory each distinct group by key and the partial aggregation. This will spill to disk based on phoenix.groupby.maxCacheSize, but performance will drop in this case. There may be other in memory representations we can use to reduce the memory footprint. I've filed PHOENIX-2800 for some potential future work. Let me know if you'd be interested in contributing that.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16855409/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218273", "body": "We need to update the index state in the reducer as we can't depend on the client remaining up for the duration of the index build (which could take 24 hours or more potentially).\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218273/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218366", "body": "Why did the row count change here for this test? The default for auto commit is already false. For transactional workloads, we should see the two rows even though they're not committed. However, we should be using the same connection (conn2 instead of conn when we do the count( \\* ) . Does the test pass if you make that change?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218447", "body": "For the non direct API branch, the reducer does something different, so we can't take the same approach. I'd leave it as-is and just do the simple test change.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218447/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218466", "body": "Ah, I missed that comment. We should change that comment as it doesn't really make sense.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218466/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218742", "body": "@twdsilva - would you mind taking a look at this?\n\nIn theory, the call to setUpRealDriver() in the setup test method should end up calling this to setup the txn client:\n\n```\n    if (clientProps.getBoolean(QueryServices.TRANSACTIONS_ENABLED, QueryServicesOptions.DEFAULT_TRANSACTIONS_ENABLED)) {\n        setupTxManager();\n    }\n```\n\nI wonder if it's somehow mistakenly using the test driver? This is somewhat convoluted, unfortunately.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218769", "body": "Would you mind committing your latest changes, @SsnL?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218769/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713140", "body": "Should this be CREATE LOCAL INDEX?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713140/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713329", "body": "Minor nit: get rid of create.getProps().get(\"\") check in outer if and add if (list != null) check\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713329/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713390", "body": "Isn't this change already in master (hopefully)?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713572", "body": "Add comment here explaining this please: Rather than not use a local index when a column not contained by it is referenced, we join back to the data table in our coprocessor since this is a relatively cheap operation given that we know the join is local.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713646", "body": "Remove this TODO comment, as it's done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713646/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713759", "body": "Comment?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713759/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713917", "body": "Comment: build map from dataColumn to what will be it's position in single KeyValue value bytes returned from the coprocessor that joins from the index row back to the data row.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14713917/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714086", "body": "This shouldn't be necessary. The local index rows will be ordered correctly within each region.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714225", "body": "Comment on if/when table is different than context.getResolver().getTables().get(0).getTable()\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714225/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714621", "body": "This code needs to be in the scanOrdered code block as well. How about factoring this outside of the call to scanOrdered/scanUnordered and passing the necessary info through both calls?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714621/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714692", "body": "Here too - add to scanOrdered code too.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14714692/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14715264", "body": "Also, if it's the same, can you use context.getCurrentTable().getTable() instead as it's a bit more readable.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14715264/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717103", "body": "There's a bit more you need to do to handle ORDER BY correctly. It'd be for the case in which a data column was referenced in the ORDER BY while the index table is being used to satisfy the query. Not here, but in [OrderedResultIterator](https://github.com/apache/phoenix/blob/master/phoenix-core/src/main/java/org/apache/phoenix/iterate/OrderedResultIterator.java). In getResultIterator, in the beginning of the for loop, you'll need to wrap the result in the same way if there are dataColumns (i.e. call IndexUtil.wrapResultUsingOffset)\n\n```\n        for (Tuple result = delegate.next(); result != null; result = delegate.next()) {\n            int pos = 0;\n            ImmutableBytesWritable[] sortKeys = new ImmutableBytesWritable[numSortKeys];\n            for (Expression expression : expressions) {\n                final ImmutableBytesWritable sortKey = new ImmutableBytesWritable();\n                boolean evaluated = expression.evaluate(result, sortKey);\n                // set the sort key that failed to get evaluated with null\n                sortKeys[pos++] = evaluated && sortKey.getLength() > 0 ? sortKey : null;\n            }\n            queueEntries.add(new ResultEntry(sortKeys, result));\n        }\n```\n\nWithout this, the table data column expressions that aren't in the local index will fail to evaluate. You should add a test for this too.\n\nI think the cleanest way to handle this is by wrapping the ResultIterator passed into the OrderedResultIterator. Just create a new ResultIterator that delegates to the original one and does the wrapping necessary. Then you won't need to change OrderedResultIterator at all.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717103/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717444", "body": "Actually, nix this, as the innerScanner you pass to OrderedResultIterator already does all of this. You likely already have a test for the ORDER BY case I mentioned, but if not, please add one.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717444/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717528", "body": "comment please\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14717528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14718183", "body": "Remove this comment, as this has already been done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14718183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14718590", "body": "This TODO should be implemented. Just implement a new method on ColumnRef for cloning:\n\n```\npublic ColumnRef cloneAtTimestamp(long timestamp) {\n    return new ColumnRef(this, timestamp);\n}\n```\n\nOveride this on LocalIndexDataColumnRef by adding this:\n\n```\n@Overide\npublic LocalIndexDataColumnRef cloneAtTimestamp(long timestamp) {\n    return new LocalIndexDataColumnRef(this, timestamp);\n}\n```\n\nMake the ColumnRef(ColumnRef columnRef, long timeStamp) constructor protected and replace existing calls to it with the calls to the new clone method. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14718590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14762643", "body": "One thing that's necessary, though, to maintain rows in row key order is to modify ScanPlan.java:118 to do a merge sort instead of a concat:\n\n```\n        if ((isSalted || isLocalIndex) &&\n                (context.getConnection().getQueryServices().getProps().getBoolean(\n                        QueryServices.ROW_KEY_ORDER_SALTED_TABLE_ATTRIB,\n                        QueryServicesOptions.DEFAULT_ROW_KEY_ORDER_SALTED_TABLE) ||\n                 orderBy == OrderBy.FWD_ROW_KEY_ORDER_BY ||\n                 orderBy == OrderBy.REV_ROW_KEY_ORDER_BY)) { // ORDER BY was optimized out b/c query is in row key order\n            scanner = new MergeSortRowKeyResultIterator(iterators, SaltingUtil.NUM_SALTING_BYTES, orderBy == OrderBy.REV_ROW_KEY_ORDER_BY);\n        } else {\n            scanner = new ConcatResultIterator(iterators);\n        }\n```\n\nLocal indexes are similar to salted tables in that the parallel scans will all be within a region, ordered correctly. As long as we do a merge sort across the results of these scans, the rows will be ordered correctly.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14762643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "chrajeshbabu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48798867", "body": "Thanks for review @JamesRTaylor. I have resolved the conflicts  and handled all the comments locally. I will submit it once I verify OrderedResultIterator scenario.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48798867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48839233", "body": "bq. Cleanest might be to just implement a simple ParallelIteratorRegionSplitter for use when a local index is used that just returns all regions:\nI will add new ParallelIteratorRegionSplitter for local index and remove the unnecessary changes in SkipRangeParallelIteratorRegionSplitter/DefaultParallelIteratorRegionSplitter. \n\nThen I will submit another pull request. \n\nThanks @JamesRTaylor \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48839233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48945719", "body": "bq. Just add the check to disable creating local indexes on a table with immutable rows and then let's check this in. \nChanged pull request to disallow local index on immutable rows and added some test cases.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48945719/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48949296", "body": "Resolved the conflicts after PHOENIX-1002 also. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48949296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834683", "body": "Somehow missed this. Corrected.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834867", "body": "corrected.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834896", "body": "Yes James. This change already there in master branch\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834896/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834905", "body": "Added the comment.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834905/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834954", "body": "removed.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14834954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835021", "body": "comment added.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835021/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835033", "body": "done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835033/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835161", "body": "Thanks for pointing this James. Yes merge sort is fine. Done the changes locally.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14835161/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850777", "body": "Moved the changes outside of scanOrdered/scanUnordered and passing through necessary info through calls.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850777/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850781", "body": "done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850781/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850789", "body": "This I will verify James.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850789/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850792", "body": "done.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/pulls/comments/14850792/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ramkrish86": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48926040", "body": "Just committing it JAmes.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48926040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48927901", "body": "I think I don't have write access to do this merge. \n     Only those with write access to this repository can merge pull requests. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48927901/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48991061", "body": "bq. just add a .patch to the url for the pull request and you'll have the patch file to apply against the normal/updatable repo\nThat's nice.  I tried it, it works. :)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/48991061/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53409242", "body": "```\nOne thing I didn't see is how you're handling multiple column families. Is that still left to be done?\n```\n\nNo not yet in this patch.  I initially created a patch where there was some sort of grouping that happened per family. Later dropped it.\n\n```\n Think a bit on when you think is the best time to \"merge\" the guideposts. Maybe when you read the stats table in MetaDataEndPointImpl you can execute this logic and then the PTableStats that get passed into PTable are already the \"merged\" ones?\n```\n\nMerge in the sense- after the guideposts are collected per family? You mean this \"merge\"\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/53409242/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54014739", "body": "@JamesRTaylor - Thanks for the review.  I will do my level best to complete all the comments.  Some of them have now become design changes because now we will be collecting based on CF rather than region name. So the guide posts will be a map with key as CF and byte[](representing the guideposts). It was a long weekend here and so could not take this up.  Will post an updated patch ASAP may be in a day or two.  After that will do performance testing. \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54014739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54037058", "body": "DefaultParallelIteratorsRegionSplitterIT - how about these test cases.  We need the new behaviour or we need to update the test cases to get the required result?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54037058/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54406335", "body": "```\njava.lang.IllegalArgumentException: KeyValue size too large\n    at org.apache.hadoop.hbase.client.HTable.validatePut(HTable.java:1312)\n    at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:941)\n    at org.apache.hadoop.hbase.client.HTable.put(HTable.java:908)\n    at org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment$HTableWrapper.put(CoprocessorHost.java:444)\n    at org.apache.phoenix.schema.stat.StatisticsTable.updateStats(StatisticsTable.java:126)\n    at org.apache.phoenix.schema.stat.StatisticsScanner.close(StatisticsScanner.java:90)\n    at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:87)\n    at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)\n    at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)\n    at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1480)\n    at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:744)\n```\n\nWhat could be ideal value for those guidePost collection - Like after how many bytes could we collect the guideposts?  And for the performance.py what could be the best value based on its distribution?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54406335/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54417314", "body": "Why is the pull request referring to the old branch ?\n\n```\ncommit b4c0cb1df9fc43d76c43de052f5f8007b51b4909\n```\n\nAuthor: Ramkrishna ramkrishna.s.vasudevan@intel.com\nDate:   Thu Sep 4 12:14:13 2014 +0530\n\n```\nPhoenix-180\n```\n\ncommit 3abb90bb0fa0721a333f919f4d0c734cf51028fc\nAuthor: Rajeshbabu Chintaguntla rajeshbabu.chintaguntla@huawei.com\nDate:   Wed Sep 3 18:16:49 2014 +0800\n\n```\nPHOENIX-1139 Failed to disable local index when index update fails(addendum)\n```\n\ncommit 845888bcb58a6fe5975de4cfabbe9266407de9dc\nAuthor: Rajeshbabu Chintaguntla rajeshbabu@apache.org\n\nThis is what gets displayed in my local log history.  And my branch is Phoenix-180_1\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54417314/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54418748", "body": "Pls use this https://github.com/apache/phoenix/pull/11\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/issues/comments/54418748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "lalinsky": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/12842382", "body": "This removed phoenix-server\\* jar files from the package. Was that intentional?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/12842382/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "djh4230": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/16779652", "body": "hi James,\nWe are using Phoenix for a while. We found the aggression operation ,like filter,group by,order by etc,cost too much cache of server side recently. We suspected  if there is a memory leak. But today i  found you have fixed a memory leak bug.  Could  you please describe the memory leak appearance in your case?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16779652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/16795183", "body": "hi, james\nI am using Phoenix for a while. Recently i found  it cost too much memcache in server side when i do aggression operation like group by etc. I suspect that there is a memory leak in server side. But i am not very sure. And i find you have fixed a bug about server side memory cache. Could you please describe the appreance in your case? \n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16795183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/16853391", "body": "yes, I saw this in 4.7.0 release.\n\n In my case, i want to get data from a table with 60 million data, about 70G. There is no index.\nselect max(PK),\"req_id_card\",\"req_name_card\",max(\"req_time\") as \"req_time\" from \"channellog\"(\"req_id_card\" varchar,\"req_name_card\" varchar,\"res_result\" varchar)  where \"resourceId\"='185' and \"channelId\"='15' and \"res_result\"='1' and to_date(\"req_time\",'yyyy-MM-dd HH:mm:ss')>=to_date('2016-03-24 14:40:10','yyyy-MM-dd HH:mm:ss')  group by \"req_id_card\" ,\"req_name_card\"\n\nIt will cost 5-6G cache in server side. I don't known if it's normal. And also this happed in join operation.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/16853391/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "SsnL": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/18218383", "body": "Yes it passes. \n\nPreviously, the two new rows are inserted through `conn`, not the new `conn2`, which contradicts the original comments. That is the reason I made the change.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218383/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218395", "body": "This makes sense. Should we do that without direct API (i.e. when using `PhoenixIndexImportMapper`) as well?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218395/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218417", "body": "I think it makes more sense to add a new query test for `conn2` with expected row count 4.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218417/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218478", "body": "Agreed\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18218510", "body": "I see. \n\nThe tests failed when direct API and transactional are both on. The error was due to `TransactionContext` having a `null` `txClient`. I wasn't sure how to trace and fix that. However, these tests passes once I changed to use \n`IndexToolUtil.updateIndexState(connection, qDataTable, indexTable, PIndexState.ACTIVE);` from client instead of what was in the reducer.\n\nDo you have any insights on what might be wrong here, James?\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18218510/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18219844", "body": "Will do when I finish them.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18219844/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dbahir": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/apache/phoenix/comments/18831582", "body": "UserGroupInformation.getCurrentUser() will not be thread safe.\n\nThe file below is a patch that addresses that and other issues\n\n[phoenix.txt](https://github.com/apache/phoenix/files/445294/phoenix.txt)\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18831582/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/apache/phoenix/comments/18834022", "body": "My bad regarding the synchronization, you are correct.\n\nDid you get to look at the comment regarding the user login ?\n\nCan you allow another to login with a different principal? Would that cause a security issue?\nIf we create one driver(One) with user A and then create another driver(Two) with user B the info in the UGI now is that of user B. So there can be a situation where driver One will be using credentials of user B.\n", "reactions": {"url": "https://api.github.com/repos/apache/phoenix/comments/18834022/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}}}}