{"_default": {"1": {"FredEnglish": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3079", "title": "Connection Lost in a non-clean fashion for some URLs on a particular domain, but not others", "body": "I have created a basic spider to scrape a small group of job listings from totaljobs.com. I have set up the spider with a single start URL, to bring up the list of jobs I am interested in. From there, I launch a separate request for each page of the results. Within each of these requests, I launch a separate request calling back to a different parse method, to handle the individual job URLs.\r\n\r\nWhat I'm finding is that the start URL and all of the results page requests are handled fine - scrapy connects to the site and returns the page content. However, when it attempts to follow the URLs for each individual job page, scrapy isn't able to form a connection. Within my log file, it states:\r\n\r\n`[<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]`\r\n\r\nI'm afraid that I don't have a huge amount of programming experience or knowledge of internet protocols etc. so please forgive me for not being able to provide more information on what might be going on here. I have tried changing the TLS connection type; updating to the latest version of scrapy, twisted and OpenSSL; rolling back to previous versions of scrapy, twisted and OpenSSL; rolling back the cryptography version, creating a custom Context Factory and trying various browser agents and proxies. I get the same outcome every time: whenever the URL relates to a specific job page, scrapy cannot connect and I get the above log file output.\r\n\r\nIt may be likely that I am overlooking something very obvious to seasoned scrapers, that is preventing me from connecting with scrapy. I have tried following some of the the advice in these threads:\r\n\r\n[https://github.com/scrapy/scrapy/issues/1429](https://github.com/scrapy/scrapy/issues/1429)\r\n[https://github.com/requests/requests/issues/4458](https://github.com/requests/requests/issues/4458)\r\n[https://github.com/scrapy/scrapy/issues/2717](https://github.com/scrapy/scrapy/issues/2717)\r\n\r\nHowever, some of it is a bit over my head e.g. how to update cipher lists etc. I presume that it is some kind of certification issue, but then again scrapy is able to connect to other URLs on that domain, so I don't know.\r\n\r\nThe code that I've been using to test this is very basic, but here it is anyway:\r\n\r\n```\r\nimport scrapy\r\n\r\nclass Test(scrapy.Spider):\r\n\t\r\n\r\n\tstart_urls = [\r\n\t\t\t\t\t'https://www.totaljobs.com/job/welder/jark-wakefield-job79229824'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/elliott-wragg-ltd-job78969310'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/exo-technical-job79019672'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/exo-technical-job79074694'\r\n\t\t\t\t\t\t]\r\n\t\r\n\tname = \"test\"\r\n\r\n\tdef parse(self, response):\r\n\t\tprint 'aaaa'\r\n                yield {'a': 1}\r\n```\r\n\r\nThe URLs in the above code **are not** being connected to successfully.\r\n\r\nThe URLs in the below code **are** being connected to successfully.\r\n\r\n```\r\nimport scrapy\r\n\r\nclass Test(scrapy.Spider):\r\n\t\r\n\r\n\tstart_urls = [\r\n\t\t\t\t\t'https://www.totaljobs.com/jobs/permanent/welder/in-uk'\r\n\t\t\t\t\t,'https://www.totaljobs.com/jobs/permanent/mig-welder/in-uk'\r\n\t\t\t\t\t,'https://www.totaljobs.com/jobs/permanent/tig-welder/in-uk'\r\n\t\t\t\t\t\t]\r\n\t\r\n\tname = \"test\"\r\n\r\n\tdef parse(self, response):\r\n\t\tprint 'aaaa'\r\n                yield {'a': 1}\r\n```\r\n\r\n\r\nIt'd be great if someone could replicate this behavior (or not as the case may be) and let me know. Please let me know if I should submit additional details. I apologise, if I have overlooked something really obvious. I am using:\r\n\r\nWindows 7 64 bit\r\nPython 2.7\r\nscrapy version 1.5.0\r\ntwisted version 17.9.0\r\nopenSSL version 17.5.0\r\nlxml version 4.1.1", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shanmuga-cv": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077", "title": "scrapy selector fails when large lines are present response", "body": "Originally encoutered when scraping [Amazon restaurant](https://www.amazon.com/restaurants/zzzuszimbos0015gammaloc1name-new-york/d/B01HH7CS44?ref_=amzrst_pnr_cp_b_B01HH7CS44_438).  \r\nThis page contains multiple script tag with lines greater then 64,000 character in one line. \r\nThe selector (xpath and css) does not search beyond these lines. \r\n\r\nDue to this the following xpath `'//h1[contains(@class, \"hw-dp-restaurant-name\")]/text()'` to extract name of the restaurant returns empty even though there is a matching tag is present.\r\n\r\n\r\nPFA the response text at [original_response.html.txt.gz](https://github.com/scrapy/scrapy/files/1631425/original_response.html.txt.gz)\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "crisfan": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075", "title": "Telnet Console ", "body": "hi, @kmike,I use telnetlib to pause scrapy engine, there is some error! although it pause the scrapy engine.\r\nthis is my poor code:\r\n``` javascript\r\nimport telnetlib\r\nhost = '127.0.0.1'\r\ntn = telnetlib.Telnet(host, 6023)\r\ntn.write(\"engine.pause()\\n\") \r\n``` \r\nerror:\r\n``` javascript\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n``` \r\n\r\ncould you fix my problem, thansk ~~~\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Caleb-Wade": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074", "title": " Cxfreeze+python3.4+scrapy1.4 failed to bundle to executables(AttributeError: module object has no attribute '_fix_up_module')", "body": "After I failed to bundle .py document into .exe document by pyinstaller, I tried cxfreeze. Similar error happened. Someting went wrong when importing scrapy module, and AttributeError: module object has no attribute '_fix_up_module' appeared in the command window. Would someone tell me what happened. Can I make it? Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073", "title": "Pyinstaller+Python3.4+Scrapy1.4 Failed to bundled exe (ImportError:No module named \"XXXX\")", "body": "I have tried to bundle my spider .py into .exe, but weeks pasted, I didn't make it. It pointed that some modules were missing. Even I used --hidden-import to put the missing modules into my bundled exe, it still didn't work. It's amazing that I can see the modules in the HTML document under build file. Pls help me ,Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lifei1245": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069", "title": "about the signal retry_complete", "body": "I didn't find the singnal in the singnal list,how can I use it", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "3xp10it": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068", "title": "scrapy always Starting new HTTP connection after crawl finished ", "body": "I reopen [#3066][1] here,there are more details [here][2].\r\n\r\n[1]: https://github.com/scrapy/scrapy/issues/3066\r\n[2]: https://stackoverflow.com/questions/48182204/scrapy-always-starting-new-http-connection-after-crawl", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "theduman": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067", "title": "scrapy.Request doesnt wait for loop", "body": "I fetch url and other parameters from database and use them in for loop and pass to scrapy.request() function but when i run the code i succesfully pass first element only. Scrapy can't get other elements of list. Here is my code\r\n\r\n```py\r\nclass QuotesSpider(scrapy.Spider):\r\n    name = \"quotes\"\r\n    custom_settings = {\r\n        'CONCURRENT_REQUESTS': 1,\r\n    }\r\n    def start_requests(self):\r\n        sourceArr = []\r\n        connection = db.connect()\r\n        try:\r\n            with connection.cursor() as cursor:\r\n                # Read a single record\r\n                sql = \"SELECT `code`, `url`, `target`,`next` FROM `source`\"\r\n                cursor.execute(sql)\r\n                result = cursor.fetchall()\r\n        finally:\r\n            connection.close()\r\n        print(result)\r\n        for i in result:\r\n            source = Source(i['code'], i['url'], i['target'], i['next'])\r\n            sourceArr.append(source)\r\n        print(sourceArr)\r\n        #for url in urls:\r\n           #yield scrapy.Request(url=url, callback=self.parse)\r\n        for s in sourceArr:\r\n            print(s.target)\r\n            yield scrapy.Request(url=s.url, meta={'target': s.target, 'next': s.next})\r\n            print(\"slept\")\r\n\r\n\r\n    def parse(self, response):\r\n        titlearr = []\r\n        count = 0\r\n        print(response.meta)\r\n        for title in response.css(response.meta['target']):\r\n            count += 1\r\n            titlearr.append(title.css('p a::text').extract_first())\r\n            #yield {'title': title.css('p a::text').extract_first()}\r\n        print(\"total count \" + str(count))\r\n        print(titlearr)\r\n        for next_page in response.css(response.meta['next']):\r\n            yield response.follow(next_page, self.parse)\r\n```\r\n\r\nWhen i print response.meta i got target and next values for only first item. List contains more than 1 item. How can i make scrapy.Request() function wait for the next element to run?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "benjolitz": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065", "title": "`Connection to the other side was lost` for older French site", "body": "Hi,\r\n\r\nI'm attempting to run scrapy on `https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`.\r\n\r\nDespite upgrading all my brew packages, scrapy installation via `pip install -U scrapy`, the website disconnects uncleanly. Specifying `-s DOWNLOADER_CLIENT_TLS_METHOD=TLSv1.0` makes no difference to the following run. Neither does a user-agent clear things up with `-s USER_AGENT=\"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\"`\r\n\r\nI am uncertain as how to debug this further.\r\n\r\n`scrapy shell https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`\r\n\r\n```\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Darwin-16.7.0-x86_64-i386-64bit\r\n2018-01-09 18:03:33 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOADER_CLIENT_TLS_METHOD': 'TLSv1.0', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2018-01-09 18:03:33 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2018-01-09 18:03:33 [scrapy.core.engine] INFO: Spider opened\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\nTraceback (most recent call last):\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/commands/shell.py\", line 73, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 48, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 115, in fetch\r\n    reactor, self._schedule, request, spider)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/python/failure.py\", line 385, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n```\r\n\r\n\r\n`scrapy version -v`:\r\n```\r\nScrapy       : 1.5.0\r\nlxml         : 4.1.1.0\r\nlibxml2      : 2.9.7\r\ncssselect    : 1.0.3\r\nparsel       : 1.3.1\r\nw3lib        : 1.18.0\r\nTwisted      : 17.9.0\r\nPython       : 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)]\r\npyOpenSSL    : 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017)\r\ncryptography : 2.1.4\r\nPlatform     : Darwin-16.7.0-x86_64-i386-64bit\r\n```\r\n\r\n`curl -sLv https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences 2>&1 | head -20`:\r\n```\r\n*   Trying 160.92.134.3...\r\n* TCP_NODELAY set\r\n* Connected to agences.creditfoncier.fr (160.92.134.3) port 443 (#0)\r\n* TLS 1.0 connection using TLS_RSA_WITH_3DES_EDE_CBC_SHA\r\n* Server certificate: agences.creditfoncier.fr\r\n* Server certificate: RapidSSL RSA CA 2018\r\n* Server certificate: DigiCert Global Root CA\r\n> GET /credit-immobilier/toutes-nos-agences HTTP/1.1\r\n> Host: agences.creditfoncier.fr\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 200 OK\r\n< Date: Wed, 10 Jan 2018 02:06:54 GMT\r\n< Server: Microsoft-IIS/6.0\r\n< X-Powered-By: ASP.NET\r\n< Content-Length: 35884\r\n< Content-Type: text/html\r\n< Set-Cookie: ASPSESSIONIDQQBACDTS=JHOJNNCCJNIFABOPLNCEEAFH; path=/\r\n< Cache-control: private\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "reaCodes": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064", "title": "I ran into a problem when I used ipython in a scrapy shell", "body": "I used this command, `scrapy shell 'www.baidu.com'`, and then used `response.body`. \r\n\r\nWhen I input `resp` and click `Tab`\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660511-e159fe1a-f47d-11e7-8499-b6589ef7de3f.png)\r\n\r\nAs you can see, there was a display error\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660548-16f0b794-f47e-11e7-996b-9862f788a771.png)\r\n\r\nUse the command completion function is encountered a display error", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmagonski": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060", "title": "HTTPERROR_ALLOWED_CODES can't parse response with 301", "body": "In scrapy.downloadermiddlewares.redirect.RedirectMiddleware#process_response \r\ndosn't get anything from settings like HTTPERROR_ALLOWED_CODES only in HttpErrorMiddleware we have \r\nhandle_httpstatus_list.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NewUserHa": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057", "title": "configure_logging unable to handle GBK", "body": "I added \r\n`import logging\r\nfrom scrapy.utils.log import configure_logging\r\n\r\nconfigure_logging(install_root_handler=False)\r\nlogging.basicConfig(\r\n    filename='log.txt',\r\n    format='%(levelname)s: %(message)s',\r\n    level=logging.INFO\r\n)`\r\nfrom scrapy documents blow my class define.\r\n\r\nthen:\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\logging\\__init__.py\", line 982, in emit\r\n    stream.write(msg)\r\nUnicodeEncodeError: 'gbk' codec can't encode character '\\ufe0f' in position 190: illegal multibyte sequence\r\nCall stack:\r\n....\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\core\\scraper.py\", line 237, in _itemproc_finished\r\n    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\r\nMessage: 'Scraped from %(src)s\\r\\n%(item)s'\r\nArguments: {'src': <200 http://...>, 'item': {'date': '12-02', 'floor': '...\\n    \u7535\\ufe0f', 'pics': ['...', ...]}}\r\n\r\nI also have 'LOG_LEVEL': 'INFO' in the spider.\r\n\r\nI googled and have no idea how to fix this.\r\nany help, please?\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056", "title": "[suggest] add a option to pipeline for some sites requiring reference in heads", "body": "it's usual case and it's ugly to override get_media_requests method of pipelines.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055", "title": "[bug] pillow will always recode images in imagepipieline", "body": "https://github.com/scrapy/scrapy/blob/aa83e159c97b441167d0510064204681bbc93f21/scrapy/pipelines/images.py#L151\r\n\r\nthis line will always recode images silently and damage the image quality.\r\n\r\nplease add an option to avoid this.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elacuesta": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054", "title": "Request serialization should fail for non-picklable objects", "body": "The Pickle-based disk queues silently serialize requests that shouldn't be serialized in Python<=3.5. I found this problem when dumping a request with an `ItemLoader` object in its `meta` dict. Python 3.6 fails in [this line](https://github.com/scrapy/scrapy/blob/1.4/scrapy/squeues.py#L27) with `TypeError: can't pickle HtmlElement objects`, because the loader contains a `Selector`, which in turns contains an `HtmlElement` object.\r\n\r\nI tested this using the https://github.com/scrapinghub/scrapinghub-stack-scrapy repository, and found that `pickle.loads(pickle.dumps(selector))` doesn't fail, but generates a broken object.\r\n\r\n#### Python 2.7, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3)\r\n```\r\nroot@04bfc6cf84cd:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 2.7.14 (default, Dec 12 2017, 16:55:09) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@04bfc6cf84cd:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:49:27 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath=u'descendant-or-self::a' data=u'<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 140144569743064\r\n```\r\n\r\n\r\n#### Python 3.5, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@1945e2154919:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.5.4 (default, Dec 12 2017, 16:43:39) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@1945e2154919:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:52:37 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath='descendant-or-self::a' data='<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 139862544625976\r\n```\r\n\r\n\r\n#### Python 3.6, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@43e690443ca7:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.6.4 (default, Dec 21 2017, 01:35:12) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@43e690443ca7:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:54:49 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\nTypeError: can't pickle HtmlElement objects\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3082", "title": "[WIP] Do not serialize unpickable objects (py3)", "body": "This addresses #3054 partially, as it catches the exception raised by `pickle`.\r\nHowever, since this exception is only raised in python >= 3.6, for earlier versions it's still not able to realize it shouldn't serialize some requests.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2956", "title": "Add from_crawler support to dupefilters", "body": "Fixes #2940\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stummjr": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046", "title": "Item loader missing values from base item", "body": "ItemLoaders behave oddly when they get a pre-populated item as an argument and `get_output_value()` gets called for one of the pre-populated fields before calling `load_item()`.\r\n\r\nCheck this out:\r\n\r\n```python\r\n>>> from scrapy.loader import ItemLoader\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': 'http://example.com'}\r\n\r\n# so far, so good... what about now?\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.get_output_value('url')\r\n[]\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': []}\r\n```\r\n\r\nThere are **2** unexpected behaviors in this snippet (at least from my point of view):\r\n\r\n**1)** `loader.get_output_value()` doesn't return the pre-populated values, even though they end up in the final item.\r\n\r\nIt seems to be like this on purpose, though. The `get_output_value()` method only queries the `_local_values` defaultdict ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\n**2)** once we call `loader.get_output_value('url')`, that field is not included in the `load_item()` result anymore.\r\n\r\nThis one doesn't look right, IMHO.\r\n\r\nIt happens because when we call `loader.get_output_value('url')` for the first time, such value is not available on `_local_values`, and so a new entry in the `_local_values` defaultdict will be created with an empty list on it ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L121)). Then, when `loader.load_item()` gets called, [these lines](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L116-L117) overwrite the current value from the internal item because the value returned by `get_output_value()` is `[]` and not `None`.\r\n\r\nAny thoughts on this?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3047", "title": "Avoid missing base item fields in item loaders", "body": "This is an attempt to fix the behavior described in #3046.\r\n\r\nInstead of just checking if the value inside the loader is not None in order to decide if a field from the initial item should be overwritten or not, `load_item()` should also make sure that the value returned by `get_output_value()` is not an empty list.\r\n\r\nThat is because `self._local_values` , which stores the new values included via `add_*` or `replace_*` methods, is a[`defaultdict(list)`](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L37). Then, when we call `get_output_value()` for a field only available in the initial item, an empty list will be set for that field in `self._local_values` (because of [this](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\nThis way, we make sure we don't miss fields from the initial item, in case `get_output_value()` gets called for one of the pre-populated fields before `load_item()`, as described on #3046.", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "exotfboy": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036", "title": "File downloaded by pipeline are blank", "body": "I have deployed a spider in my remote server, and I am using `FilePipeline` to download images for `item`, however I found that the downloaded image have size of  zero, which is not expected.\r\n\r\nThen I test the project in my local machine, it worked. So I think maybe my remote server has been banned, however when I tried `wget ..` I can download the image normally, and I also tried `scrapy sheel image_src` it still work.\r\n\r\nNow I have no idea what's going on , I just want to intercept the response of the request re-sent by the pipeline, is it possible?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ReLLL": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034", "title": "Line-ends (unnecessary blank lines) problem in CSV export on Windows ", "body": "CSV export on Windows create unnecessary blank lines after each line.\r\n\r\nYou can fix the problem just by adding \r\nnewline='' \r\nas parameter to io.TextIOWrapper in the __init__ method of the CsvItemExporter class in scrapy.exporters\r\n\r\nDetails are over here:\r\nhttps://stackoverflow.com/questions/39477662/scrapy-csv-file-has-uniform-empty-rows/43394566#43394566", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3039", "title": "Fix for #3034, CSV export unnecessary blank lines problem on Windows", "body": "Fixed the issue I've mentioned there, this is the pull request to merge, (added one line), hope all is fine. \r\nhttps://github.com/scrapy/scrapy/issues/3034\r\n\r\nCloses #3034 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cp2587": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029", "title": "Invalid DNS pattern", "body": "Hello,\r\n\r\nWe are using https proxies to crawl some website and sometimes i get the following stack trace:\r\n\r\n```\r\nError during info_callback\r\nTraceback (most recent call last):\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 315, in dataReceived\r\n    self._checkHandshakeStatus()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 235, in _checkHandshakeStatus\r\n    self._tlsConnection.do_handshake()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1442, in do_handshake\r\n    result = _lib.SSL_do_handshake(self._ssl)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 933, in wrapper\r\n    callback(Connection._reverse_mapping[ssl], where, return_code)\r\n--- <exception caught here> ---\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1102, in infoCallback\r\n    return wrapped(connection, where, ret)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/scrapy/core/downloader/tls.py\", line 67, in _identityVerifyingInfoCallback\r\n    verifyHostname(connection, self._hostnameASCII)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 44, in verify_hostname\r\n    cert_patterns=extract_ids(connection.get_peer_certificate()),\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 73, in extract_ids\r\n    ids.append(DNSPattern(n.getComponent().asOctets()))\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/_common.py\", line 156, in __init__\r\n    \"Invalid DNS pattern {0!r}.\".format(pattern)\r\nservice_identity.exceptions.CertificateError: Invalid DNS pattern '194.167.13.105'.\r\n```\r\n\r\nI think this issue is somewhat similar to https://github.com/scrapy/scrapy/issues/2092 and this 'invalid DNS pattern' error should be caught similarly as the 'Invalid DNS-ID'. What do you think ?\r\n\r\nIn the meantime, how can i catch it myself and silent it ?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmike": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/7c9e32213db2ce757c784e7c29e30caa57dc3d48", "message": "Merge pull request #3059 from jesuslosada/fix-typo\n\nFix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/786144e0c7b25a02151b6d3135451da90e912719", "message": "Merge pull request #3058 from jesuslosada/fix-link\n\nFix link in news.rst"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/aa83e159c97b441167d0510064204681bbc93f21", "message": "Bump version: 1.4.0 \u2192 1.5.0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d07fe11981a07e493faf7454db79b98c02a53118", "message": "set release date"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c107059ef82a4b7b491b23b740b71353f03ab891", "message": "DOC fix rst syntax"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d4e5671d07a8dcf18b665ed3ce4136dccae222fb", "message": "make release docs more readable, add highlights"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/45b0e1a0e4c51a773b39be14334a999cc5f0fe56", "message": "DOC draft 1.5 release notes"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/930f6ed8002e27c9d51f5d5abbb28c36460eb1bb", "message": "Merge pull request #3050 from lopuhin/pypy3\n\nAdd PyPy3 support"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/632f1cc07305d6967c9e8ce3bf150337e2bf3ecd", "message": "Merge pull request #3049 from scrapy/trove-classifiers\n\n[MRG+1] setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9f9edeadfc9d8d3422d4ca15b2b13d5b502ca70e", "message": "Merge pull request #3048 from lopuhin/pypy-install-docs\n\n[MRG+1] Mention PyPy support, add PyPy to install docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1058169f0e3a8646dbd20f9b4c0b599ed9f6d08e", "message": "setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/bdc12f39949731e69aaa32c36490e1b6e56ca98b", "message": "Merge pull request #3045 from hugovk/rm-3.3\n\n[MRG+1] Drop support for EOL Python 3.3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9aa9dd8d45a2ce0c8e6ae0732e610f020735df7e", "message": "DOC mention an easier way to track pull requests locally.\nThanks @eliasdorneles!"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f716843a66829350063e55f8df768eb538c6b05c", "message": "DOC update \"Contributing\" docs:\n\n* suggest Stack Overflow for Scrapy usage questions;\n* encourage users to submit test-only pull requests with reproducable examples;\n* encourage users to pick up stalled pull requests;\n* we don't use AUTHORS file as a main acknowledgement source;\n* suggest using Sphinx autodocs extension"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/4948548", "body": "whoops, I clicked github's Edit button and thought it will become a PR\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4948548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104443", "body": "For me it is 2014-01-18 :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": []}, "dangra": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/b170e9fb96dc763b9b78916e1557021e5e004d59", "message": "Merge pull request #2609 from otobrglez/extending-s3-files-store\n\nS3FilesStore support for other S3 providers (botocore options)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/2dee191374e7fb7f2ed35ab9eea07ba7d1ee8b89", "message": "Merge branch 'master' into extending-s3-files-store"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9b4d6a40a6d56acbd9e068e15e6717dc06aee79b", "message": "Merge pull request #3053 from scrapy/release-notes-1.5\n\nRelease notes for the upcoming 1.5.0 version"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/57d04aa9601bc237da4b08777327df241483b389", "message": "Merge pull request #2767 from redapple/http-proxy-endpoint-key\n\n[MRG+1] Use HTTP pool and proper endpoint key for ProxyAgent"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/86c322c3a819405020cd884f128246ae55b6eaeb", "message": "Merge pull request #3038 from scrapy/update-contributing-docs\n\nDOC update \"Contributing\" docs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355950", "body": "@pablohoffman: \"a global limit **and** a per-domain limit\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355969", "body": "\"to run **Scrapy** from a script\" right?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355981", "body": "\"if possible, use...\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355981/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379551", "body": "The issue this commit tries to address is real, but the fix introduces a new bug when items contains unicode values not encodeable with default encoding.\n\nIt's a twisted shame that `log.err` doesn't call `_safeFormat` on `_why`  here http://twistedmatrix.com/trac/browser/tags/releases/twisted-12.3.0/twisted/python/log.py#L328\n\nThe good news is that we can call _safeFormat ourself on our `scrapy.log.err` wrapper\n\nwhat do you think?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379551/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379680", "body": "to make it more clear, this is what I am proposing https://gist.github.com/4445963\n\nit has an obvious dislike because of private function import and it doesn't improve on lazy evaluation side\n\nafter all, I am not sure. :sake: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379712", "body": "Now I figured out that previous Gist was an implementation of `format` instead of `_why`\n\nhere is the `_why` version https://gist.github.com/4446084 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379712/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2966628", "body": "I guess you refer to Scrapy 0.16. is it the same case on Scrapy 0.18? \n\nThe motivation was to avoid hiding the real error that makes debugging images error a bit harder.\nCan you provide an test case that reproduces \"image file is truncated\" error and that is valid case to silence instead of propagating and logging the full traceback?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2966628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976147", "body": "hey @a7ch3r, you welcome.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048440", "body": "unused import?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3049106", "body": ":beers:\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3049106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647682", "body": "@redapple: what about gzip,x-gzip,deflate?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647837", "body": "Is it time to move forward and remove `x-gzip` from default `Accept-Encoding`? :)\n\nChrome doesnt send x-gzip anymore:\n\n```\nGET / HTTP/1.1\nHost: example.com\nConnection: keep-alive\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.76 Safari/537.36\nDNT: 1\nAccept-Encoding: gzip,deflate,sdch\nAccept-Language: en-US,en;q=0.8,es-419;q=0.6,es;q=0.4\n```\n\nFirefox neither:\n\n```\nGET / HTTP/1.1\nHost: example.com\nUser-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:24.0) Gecko/20100101 Firefox/24.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nConnection: keep-alive\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4956151", "body": "IDEA: as classes are declared in order, with parents been defined before childs, it's possible to set a class attribute into the first class built from this metaclass.\n\nand I think we can simplify finding the new class name by always pointing to the ~~first~~ second element of the MRO.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4956151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104487", "body": "PYPI says it was released 2014-01-17 :) \n\n![image](https://f.cloud.github.com/assets/37369/1943916/f8783836-7fb1-11e3-9348-0ec972f5b626.png)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104487/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796", "body": "Can we avoid checking for \" xmlns \" in every loop iteration? it is an invariant check. move it to the top of the function.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810", "body": "alternative and less boilerplate: `inputs.extend(formdata.iteritems())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942", "body": "I tried removing the call to `_nons()` and still passed all tests.\nWhat about removing it completely of figuring out a test case that justifies its use? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002", "body": "not testing for string before splitting?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "jesuslosada": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/61c0b1478284b02a4fcfd2cc4931587c348c5d3a", "message": "Fix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a0836b8fd9720a9439cb3b940aca53b6844a094b", "message": "Fix link in news.rst"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "redapple": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/461f9daff5747728e26cd60e9dfe531092f58132", "message": "Update release notes for upcoming 1.4.1 version"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2906", "title": "[WIP] Downloader timings in request.meta", "body": "While working on a requests log plugin for Scrapy, I felt the need to get more fine-grained timing information on the various steps a `Request` goes through from reaching the `Downloader` to getting the associated response body.\r\nOne can work with `request.meta['download_delay']` and timing inserted from a downloader middleware but I found this additional information interesting for example for HAR output.\r\n\r\nNote: This does not address DNS lookup time nor TCP/TLS connection time.\r\n\r\nI would even set this to ON by default myself.\r\n\r\nMissing:\r\n\r\n- [ ] Documentation\r\n- [ ] How does this play with proxies?\r\n- [x] Handle download exceptions (DNS errors, dowload timeouts, connection errors etc.)", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/4617865", "body": "@pablohoffman , what would you think if the order was changed to `gzip,deflate,x-gzip`?\nI've found at least 1 web server that doesn't compress responses when `x-gzip` is first\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4617865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4650042", "body": "yes @dangra , `gzip,x-gzip,deflate` also works for this server\n(as does `gzip,deflate,x-gzip`)\n\nFrom wireshark:\n\n```\nGET / HTTP/1.1\nHost: ...\nAccept-Language: en\nAccept-Encoding: gzip,x-gzip,deflate\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Scrapy/0.21.0 (+http://scrapy.org)\n```\n\n```\nHTTP/1.1 200 OK\nCache-Control: no-cache\nPragma: no-cache\nContent-Type: text/html; charset=utf-8\nContent-Encoding: gzip\nExpires: -1\nVary: Accept-Encoding\nServer: Microsoft-IIS/7.5\nSet-Cookie: ASP.NET_SessionId=xyps42n5jtpprtrormg45q45; path=/; HttpOnly\nX-AspNet-Version: 2.0.50727\nX-Powered-By: ASP.NET\nDate: Wed, 20 Nov 2013 09:47:54 GMT\nContent-Length: 11786\n\n.............`.I.%&/m.{.J.J..t...`.$..@.........iG#).*..eVe]f.@......{....{....;.N'...?\\fd.l..J...!....?~|.?\"~..7N....j.^..t...#|.....<-...4}.~.(}...2[^|.Q..(..Y.|..w^?-.lR.3j..G..w.^..........(.....M......&@.....A@[.L..w~.w.:;...#|A....E.$P..]=.{...j|uo\\..ww.>|x...D.....;...lF?...7N~.>h....^..U....y..M.~./?n.l9K.[-..gY3/.e...,.y..>/......y..x|W.0#,.6#&hW../Z...}4..m.l.A.uv..........c?&.-.E..Go.....5^......7...n..w...^........F..|Y,..)..y=Ji.\nQ._..W...w%.j..+. ..,o.u.j.\n.^L9.E$.....~..R.Y..k..d._1.K.xV..eV\n.j..M.5.`-.+.h..2o...:.......j4/.a.(..2%..u.V.y..n...8.a\\.D...jo...IY..5.'.._dK.'.J...&........F3...{.~`...3}.<.....u[y... @O....V$\"....rc...!]S...h...g.`..8[..7.9i:..u^~...u.N.mZ.f..P].}T,h.w.m.g..?k..g..tL..@..m>..\n.pN.l..UuQ...h...i......(.?.r./?y.-.G.;;.3\".......X......x......{......\n...\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4650042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "raphapassini": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/a1cc5a63d3e253c325159fdc6ebf4cd3faa37c49", "message": "Add mention to dont_merge_cookies in CookiesMiddlewares docs (#2999) (#3030)\n\nAdd mention to dont_merge_cookies in CookiesMiddlewares docs (#2999)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lopuhin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/bb1f31189128cb2272c1302350387075fbbb730a", "message": "Add PyPy3 support to faq and install doc"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/041308afe7c40de7088f75b0e0c312ecd5de428a", "message": "Fix get_func_args test for pypy3\n\nThese built-in functions are exposed as methods in PyPy3.\nFor scrapy this does not matter as:\n1) they do not work for CPython at all\n2) get_func_args is checked for presense of an argument in scrapy,\n   extra \"self\" does not matter.\nBut it still makes sense to leave these tests so that we know we\nshouldn't use get_func_args for built-in functions/methods."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f71df6f9addca10b562bb22890b5ea1c37efde5c", "message": "Run tests for PyPy3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/ea41114cf0ab2782650792ad204cf43fc148c749", "message": "Mention PyPy support, add PyPy to install docs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hugovk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/cbcf80b98ff66db1ccf625fa52c4de8935331972", "message": "Fix typo\n\n[CI skip]"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f11c21c6fc62b64a2bbee0e19e2098ed6257cf19", "message": "Test on Python 3.4"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/44623687ab8936c5696f68f74e438a2891880c82", "message": "Drop support for EOL Python 3.3"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tchiotludo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3072", "title": "Handle Webp Image transparency", "body": "Webp transparent image have a dark background : \r\n\r\nreference image : https://www.lavera.de/typo3temp/fl_realurl_image/105115-4021457609482-glossylips-deliciouspeach09-1294b-43402bf213-6cf9c.png", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BrandonSmithJ": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3062", "title": "Use as a library: Reactor decorator code + MWE", "body": "First, let me say thanks for the extremely useful tool - it really has made a lot of things much simpler than they otherwise would have been. \r\n\r\nWith that in mind, I'd like to give back what I can if it's useful. The problem I'm having is I don't know exactly where to post this. I'm not familiar enough with scrapy internals to suggest where to put the feature, or if it's even necessary enough to put into the main code base. It also likely needs one more modification before being ready for a main branch contribution. \r\n\r\nThe problem this solves is something at least a few people seem to have run into:\r\n\r\n- https://stackoverflow.com/questions/35289054/scrapy-crawl-multiple-times-in-long-running-process\r\n\r\n- https://stackoverflow.com/questions/22116493/run-a-scrapy-spider-in-a-celery-task/22202877#22202877\r\n\r\nMy use case is a website which requires a virtually endless process able to use various scrapy functionalities at whim. To solve the problem of restarting the twisted reactor, I essentially took the solutions I could find and rolled everything into an extremely simple decorator. Basically all the end-user needs to do is label the function they'd like to run as a scrapy function, and the decorator handles everything necessary in order to use scrapy in a library capacity:\r\n\r\n```\r\n\t@reactor_process(timeout=10)\r\n\tdef execute(self, keyword_list):\r\n\t\t''' Crawl the site specifically for certain keywords '''\r\n\t\tfrom crawlers.backend.spider_interface import construct_spider\r\n\t\tfrom scrapy.crawler import CrawlerProcess\r\n\r\n\t\tspider = construct_spider(self)\r\n\t\tspider.create_start_urls(keyword_list)\r\n\t\t\r\n\t\tcrawler  = CrawlerProcess(self.settings)\r\n\t\tdeferred = crawler.crawl(spider)\r\n\t\tdeferred.addBoth(lambda _: crawler._stop_reactor())\r\n```\r\n\r\nWithout the decorator, the function above would only be able to run once - the reactor engine would complain about being restarted. The one problem is that the necessary imports have to be made *within* the function itself - though I think this can be solved with a multiprocessing manager (or worst case, global inspection). \r\n\r\nI also have an interface which allows dynamic creation of contract docstrings / crawlers with different settings inside the same process, which might be useful; but that's something that would probably be better as its own discussion. \r\n  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Matthijsy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3061", "title": "Make autothrottle slow down on HTTP 429 response", "body": "When a response has status 429 (Too Many Requests) it is ignored by the AutoThrottle (because the latency is low, so otherwise the spider will speed up). I think that the wanted behaviour is that the spider will slow down when a 429 is received, but currently the spider will stay at a constant scraping speed. \r\n\r\nThis PR adds a new setting `AUTOTHROTTLE_429_DELAY` when it is set the download delay will be increased with this value every time a 429 is received. It still respects the 'MAX_DOWNLOAD_DELAY` value.  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "parlays": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3051", "title": "The URI gets set again before feed export store gets called.  This al\u2026", "body": "\u2026lows the spider to change settings in the parse method based on the content scraped.  The IFeedStorage interface has been changed for the store method to allow uri to be passed.  The BlockingFeedStorage class now sets the new uri settings in the store method.\r\n\r\nIn my personal scraper I overwrite the FeedExporter object and S3FeedStorage object.  But it would be great if this was part of the core.  It may be a tough change though because it requires a change to the IFeedStorage interface and that is a breaking change for older code.\r\n\r\nIt is common for me to scrape a site and then decide the folder name to use in the S3 bucket.  To be able to change the bucket name or directory in the parse method is important.  If this change is not agreed upon, let's think of another way to accomplish this.  Thanks!", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "munderseth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3041", "title": "Add Test Reporting to Travis CI", "body": "**Test Reporting for Travis using [Testspace.com](https://testspace.com)**\r\n\r\nHi `scrapy` team. We added test reporting to your repo. We have a blog article on [why we are staging repos](https://blog.testspace.com/testspace-and-open-source).\r\n\r\nNote that we contacted you in the past via an email. We thought it would be easier for you (if interested) by using a Pull Request.   \r\n\r\nFew of the benefits using Testspace:\r\n- view all your test results from a single dashboard\r\n- triage and manage your test failures faster\r\n- add more metrics\r\n- leverage built-in analytics \r\n- get a [test badge](https://help.testspace.com/how-to:get-badge) \r\n[![Space Health](https://open.testspace.com/spaces/74470/badge?token=2f48b759d52023674602ea42e90a5508cad6c953)](https://open.testspace.com/spaces/74470?utm_campaign=badge&utm_medium=referral&utm_source=test \"Test Cases\")\r\n\r\nCheckout **your** test results: https://open.testspace.com/projects/TryTestspace:scrapy from our fork.\r\n\r\n**Why** we are doing this: https://blog.testspace.com/testspace-and-open-source \r\n\r\n----\r\n\r\n**Give Testspace a try?**\r\n\r\n1. Create **Open** (free) account: https://testspace.com/pricing\r\n2. Add a **New Project** from the list of Github repo's\r\n3. Add a Travis **Environment Variable**: Name: `TS_ORG`  Value: `organization name` - based on subdomain selected in step 1\r\n4. Invite others: https://help.testspace.com/how-to:invite-other-users\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslay88": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3040", "title": "Fix for Issue #2919", "body": "Splits out url params and updates them with formdata. Passes local tests.\r\n\r\nFirst PR, please be nice :)\r\n\r\nFixes Issue #2919 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mylh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3028", "title": "Update practice.rst docs", "body": "added tips to avoid getting banned", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chainly": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3027", "title": "fix Spider.log to record right caller information", "body": "Spider.log always log itself in `pathname), funcName, lineno, etc.` format. Because `currentframe = lambda: sys._getframe(3)` and `f = f.f_back` in ``logging.Logger.findCaller`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3016", "title": "Added Scrapy logo", "body": "I replaced the \"Scrapy\" text with the logo shown on the Scrapy website", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web crawling.\nI thought that you guys would be doing that in multithreading but I heard that you guys never use any threads.How could this be possible?can anyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128", "body": "@gmeans code gives warning:\r\n\r\n /usr/local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py:51: builtins.UserWarning:\r\n         'broadd.context.CustomContextFactory' does not accept `method` argument (type OpenSSL.SSL method, e.g. OpenSSL.SSL.SSLv23_METHOD). Please upgrade your context factory class to handle it or ignore it.\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454", "body": "@redapple I had errors:\r\n`<twisted.python.failure.Failure OpenSSL.SSL.Error: ('SSL routines', 'SSL3_READ_BYTES', 'sslv3 alert handshake failure'), ('SSL routines', 'SSL3_WRITE_BYTES', 'ssl handshake failure')>` \r\nwith all packages up-to-date", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745", "body": "Scrapy    : 1.4.0\r\nlxml      : 3.8.0.0\r\nlibxml2   : 2.9.4\r\ncssselect : 1.0.1\r\nparsel    : 1.2.0\r\nw3lib     : 1.17.0\r\nTwisted   : 17.5.0\r\nPython    : 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 17:54:52) [MSC v.1900 32 bit (Intel)]\r\npyOpenSSL : 17.1.0 (OpenSSL 1.1.0f  25 May 2017)\r\nPlatform  : Windows-10-10.0.15063-SP0\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674", "body": "@redapple Thank you", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web \ncrawling.\nI thought that you guys would be doing that in multithreading but I \nheard that you guys never use threading.How could this be possible?can \nanyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160", "body": "Thank you, missconfiguration.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683", "body": "https://github.com/kjd/idna/issues/50#issuecomment-312908539", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "isra17": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2996", "title": "Add signals to handle error from downloader or pipeline", "body": "As of right now, there's is no simple way to handle exception coming from items pipeline or a the `process_response` of a downloader middleware.\r\n\r\nThis PR adds two signals for those use case. Note that the downloader signals also catch any exception from the downloader engine, including `process_request` and `process_exception`.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2995", "title": "Allow passing Failure object to middlewares", "body": "Trying to handle errors coming from middleware, it happens that Twisted strip traceback from an exception returned from a completed deferred (Needed to avoid GC issues). This mean that trying to use `exception.__traceback__` always yield `None`.\r\n\r\nThis PR adds a decorator that can be used on middleware `process_exception` or `process_spider_exception` to get a Failure object instead of an Exception. The Failure object provides a `getTracebackObject` to get a `Traceback`-like object which come really handy when trying to pinpoint an issue.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2962", "title": "Add signal `request_downloading` called right before the download handler", "body": "It was needed for one of our project, maybe it can be useful for core as well.\r\n\r\nThe `request_downloading` signal is sent when the engine is about to download a request. If one handler raise an exception, the download is aborted. The signal supports returning deferreds from their handlers.\r\n\r\nThis is an alternative to the Downloader middlewares where there might be a significant delay between the middleware call and the download handler in case of slow queue processing.\r\nThis even handler allow some extension to tamper the request right before the download time and possibly cancel the request by raising an exception.\r\n\r\n`signal.send_deferred` was needed for the exception handling raised by signal handler in the download manager.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "starrify": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2988", "title": "Added: Customizing request fingerprint calculation via request meta", "body": "Partially implements #900", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2984", "title": "Added: Making the list of exceptions to retry configurable via settings", "body": "Implements #2701", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "immerrr": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2986", "title": "WIP: CookiesMiddleware: add \"reset_cookies\" meta to clear the jar", "body": "This PR adds a `reset_cookies` meta to clean the active cookiejar.\r\n\r\nWhen I try working with sessions I often find myself in a situation when I'd like to restart the session \"from scratch,\" including cookies. It's doable by just setting `cookiejar` meta to an arbitrary value, but then we'd be accumulating the cookiejars over time.\r\n\r\nI'm not entirely sure about the meta name, as I'd probably like it namespaced, e.g. `cookies_reset`, but there's a precedent already with `dont_merge_cookies`, so I chose to follow that pattern.\r\n\r\n@kmike am I missing something here?", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2985", "title": "utils.curl: add parse_curl_cmd func", "body": "Given that the major browsers are able to export the requests in cURL format, it's logical that we have a utility in scrapy to make it a request.\r\n\r\nThis PR works towards adding a function that creates kwargs that could be passed to a `Request` constructor.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2950", "title": "loader add_* funcs: pass **kw to self.selector.xpath", "body": "I have found out that kwargs that one specifies for `loader.add_xpath` are not getting to the actual `selector.xpath` invocation, which looks like a bug.\r\n\r\nI wonder if there's code out there that would be broken by this fix...", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aitoehigie": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2972", "title": "Add a note to allowed_domains", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NoExitTV": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2955", "title": "Handle \"invalid\" relative URL issue #1304", "body": "Did some minor tweaks on how scrapy handle relative URL's as discussed in #1304 \r\n\r\nTested it with some basic code in the scrapy shell:\r\n```\r\n>>> resp = scrapy.http.response.html.HtmlResponse('http://www.example.com',\r\n...      body='''<html>\r\n...                  <body>\r\n...                      <a href=\"../index1.html\">Link1</a>\r\n...                      <a href=\"../../index2.html\">Link2</a>\r\n...                      <a href=\"index3.html\">Link3</a>\r\n...                      <a href=\"other_html/../index4.html\">Link4</a>\r\n...                      <a href=\"other_html/folder2/../index5.html\">Link5</a>\r\n...                      <a href=\"other_html/index6.html\">Link6</a>\r\n...                      <a href=\"../other_html/index7.html\">Link7</a>\r\n...                 </body>\r\n...          </html>''')\r\n>>> links = scrapy.linkextractors.LinkExtractor().extract_links(resp)\r\n>>> links\r\n[Link(url='http://www.example.com/index1.html', text='Link1', fragment='', nofollow=False), Link(url='http://www.example.com/index2.html', text='Link2', fragment='', nofollow=False), Link(url='http://www.example.com/index3.html', text='Link3', fragment='', nofollow=False), Link(url='http://www.example.com/index4.html', text='Link4', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index5.html', text='Link5', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index6.html', text='Link6', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index7.html', text='Link7', fragment='', nofollow=False)]\r\n```\r\n\r\nPassed the same unit tests as the original code when running tox\r\n```\r\n========================================================= 1679 passed, 6 skipped, 14 xfailed in 458.89 seconds =========================================================\r\n\r\n_______________________________________________________________________________ summary ________________________________________________________________________________\r\n\r\n  py27: commands succeeded\r\n  congratulations :)\r\n```\r\n\r\nI believe that scrapy now handle relative url's as expected in python 2.7.13\r\nWhat are your thoughts?", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kaplun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2954", "title": "spiders: add OAI-PMH support WIP", "body": "Signed-off-by: Samuele Kaplun <samuele.kaplun@cern.ch>", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phnk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2953", "title": "Added debug message in spiders/crawl", "body": "As requested in #2925.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mGalarnyk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2931", "title": "Update install.rst", "body": "Updated installation instructions for installing scrapy using conda.\r\n\r\nYou can now just do: \r\n\r\n```\r\nconda install scrapy\r\n```", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "HarrisonGregg": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2918", "title": "Fix SIGINT handling when using inspect_response", "body": "Currently, after calling `scrapy.shell.inspect_response` and then closing the opened shell, SIGINT (Ctrl-C) no longer works to terminate the spider.  This is because `Shell.start`, called in `inspect_response`, removes the signal handler.  To fix this, save the SIGINT handler before calling `Shell.start` in `inspect_response` and add it again after the shell has closed.\r\n\r\nIt doesn't appear that there are any tests for `inspect_response`, so I haven't added any tests for this fix.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangtua1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2917", "title": "support giantfiles download ,fix CVE-2017-14158", "body": "from the issue https://github.com/scrapy/scrapy/issues/482.\r\nWe can see if we download a file with filespipeline, the whole file is stored into the memory.\r\nIf the file is a giant file or an extremely giant file,(over 1G or more), the crawling thread will be crashed.\r\nWe can use this POC to prove it.\r\n[POC.zip](https://github.com/scrapy/scrapy/files/1289514/POC.zip)\r\n\r\nThis has been asigned CVE-2017-14158.\r\nbut in this new testcase using GiantFilesPipeline in the commit,we can download it correctly.\r\n\r\n[testdownload.zip](https://github.com/scrapy/scrapy/files/1289512/testdownload.zip)\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhaojiedi1992": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2911", "title": "Update exporters.rst", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfdeshom": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118", "body": "Hi, to disable `scrapy.contrib.feedexport.FeedExporter`, I had to set it to `None` in my `EXTENSIONS` dict instead of `0`. I am getting this error when using scrapyd.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185", "body": "off-topic but: in general to disable any extension, middleware, etc I have to set it to `None`, which can be a little confusing since you would think that `0` would work too\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375", "body": "@dangra unfortunately, I don't see a documented way to disable an extension and I consider myself pretty familiar with scrapy and its docs. Maybe it's already there in the docs, but simply needs to be more prominent.\n\nFor me there is a larger issue: some extensions have settings associated with them that make it confusing to know exactly how/where to disable them. For example, the cookies extension seems to have 2 ways to disable it:\n-  set `COOKIES_ENABLED` to `False` (https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/cookies.py)\n- set the priority to `None`, ie set `scrapy.contrib.downloadermiddleware.CookiesMiddleware` to `None`\n\nIt would be nice to have just one way to disable extensions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "NicolasP": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831", "body": "Hi, any chance for a review of this PR?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "knaveofdiamonds": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951", "body": "@kmike - ah, ok. I'm working on an inherited codebase that doesn't use the `LinkExtractors` directly, so this was obviously just a misunderstanding of the API/using something non-public. I'll close this issue - agree with backwards incompatible change reasons.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nside": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706", "body": "Seeing the same issue\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703", "body": "@dangra crawl() fixed it for me. Still I'd expect any requests scheduled to have the same treatment, whatever their \"entry point\" in the pipeline is. Feel free to close if you disagree.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941", "body": "I'm on 0.21 (dev). These are good subtleties to know!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606", "body": "Hi,\n\nI've copied a URL below which failed with the original function because it has no url argument and the arguments are in an unexpected order. I can certainly add some unit tests, I'll have a look into it now. It's the end of the day and I want some food!\n\nCheers,\nRob\n\nhttp://www.firstchoice.co.uk/fcsun/page/search/searchresults?sttrkr=mthyr:02/2011_durT:7/n_ls:true_tuidesc:000832_day:15_mps:9_isvid:false_pconfig:1|2|0|0|0|_tchd:0_rating:0_act:0_jsen:true_resc:_attrstr:||||||||null|null_mdest:false_tinf:0_mnth:02_desc:_bc:17_margindt:7_tadt:2_numr:1_spp:mainSearch_depm:7_tuiresc:004287_dur:7_dtx:0_df:false_dxsel:0_imgsel:0_dac:MAN_loct:0_tsnr:0_year:2011_dta:false_tuiacc:028367_acc:\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615", "body": "Which distro are you using? Here in ubuntu, works fine. Try:\n\napt-get install -y build-essential python-dev\napt-get install -y libssl-dev libxml2-dev libxslt1-dev libssl-dev libffi-dev\n\nthen pip install scrapy in a brand new virtualenv\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982", "body": "Let me try again:\n\nI want to use scrapy like this:\n\n```\n# prints options\nscrapy crawl -t csv|sql|mongo|etc -h\n\n# uses a particular item exporter\nscrapy crawl -t csv -f somefile.csv\nscrapy crawl -t mongo --db somedb --col somecollection\n```\n\nWhat do you guys think of this? Is it desired behavior? Does not feel like a whole lot of code to modify. I could do it. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ariddell": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661", "body": "@nramirezuy there's a reference implementation for pep 3156 here: https://code.google.com/p/tulip/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sabren": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629", "body": "Here you go: \n\nhttps://github.com/scrapy/scrapy/pull/45\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490", "body": "Thanks, Scotty! \n\nI'm sure my client would appreciate it.\n\nCan you make a combined pull request, or should I pull from you and open another pull request here?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "mvj3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848", "body": "Thanks @curita for the careful review! I correct it in a new commit.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "RFDAJE": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937", "body": "I had been facing the same issue, so far the simplest solution I found is inside `item_completed`, after getting all things done, reset downloaded to empty. `self.spiderinfo.downloaded = {}`, memory leak issue resolved. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "paulproteus": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573", "body": "I did some work refreshing these patches against current origin/master.\n\nhttps://github.com/paulproteus/scrapy/tree/revise-pullrequest-109\n\nSome notes:\n\nSee 3077ac8b6f592b044ad67f15af3b065d06f27cf7 for a fix where Http11DownloadHandler needs to accept second argument, since that's how the tests use it.\n\nSee 888dfadd24f7d325bffab367acc9dd5a7405e5bc for a fix where the call to log.err() creates an exception in the log that test runner notices, so tests that call this method begin to fail. For now I've disabled the call to log.err(). If we want to keep logging the error, we'll need to call flushLoggedErrors() -- see http://twistedmatrix.com/documents/current/core/howto/trial.html#auto11 . (The test this makes fail is scrapy.tests.test_downloader_handlers.Http11TestCase.test_timeout_download_from_spider )\n\nCommit bdd2a1b02c944845fec4d6142fcb63367b17cc11 (rebased from the original, but otherwise the same) makes many tests fail because there is work left in the reactor. One test you can see this with is scrapy.tests.test_engine.EngineTest.test_crawler\n\nFor now I can't promise I'll have time to figure out what's going on with leaving the reactor unclean, but I thought I'd leave a comment with what I have found in a few hours of looking into this pull request!\n\n-- Asheesh.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rmax": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800", "body": "#62 is a duplicated report of this issue but includes a pull request with a fix.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083", "body": "See issue #58\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355", "body": "As a workaround you can use `process_value` argument:\n\n`SgmlLinkExtractor(..., process_value=lambda v: v.strip())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580", "body": "I have reverted the redirect middleware order and added tests for gzipped meta redirection. Although I'm not sure about leaving the body compressed if it fails, I think there could be more errors (ie. connection drop) that could make fail the decompression and leaving the body as is could produce misbehavior in other components.\n\nAdding more tests for the middlewares integration could help us to identify the best solution.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823", "body": "without any fix fails test_gzipped_redirect_30x\nwith the redirect reorder fails test_gzipped_meta_redirect\nwith dangra's suggestion passes all tests (See https://gist.github.com/1718659)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882", "body": "Nice!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "djm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/426681", "body": "@pablohoffman Cheers!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/426681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aalvarado": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/749296", "body": "```\nfound in the dmoz directory\n```\n\nI think this wasn't updated. Should say `tutorial` now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/749296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "mohsinhijazee": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2180902", "body": "The settings have FEED_URI whereas here it is picking up from SCRAPY_FEED_URI. I think this is kind of conflicting  here.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2180902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "pablohoffman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2184054", "body": "When you access `settings['FEED_URI']` Scrapy ends up looking at the `SCRAPY_FEED_URI` environment variable.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2184054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379561", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701990", "body": "Indeed, shorter and faster. Change pushed, thanks!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701990/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863", "body": "Is there a way to avoid calling a protected method of lxml.html?. As this may raise some compatibility issues on future/past lxml versions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702", "body": "touple -> tuple\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727", "body": "should we make these settings dicts (like extension and middlewares) and use `scrapy.utils.conf.build_component_list` to load them?.\n\nany particular reason why you went with lists instead?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737", "body": "I think this should inherit from `AssertionFailure`, to appear as test failures (and not errors) in some testing frameworks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755", "body": "I think I would prefer two contracts for these:\n\n```\n@minitems 1\n@minrequests 2\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772", "body": "Why do we need this method for?. I don't see it used, and we already have `adjust_request_args`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784", "body": "what about just ignoring those who don't?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786", "body": "perhaps in the future we can add a way to list contracts per spider\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791", "body": "I'm not sure a separte register method is needed. How about the constructor receiving the contracts? Like middlewares and extension do. See `scrapy.middleware` module.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796", "body": "these changes should go into a separate PR\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460", "body": "I think we should reuse the convention of using dicts. (we were planning to port pipelines too).\n\nIt's true that priorities won't add much in in this case, but it's not only priorities what the dict mechanism provides, but also being to disable specific components of the (presumably most sensible default). Priorioties wont' add much to this (as they don't to extensions) but don't harm and it could be useful if someone ever needs them. We should make sure internal code in an intuitive behaviour. Like pre_process being called in priority-order.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558", "body": "I wasn't considering all cases. In that case, how about this:\n\n```\n@returns <type> [M [N]]\n```\n\nHere are some examples to illustrate:\n- `@returns request` - returns at least one request\n- `@returns request 2` - returns at least two requests\n- `@returns request 2 10` - returns at least two requests and no more than 10 requests\n- `@returns request 0 10` - returns no more than 10 requests\n\nSame goes for items.\n\nMaybe make aliases (`request` -> `requests` , `item` -> `items`) so that both plurals and singulars work.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903", "body": "I can't see that comment on github now, did you remove it?\n\nOn Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi\nnotifications@github.comwrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> \n> @returns requests 2 - returns at least two requests\n> \n> -- wouldn't this be considered unexpected behaviour?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/167/files#r1570864.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908", "body": "this one looks very similar to `scrapy.utils.get_func_args` (which is tested and supports methods, function, classes, etc) - could that one be reused?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916", "body": "`from scrapy.conf import settings` is deprecated  API, use `self.settings` within the command methods,  that attribute is assigned by the `scrapy.cmdline` mechanics.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "nuklea": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701979", "body": "`isinstance(arg, (dict, BaseItem))` short.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155", "body": "What about pep8?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brunsgaard": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2853556", "body": "Ahh yeah.. good point. I will write up another commit in the near future taking the settings instance from the crawler. Had totally forgotten about overrides :/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2853556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "archerhu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2965291", "body": "can you explain why you remove the try catch?\n\nat 1.6 version, using ImagePipeline may raise a lot \"exceptions.IOError: image file is truncated\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2965291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976024", "body": "I get your\u00a0motivation,thanks very much\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "nramirezuy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048968", "body": "ssh! :dancer: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104492", "body": "It is 2014-01-17 on UTC :P\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "scottyallen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154", "body": "Thanks for the patch - I was in the process of trying to fix this, and it saved me a ton of time:)  However, I don't think line 191 is quite right for the tunnel case.  It results in sending a GET request with the full url to the destination webserver, which is technically wrong and some sites refuse to handle.  Instead, self.path should remain unchanged for the tunnel case.  I can send a patch to your patch, if you like...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "alexcepoi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975", "body": "what about maxitems, maxrequests? Or the case where you expect to receive exactly one request (the original sep describes a returns_request contract which checks this).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004", "body": "Ignoring sounded unfriendly to me at first (i.e. you may wonder why it does not work). Also eliminating the assertion gives a very misleading error.\nListing contracts sounds good to me maybe a \"--list / -l\" option?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025", "body": "This is a reminiscence from the original implementation. I thought it could prove useful, but now that I think about it's hard to find a scenario in which adjust_request_args is insufficient.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066", "body": "I did not think priorities would be useful, or that we should encourage users to rely on contracts priorities.\nEspecially since for some hooks (pre_process comes into mind) the last contract hooked in is the first one to be executed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090", "body": "note: also change docstring\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864", "body": "`@returns requests 2` - returns at least two requests\n\n-- wouldn't this be considered unexpected behaviour?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929", "body": "It's an outdated diff (i.e. comment on a line of code which has been modified). You should still be able to find it in the issue\n\nOn Sep 10, 2012, at 11:30 PM, Pablo Hoffman notifications@github.com wrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> >   I can't see that comment on github now, did you remove it? On Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi notifications@github.comwrote:\n> >   \u2026\n> >   \u2014\n> >   Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}, "2": {"FredEnglish": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3079", "title": "Connection Lost in a non-clean fashion for some URLs on a particular domain, but not others", "body": "I have created a basic spider to scrape a small group of job listings from totaljobs.com. I have set up the spider with a single start URL, to bring up the list of jobs I am interested in. From there, I launch a separate request for each page of the results. Within each of these requests, I launch a separate request calling back to a different parse method, to handle the individual job URLs.\r\n\r\nWhat I'm finding is that the start URL and all of the results page requests are handled fine - scrapy connects to the site and returns the page content. However, when it attempts to follow the URLs for each individual job page, scrapy isn't able to form a connection. Within my log file, it states:\r\n\r\n`[<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]`\r\n\r\nI'm afraid that I don't have a huge amount of programming experience or knowledge of internet protocols etc. so please forgive me for not being able to provide more information on what might be going on here. I have tried changing the TLS connection type; updating to the latest version of scrapy, twisted and OpenSSL; rolling back to previous versions of scrapy, twisted and OpenSSL; rolling back the cryptography version, creating a custom Context Factory and trying various browser agents and proxies. I get the same outcome every time: whenever the URL relates to a specific job page, scrapy cannot connect and I get the above log file output.\r\n\r\nIt may be likely that I am overlooking something very obvious to seasoned scrapers, that is preventing me from connecting with scrapy. I have tried following some of the the advice in these threads:\r\n\r\n[https://github.com/scrapy/scrapy/issues/1429](https://github.com/scrapy/scrapy/issues/1429)\r\n[https://github.com/requests/requests/issues/4458](https://github.com/requests/requests/issues/4458)\r\n[https://github.com/scrapy/scrapy/issues/2717](https://github.com/scrapy/scrapy/issues/2717)\r\n\r\nHowever, some of it is a bit over my head e.g. how to update cipher lists etc. I presume that it is some kind of certification issue, but then again scrapy is able to connect to other URLs on that domain, so I don't know.\r\n\r\nThe code that I've been using to test this is very basic, but here it is anyway:\r\n\r\n```\r\nimport scrapy\r\n\r\nclass Test(scrapy.Spider):\r\n\t\r\n\r\n\tstart_urls = [\r\n\t\t\t\t\t'https://www.totaljobs.com/job/welder/jark-wakefield-job79229824'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/elliott-wragg-ltd-job78969310'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/exo-technical-job79019672'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/exo-technical-job79074694'\r\n\t\t\t\t\t\t]\r\n\t\r\n\tname = \"test\"\r\n\r\n\tdef parse(self, response):\r\n\t\tprint 'aaaa'\r\n                yield {'a': 1}\r\n```\r\n\r\nThe URLs in the above code **are not** being connected to successfully.\r\n\r\nThe URLs in the below code **are** being connected to successfully.\r\n\r\n```\r\nimport scrapy\r\n\r\nclass Test(scrapy.Spider):\r\n\t\r\n\r\n\tstart_urls = [\r\n\t\t\t\t\t'https://www.totaljobs.com/jobs/permanent/welder/in-uk'\r\n\t\t\t\t\t,'https://www.totaljobs.com/jobs/permanent/mig-welder/in-uk'\r\n\t\t\t\t\t,'https://www.totaljobs.com/jobs/permanent/tig-welder/in-uk'\r\n\t\t\t\t\t\t]\r\n\t\r\n\tname = \"test\"\r\n\r\n\tdef parse(self, response):\r\n\t\tprint 'aaaa'\r\n                yield {'a': 1}\r\n```\r\n\r\n\r\nIt'd be great if someone could replicate this behavior (or not as the case may be) and let me know. Please let me know if I should submit additional details. I apologise, if I have overlooked something really obvious. I am using:\r\n\r\nWindows 7 64 bit\r\nPython 2.7\r\nscrapy version 1.5.0\r\ntwisted version 17.9.0\r\nopenSSL version 17.5.0\r\nlxml version 4.1.1", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shanmuga-cv": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077", "title": "scrapy selector fails when large lines are present response", "body": "Originally encoutered when scraping [Amazon restaurant](https://www.amazon.com/restaurants/zzzuszimbos0015gammaloc1name-new-york/d/B01HH7CS44?ref_=amzrst_pnr_cp_b_B01HH7CS44_438).  \r\nThis page contains multiple script tag with lines greater then 64,000 character in one line. \r\nThe selector (xpath and css) does not search beyond these lines. \r\n\r\nDue to this the following xpath `'//h1[contains(@class, \"hw-dp-restaurant-name\")]/text()'` to extract name of the restaurant returns empty even though there is a matching tag is present.\r\n\r\n\r\nPFA the response text at [original_response.html.txt.gz](https://github.com/scrapy/scrapy/files/1631425/original_response.html.txt.gz)\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "crisfan": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075", "title": "Telnet Console ", "body": "hi, @kmike,I use telnetlib to pause scrapy engine, there is some error! although it pause the scrapy engine.\r\nthis is my poor code:\r\n``` javascript\r\nimport telnetlib\r\nhost = '127.0.0.1'\r\ntn = telnetlib.Telnet(host, 6023)\r\ntn.write(\"engine.pause()\\n\") \r\n``` \r\nerror:\r\n``` javascript\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n``` \r\n\r\ncould you fix my problem, thansk ~~~\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Caleb-Wade": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074", "title": " Cxfreeze+python3.4+scrapy1.4 failed to bundle to executables(AttributeError: module object has no attribute '_fix_up_module')", "body": "After I failed to bundle .py document into .exe document by pyinstaller, I tried cxfreeze. Similar error happened. Someting went wrong when importing scrapy module, and AttributeError: module object has no attribute '_fix_up_module' appeared in the command window. Would someone tell me what happened. Can I make it? Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073", "title": "Pyinstaller+Python3.4+Scrapy1.4 Failed to bundled exe (ImportError:No module named \"XXXX\")", "body": "I have tried to bundle my spider .py into .exe, but weeks pasted, I didn't make it. It pointed that some modules were missing. Even I used --hidden-import to put the missing modules into my bundled exe, it still didn't work. It's amazing that I can see the modules in the HTML document under build file. Pls help me ,Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lifei1245": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069", "title": "about the signal retry_complete", "body": "I didn't find the singnal in the singnal list,how can I use it", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "3xp10it": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068", "title": "scrapy always Starting new HTTP connection after crawl finished ", "body": "I reopen [#3066][1] here,there are more details [here][2].\r\n\r\n[1]: https://github.com/scrapy/scrapy/issues/3066\r\n[2]: https://stackoverflow.com/questions/48182204/scrapy-always-starting-new-http-connection-after-crawl", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "theduman": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067", "title": "scrapy.Request doesnt wait for loop", "body": "I fetch url and other parameters from database and use them in for loop and pass to scrapy.request() function but when i run the code i succesfully pass first element only. Scrapy can't get other elements of list. Here is my code\r\n\r\n```py\r\nclass QuotesSpider(scrapy.Spider):\r\n    name = \"quotes\"\r\n    custom_settings = {\r\n        'CONCURRENT_REQUESTS': 1,\r\n    }\r\n    def start_requests(self):\r\n        sourceArr = []\r\n        connection = db.connect()\r\n        try:\r\n            with connection.cursor() as cursor:\r\n                # Read a single record\r\n                sql = \"SELECT `code`, `url`, `target`,`next` FROM `source`\"\r\n                cursor.execute(sql)\r\n                result = cursor.fetchall()\r\n        finally:\r\n            connection.close()\r\n        print(result)\r\n        for i in result:\r\n            source = Source(i['code'], i['url'], i['target'], i['next'])\r\n            sourceArr.append(source)\r\n        print(sourceArr)\r\n        #for url in urls:\r\n           #yield scrapy.Request(url=url, callback=self.parse)\r\n        for s in sourceArr:\r\n            print(s.target)\r\n            yield scrapy.Request(url=s.url, meta={'target': s.target, 'next': s.next})\r\n            print(\"slept\")\r\n\r\n\r\n    def parse(self, response):\r\n        titlearr = []\r\n        count = 0\r\n        print(response.meta)\r\n        for title in response.css(response.meta['target']):\r\n            count += 1\r\n            titlearr.append(title.css('p a::text').extract_first())\r\n            #yield {'title': title.css('p a::text').extract_first()}\r\n        print(\"total count \" + str(count))\r\n        print(titlearr)\r\n        for next_page in response.css(response.meta['next']):\r\n            yield response.follow(next_page, self.parse)\r\n```\r\n\r\nWhen i print response.meta i got target and next values for only first item. List contains more than 1 item. How can i make scrapy.Request() function wait for the next element to run?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "benjolitz": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065", "title": "`Connection to the other side was lost` for older French site", "body": "Hi,\r\n\r\nI'm attempting to run scrapy on `https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`.\r\n\r\nDespite upgrading all my brew packages, scrapy installation via `pip install -U scrapy`, the website disconnects uncleanly. Specifying `-s DOWNLOADER_CLIENT_TLS_METHOD=TLSv1.0` makes no difference to the following run. Neither does a user-agent clear things up with `-s USER_AGENT=\"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\"`\r\n\r\nI am uncertain as how to debug this further.\r\n\r\n`scrapy shell https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`\r\n\r\n```\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Darwin-16.7.0-x86_64-i386-64bit\r\n2018-01-09 18:03:33 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOADER_CLIENT_TLS_METHOD': 'TLSv1.0', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2018-01-09 18:03:33 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2018-01-09 18:03:33 [scrapy.core.engine] INFO: Spider opened\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\nTraceback (most recent call last):\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/commands/shell.py\", line 73, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 48, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 115, in fetch\r\n    reactor, self._schedule, request, spider)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/python/failure.py\", line 385, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n```\r\n\r\n\r\n`scrapy version -v`:\r\n```\r\nScrapy       : 1.5.0\r\nlxml         : 4.1.1.0\r\nlibxml2      : 2.9.7\r\ncssselect    : 1.0.3\r\nparsel       : 1.3.1\r\nw3lib        : 1.18.0\r\nTwisted      : 17.9.0\r\nPython       : 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)]\r\npyOpenSSL    : 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017)\r\ncryptography : 2.1.4\r\nPlatform     : Darwin-16.7.0-x86_64-i386-64bit\r\n```\r\n\r\n`curl -sLv https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences 2>&1 | head -20`:\r\n```\r\n*   Trying 160.92.134.3...\r\n* TCP_NODELAY set\r\n* Connected to agences.creditfoncier.fr (160.92.134.3) port 443 (#0)\r\n* TLS 1.0 connection using TLS_RSA_WITH_3DES_EDE_CBC_SHA\r\n* Server certificate: agences.creditfoncier.fr\r\n* Server certificate: RapidSSL RSA CA 2018\r\n* Server certificate: DigiCert Global Root CA\r\n> GET /credit-immobilier/toutes-nos-agences HTTP/1.1\r\n> Host: agences.creditfoncier.fr\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 200 OK\r\n< Date: Wed, 10 Jan 2018 02:06:54 GMT\r\n< Server: Microsoft-IIS/6.0\r\n< X-Powered-By: ASP.NET\r\n< Content-Length: 35884\r\n< Content-Type: text/html\r\n< Set-Cookie: ASPSESSIONIDQQBACDTS=JHOJNNCCJNIFABOPLNCEEAFH; path=/\r\n< Cache-control: private\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "reaCodes": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064", "title": "I ran into a problem when I used ipython in a scrapy shell", "body": "I used this command, `scrapy shell 'www.baidu.com'`, and then used `response.body`. \r\n\r\nWhen I input `resp` and click `Tab`\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660511-e159fe1a-f47d-11e7-8499-b6589ef7de3f.png)\r\n\r\nAs you can see, there was a display error\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660548-16f0b794-f47e-11e7-996b-9862f788a771.png)\r\n\r\nUse the command completion function is encountered a display error", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmagonski": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060", "title": "HTTPERROR_ALLOWED_CODES can't parse response with 301", "body": "In scrapy.downloadermiddlewares.redirect.RedirectMiddleware#process_response \r\ndosn't get anything from settings like HTTPERROR_ALLOWED_CODES only in HttpErrorMiddleware we have \r\nhandle_httpstatus_list.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NewUserHa": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057", "title": "configure_logging unable to handle GBK", "body": "I added \r\n`import logging\r\nfrom scrapy.utils.log import configure_logging\r\n\r\nconfigure_logging(install_root_handler=False)\r\nlogging.basicConfig(\r\n    filename='log.txt',\r\n    format='%(levelname)s: %(message)s',\r\n    level=logging.INFO\r\n)`\r\nfrom scrapy documents blow my class define.\r\n\r\nthen:\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\logging\\__init__.py\", line 982, in emit\r\n    stream.write(msg)\r\nUnicodeEncodeError: 'gbk' codec can't encode character '\\ufe0f' in position 190: illegal multibyte sequence\r\nCall stack:\r\n....\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\core\\scraper.py\", line 237, in _itemproc_finished\r\n    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\r\nMessage: 'Scraped from %(src)s\\r\\n%(item)s'\r\nArguments: {'src': <200 http://...>, 'item': {'date': '12-02', 'floor': '...\\n    \u7535\\ufe0f', 'pics': ['...', ...]}}\r\n\r\nI also have 'LOG_LEVEL': 'INFO' in the spider.\r\n\r\nI googled and have no idea how to fix this.\r\nany help, please?\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056", "title": "[suggest] add a option to pipeline for some sites requiring reference in heads", "body": "it's usual case and it's ugly to override get_media_requests method of pipelines.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055", "title": "[bug] pillow will always recode images in imagepipieline", "body": "https://github.com/scrapy/scrapy/blob/aa83e159c97b441167d0510064204681bbc93f21/scrapy/pipelines/images.py#L151\r\n\r\nthis line will always recode images silently and damage the image quality.\r\n\r\nplease add an option to avoid this.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elacuesta": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054", "title": "Request serialization should fail for non-picklable objects", "body": "The Pickle-based disk queues silently serialize requests that shouldn't be serialized in Python<=3.5. I found this problem when dumping a request with an `ItemLoader` object in its `meta` dict. Python 3.6 fails in [this line](https://github.com/scrapy/scrapy/blob/1.4/scrapy/squeues.py#L27) with `TypeError: can't pickle HtmlElement objects`, because the loader contains a `Selector`, which in turns contains an `HtmlElement` object.\r\n\r\nI tested this using the https://github.com/scrapinghub/scrapinghub-stack-scrapy repository, and found that `pickle.loads(pickle.dumps(selector))` doesn't fail, but generates a broken object.\r\n\r\n#### Python 2.7, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3)\r\n```\r\nroot@04bfc6cf84cd:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 2.7.14 (default, Dec 12 2017, 16:55:09) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@04bfc6cf84cd:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:49:27 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath=u'descendant-or-self::a' data=u'<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 140144569743064\r\n```\r\n\r\n\r\n#### Python 3.5, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@1945e2154919:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.5.4 (default, Dec 12 2017, 16:43:39) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@1945e2154919:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:52:37 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath='descendant-or-self::a' data='<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 139862544625976\r\n```\r\n\r\n\r\n#### Python 3.6, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@43e690443ca7:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.6.4 (default, Dec 21 2017, 01:35:12) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@43e690443ca7:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:54:49 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\nTypeError: can't pickle HtmlElement objects\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3082", "title": "[WIP] Do not serialize unpickable objects (py3)", "body": "This addresses #3054 partially, as it catches the exception raised by `pickle`.\r\nHowever, since this exception is only raised in python >= 3.6, for earlier versions it's still not able to realize it shouldn't serialize some requests.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2956", "title": "Add from_crawler support to dupefilters", "body": "Fixes #2940\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stummjr": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046", "title": "Item loader missing values from base item", "body": "ItemLoaders behave oddly when they get a pre-populated item as an argument and `get_output_value()` gets called for one of the pre-populated fields before calling `load_item()`.\r\n\r\nCheck this out:\r\n\r\n```python\r\n>>> from scrapy.loader import ItemLoader\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': 'http://example.com'}\r\n\r\n# so far, so good... what about now?\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.get_output_value('url')\r\n[]\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': []}\r\n```\r\n\r\nThere are **2** unexpected behaviors in this snippet (at least from my point of view):\r\n\r\n**1)** `loader.get_output_value()` doesn't return the pre-populated values, even though they end up in the final item.\r\n\r\nIt seems to be like this on purpose, though. The `get_output_value()` method only queries the `_local_values` defaultdict ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\n**2)** once we call `loader.get_output_value('url')`, that field is not included in the `load_item()` result anymore.\r\n\r\nThis one doesn't look right, IMHO.\r\n\r\nIt happens because when we call `loader.get_output_value('url')` for the first time, such value is not available on `_local_values`, and so a new entry in the `_local_values` defaultdict will be created with an empty list on it ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L121)). Then, when `loader.load_item()` gets called, [these lines](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L116-L117) overwrite the current value from the internal item because the value returned by `get_output_value()` is `[]` and not `None`.\r\n\r\nAny thoughts on this?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3047", "title": "Avoid missing base item fields in item loaders", "body": "This is an attempt to fix the behavior described in #3046.\r\n\r\nInstead of just checking if the value inside the loader is not None in order to decide if a field from the initial item should be overwritten or not, `load_item()` should also make sure that the value returned by `get_output_value()` is not an empty list.\r\n\r\nThat is because `self._local_values` , which stores the new values included via `add_*` or `replace_*` methods, is a[`defaultdict(list)`](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L37). Then, when we call `get_output_value()` for a field only available in the initial item, an empty list will be set for that field in `self._local_values` (because of [this](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\nThis way, we make sure we don't miss fields from the initial item, in case `get_output_value()` gets called for one of the pre-populated fields before `load_item()`, as described on #3046.", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "exotfboy": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036", "title": "File downloaded by pipeline are blank", "body": "I have deployed a spider in my remote server, and I am using `FilePipeline` to download images for `item`, however I found that the downloaded image have size of  zero, which is not expected.\r\n\r\nThen I test the project in my local machine, it worked. So I think maybe my remote server has been banned, however when I tried `wget ..` I can download the image normally, and I also tried `scrapy sheel image_src` it still work.\r\n\r\nNow I have no idea what's going on , I just want to intercept the response of the request re-sent by the pipeline, is it possible?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ReLLL": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034", "title": "Line-ends (unnecessary blank lines) problem in CSV export on Windows ", "body": "CSV export on Windows create unnecessary blank lines after each line.\r\n\r\nYou can fix the problem just by adding \r\nnewline='' \r\nas parameter to io.TextIOWrapper in the __init__ method of the CsvItemExporter class in scrapy.exporters\r\n\r\nDetails are over here:\r\nhttps://stackoverflow.com/questions/39477662/scrapy-csv-file-has-uniform-empty-rows/43394566#43394566", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3039", "title": "Fix for #3034, CSV export unnecessary blank lines problem on Windows", "body": "Fixed the issue I've mentioned there, this is the pull request to merge, (added one line), hope all is fine. \r\nhttps://github.com/scrapy/scrapy/issues/3034\r\n\r\nCloses #3034 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cp2587": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029", "title": "Invalid DNS pattern", "body": "Hello,\r\n\r\nWe are using https proxies to crawl some website and sometimes i get the following stack trace:\r\n\r\n```\r\nError during info_callback\r\nTraceback (most recent call last):\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 315, in dataReceived\r\n    self._checkHandshakeStatus()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 235, in _checkHandshakeStatus\r\n    self._tlsConnection.do_handshake()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1442, in do_handshake\r\n    result = _lib.SSL_do_handshake(self._ssl)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 933, in wrapper\r\n    callback(Connection._reverse_mapping[ssl], where, return_code)\r\n--- <exception caught here> ---\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1102, in infoCallback\r\n    return wrapped(connection, where, ret)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/scrapy/core/downloader/tls.py\", line 67, in _identityVerifyingInfoCallback\r\n    verifyHostname(connection, self._hostnameASCII)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 44, in verify_hostname\r\n    cert_patterns=extract_ids(connection.get_peer_certificate()),\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 73, in extract_ids\r\n    ids.append(DNSPattern(n.getComponent().asOctets()))\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/_common.py\", line 156, in __init__\r\n    \"Invalid DNS pattern {0!r}.\".format(pattern)\r\nservice_identity.exceptions.CertificateError: Invalid DNS pattern '194.167.13.105'.\r\n```\r\n\r\nI think this issue is somewhat similar to https://github.com/scrapy/scrapy/issues/2092 and this 'invalid DNS pattern' error should be caught similarly as the 'Invalid DNS-ID'. What do you think ?\r\n\r\nIn the meantime, how can i catch it myself and silent it ?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmike": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/7c9e32213db2ce757c784e7c29e30caa57dc3d48", "message": "Merge pull request #3059 from jesuslosada/fix-typo\n\nFix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/786144e0c7b25a02151b6d3135451da90e912719", "message": "Merge pull request #3058 from jesuslosada/fix-link\n\nFix link in news.rst"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/aa83e159c97b441167d0510064204681bbc93f21", "message": "Bump version: 1.4.0 \u2192 1.5.0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d07fe11981a07e493faf7454db79b98c02a53118", "message": "set release date"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c107059ef82a4b7b491b23b740b71353f03ab891", "message": "DOC fix rst syntax"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d4e5671d07a8dcf18b665ed3ce4136dccae222fb", "message": "make release docs more readable, add highlights"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/45b0e1a0e4c51a773b39be14334a999cc5f0fe56", "message": "DOC draft 1.5 release notes"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/930f6ed8002e27c9d51f5d5abbb28c36460eb1bb", "message": "Merge pull request #3050 from lopuhin/pypy3\n\nAdd PyPy3 support"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/632f1cc07305d6967c9e8ce3bf150337e2bf3ecd", "message": "Merge pull request #3049 from scrapy/trove-classifiers\n\n[MRG+1] setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9f9edeadfc9d8d3422d4ca15b2b13d5b502ca70e", "message": "Merge pull request #3048 from lopuhin/pypy-install-docs\n\n[MRG+1] Mention PyPy support, add PyPy to install docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1058169f0e3a8646dbd20f9b4c0b599ed9f6d08e", "message": "setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/bdc12f39949731e69aaa32c36490e1b6e56ca98b", "message": "Merge pull request #3045 from hugovk/rm-3.3\n\n[MRG+1] Drop support for EOL Python 3.3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9aa9dd8d45a2ce0c8e6ae0732e610f020735df7e", "message": "DOC mention an easier way to track pull requests locally.\nThanks @eliasdorneles!"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f716843a66829350063e55f8df768eb538c6b05c", "message": "DOC update \"Contributing\" docs:\n\n* suggest Stack Overflow for Scrapy usage questions;\n* encourage users to submit test-only pull requests with reproducable examples;\n* encourage users to pick up stalled pull requests;\n* we don't use AUTHORS file as a main acknowledgement source;\n* suggest using Sphinx autodocs extension"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/4948548", "body": "whoops, I clicked github's Edit button and thought it will become a PR\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4948548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104443", "body": "For me it is 2014-01-18 :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": []}, "dangra": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/b170e9fb96dc763b9b78916e1557021e5e004d59", "message": "Merge pull request #2609 from otobrglez/extending-s3-files-store\n\nS3FilesStore support for other S3 providers (botocore options)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/2dee191374e7fb7f2ed35ab9eea07ba7d1ee8b89", "message": "Merge branch 'master' into extending-s3-files-store"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9b4d6a40a6d56acbd9e068e15e6717dc06aee79b", "message": "Merge pull request #3053 from scrapy/release-notes-1.5\n\nRelease notes for the upcoming 1.5.0 version"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/57d04aa9601bc237da4b08777327df241483b389", "message": "Merge pull request #2767 from redapple/http-proxy-endpoint-key\n\n[MRG+1] Use HTTP pool and proper endpoint key for ProxyAgent"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/86c322c3a819405020cd884f128246ae55b6eaeb", "message": "Merge pull request #3038 from scrapy/update-contributing-docs\n\nDOC update \"Contributing\" docs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355950", "body": "@pablohoffman: \"a global limit **and** a per-domain limit\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355969", "body": "\"to run **Scrapy** from a script\" right?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355981", "body": "\"if possible, use...\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355981/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379551", "body": "The issue this commit tries to address is real, but the fix introduces a new bug when items contains unicode values not encodeable with default encoding.\n\nIt's a twisted shame that `log.err` doesn't call `_safeFormat` on `_why`  here http://twistedmatrix.com/trac/browser/tags/releases/twisted-12.3.0/twisted/python/log.py#L328\n\nThe good news is that we can call _safeFormat ourself on our `scrapy.log.err` wrapper\n\nwhat do you think?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379551/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379680", "body": "to make it more clear, this is what I am proposing https://gist.github.com/4445963\n\nit has an obvious dislike because of private function import and it doesn't improve on lazy evaluation side\n\nafter all, I am not sure. :sake: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379712", "body": "Now I figured out that previous Gist was an implementation of `format` instead of `_why`\n\nhere is the `_why` version https://gist.github.com/4446084 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379712/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2966628", "body": "I guess you refer to Scrapy 0.16. is it the same case on Scrapy 0.18? \n\nThe motivation was to avoid hiding the real error that makes debugging images error a bit harder.\nCan you provide an test case that reproduces \"image file is truncated\" error and that is valid case to silence instead of propagating and logging the full traceback?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2966628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976147", "body": "hey @a7ch3r, you welcome.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048440", "body": "unused import?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3049106", "body": ":beers:\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3049106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647682", "body": "@redapple: what about gzip,x-gzip,deflate?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647837", "body": "Is it time to move forward and remove `x-gzip` from default `Accept-Encoding`? :)\n\nChrome doesnt send x-gzip anymore:\n\n```\nGET / HTTP/1.1\nHost: example.com\nConnection: keep-alive\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.76 Safari/537.36\nDNT: 1\nAccept-Encoding: gzip,deflate,sdch\nAccept-Language: en-US,en;q=0.8,es-419;q=0.6,es;q=0.4\n```\n\nFirefox neither:\n\n```\nGET / HTTP/1.1\nHost: example.com\nUser-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:24.0) Gecko/20100101 Firefox/24.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nConnection: keep-alive\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4956151", "body": "IDEA: as classes are declared in order, with parents been defined before childs, it's possible to set a class attribute into the first class built from this metaclass.\n\nand I think we can simplify finding the new class name by always pointing to the ~~first~~ second element of the MRO.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4956151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104487", "body": "PYPI says it was released 2014-01-17 :) \n\n![image](https://f.cloud.github.com/assets/37369/1943916/f8783836-7fb1-11e3-9348-0ec972f5b626.png)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104487/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796", "body": "Can we avoid checking for \" xmlns \" in every loop iteration? it is an invariant check. move it to the top of the function.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810", "body": "alternative and less boilerplate: `inputs.extend(formdata.iteritems())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942", "body": "I tried removing the call to `_nons()` and still passed all tests.\nWhat about removing it completely of figuring out a test case that justifies its use? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002", "body": "not testing for string before splitting?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "jesuslosada": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/61c0b1478284b02a4fcfd2cc4931587c348c5d3a", "message": "Fix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a0836b8fd9720a9439cb3b940aca53b6844a094b", "message": "Fix link in news.rst"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "redapple": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/461f9daff5747728e26cd60e9dfe531092f58132", "message": "Update release notes for upcoming 1.4.1 version"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2906", "title": "[WIP] Downloader timings in request.meta", "body": "While working on a requests log plugin for Scrapy, I felt the need to get more fine-grained timing information on the various steps a `Request` goes through from reaching the `Downloader` to getting the associated response body.\r\nOne can work with `request.meta['download_delay']` and timing inserted from a downloader middleware but I found this additional information interesting for example for HAR output.\r\n\r\nNote: This does not address DNS lookup time nor TCP/TLS connection time.\r\n\r\nI would even set this to ON by default myself.\r\n\r\nMissing:\r\n\r\n- [ ] Documentation\r\n- [ ] How does this play with proxies?\r\n- [x] Handle download exceptions (DNS errors, dowload timeouts, connection errors etc.)", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/4617865", "body": "@pablohoffman , what would you think if the order was changed to `gzip,deflate,x-gzip`?\nI've found at least 1 web server that doesn't compress responses when `x-gzip` is first\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4617865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4650042", "body": "yes @dangra , `gzip,x-gzip,deflate` also works for this server\n(as does `gzip,deflate,x-gzip`)\n\nFrom wireshark:\n\n```\nGET / HTTP/1.1\nHost: ...\nAccept-Language: en\nAccept-Encoding: gzip,x-gzip,deflate\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Scrapy/0.21.0 (+http://scrapy.org)\n```\n\n```\nHTTP/1.1 200 OK\nCache-Control: no-cache\nPragma: no-cache\nContent-Type: text/html; charset=utf-8\nContent-Encoding: gzip\nExpires: -1\nVary: Accept-Encoding\nServer: Microsoft-IIS/7.5\nSet-Cookie: ASP.NET_SessionId=xyps42n5jtpprtrormg45q45; path=/; HttpOnly\nX-AspNet-Version: 2.0.50727\nX-Powered-By: ASP.NET\nDate: Wed, 20 Nov 2013 09:47:54 GMT\nContent-Length: 11786\n\n.............`.I.%&/m.{.J.J..t...`.$..@.........iG#).*..eVe]f.@......{....{....;.N'...?\\fd.l..J...!....?~|.?\"~..7N....j.^..t...#|.....<-...4}.~.(}...2[^|.Q..(..Y.|..w^?-.lR.3j..G..w.^..........(.....M......&@.....A@[.L..w~.w.:;...#|A....E.$P..]=.{...j|uo\\..ww.>|x...D.....;...lF?...7N~.>h....^..U....y..M.~./?n.l9K.[-..gY3/.e...,.y..>/......y..x|W.0#,.6#&hW../Z...}4..m.l.A.uv..........c?&.-.E..Go.....5^......7...n..w...^........F..|Y,..)..y=Ji.\nQ._..W...w%.j..+. ..,o.u.j.\n.^L9.E$.....~..R.Y..k..d._1.K.xV..eV\n.j..M.5.`-.+.h..2o...:.......j4/.a.(..2%..u.V.y..n...8.a\\.D...jo...IY..5.'.._dK.'.J...&........F3...{.~`...3}.<.....u[y... @O....V$\"....rc...!]S...h...g.`..8[..7.9i:..u^~...u.N.mZ.f..P].}T,h.w.m.g..?k..g..tL..@..m>..\n.pN.l..UuQ...h...i......(.?.r./?y.-.G.;;.3\".......X......x......{......\n...\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4650042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "raphapassini": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/a1cc5a63d3e253c325159fdc6ebf4cd3faa37c49", "message": "Add mention to dont_merge_cookies in CookiesMiddlewares docs (#2999) (#3030)\n\nAdd mention to dont_merge_cookies in CookiesMiddlewares docs (#2999)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lopuhin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/bb1f31189128cb2272c1302350387075fbbb730a", "message": "Add PyPy3 support to faq and install doc"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/041308afe7c40de7088f75b0e0c312ecd5de428a", "message": "Fix get_func_args test for pypy3\n\nThese built-in functions are exposed as methods in PyPy3.\nFor scrapy this does not matter as:\n1) they do not work for CPython at all\n2) get_func_args is checked for presense of an argument in scrapy,\n   extra \"self\" does not matter.\nBut it still makes sense to leave these tests so that we know we\nshouldn't use get_func_args for built-in functions/methods."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f71df6f9addca10b562bb22890b5ea1c37efde5c", "message": "Run tests for PyPy3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/ea41114cf0ab2782650792ad204cf43fc148c749", "message": "Mention PyPy support, add PyPy to install docs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hugovk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/cbcf80b98ff66db1ccf625fa52c4de8935331972", "message": "Fix typo\n\n[CI skip]"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f11c21c6fc62b64a2bbee0e19e2098ed6257cf19", "message": "Test on Python 3.4"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/44623687ab8936c5696f68f74e438a2891880c82", "message": "Drop support for EOL Python 3.3"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tchiotludo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3072", "title": "Handle Webp Image transparency", "body": "Webp transparent image have a dark background : \r\n\r\nreference image : https://www.lavera.de/typo3temp/fl_realurl_image/105115-4021457609482-glossylips-deliciouspeach09-1294b-43402bf213-6cf9c.png", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BrandonSmithJ": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3062", "title": "Use as a library: Reactor decorator code + MWE", "body": "First, let me say thanks for the extremely useful tool - it really has made a lot of things much simpler than they otherwise would have been. \r\n\r\nWith that in mind, I'd like to give back what I can if it's useful. The problem I'm having is I don't know exactly where to post this. I'm not familiar enough with scrapy internals to suggest where to put the feature, or if it's even necessary enough to put into the main code base. It also likely needs one more modification before being ready for a main branch contribution. \r\n\r\nThe problem this solves is something at least a few people seem to have run into:\r\n\r\n- https://stackoverflow.com/questions/35289054/scrapy-crawl-multiple-times-in-long-running-process\r\n\r\n- https://stackoverflow.com/questions/22116493/run-a-scrapy-spider-in-a-celery-task/22202877#22202877\r\n\r\nMy use case is a website which requires a virtually endless process able to use various scrapy functionalities at whim. To solve the problem of restarting the twisted reactor, I essentially took the solutions I could find and rolled everything into an extremely simple decorator. Basically all the end-user needs to do is label the function they'd like to run as a scrapy function, and the decorator handles everything necessary in order to use scrapy in a library capacity:\r\n\r\n```\r\n\t@reactor_process(timeout=10)\r\n\tdef execute(self, keyword_list):\r\n\t\t''' Crawl the site specifically for certain keywords '''\r\n\t\tfrom crawlers.backend.spider_interface import construct_spider\r\n\t\tfrom scrapy.crawler import CrawlerProcess\r\n\r\n\t\tspider = construct_spider(self)\r\n\t\tspider.create_start_urls(keyword_list)\r\n\t\t\r\n\t\tcrawler  = CrawlerProcess(self.settings)\r\n\t\tdeferred = crawler.crawl(spider)\r\n\t\tdeferred.addBoth(lambda _: crawler._stop_reactor())\r\n```\r\n\r\nWithout the decorator, the function above would only be able to run once - the reactor engine would complain about being restarted. The one problem is that the necessary imports have to be made *within* the function itself - though I think this can be solved with a multiprocessing manager (or worst case, global inspection). \r\n\r\nI also have an interface which allows dynamic creation of contract docstrings / crawlers with different settings inside the same process, which might be useful; but that's something that would probably be better as its own discussion. \r\n  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Matthijsy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3061", "title": "Make autothrottle slow down on HTTP 429 response", "body": "When a response has status 429 (Too Many Requests) it is ignored by the AutoThrottle (because the latency is low, so otherwise the spider will speed up). I think that the wanted behaviour is that the spider will slow down when a 429 is received, but currently the spider will stay at a constant scraping speed. \r\n\r\nThis PR adds a new setting `AUTOTHROTTLE_429_DELAY` when it is set the download delay will be increased with this value every time a 429 is received. It still respects the 'MAX_DOWNLOAD_DELAY` value.  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "parlays": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3051", "title": "The URI gets set again before feed export store gets called.  This al\u2026", "body": "\u2026lows the spider to change settings in the parse method based on the content scraped.  The IFeedStorage interface has been changed for the store method to allow uri to be passed.  The BlockingFeedStorage class now sets the new uri settings in the store method.\r\n\r\nIn my personal scraper I overwrite the FeedExporter object and S3FeedStorage object.  But it would be great if this was part of the core.  It may be a tough change though because it requires a change to the IFeedStorage interface and that is a breaking change for older code.\r\n\r\nIt is common for me to scrape a site and then decide the folder name to use in the S3 bucket.  To be able to change the bucket name or directory in the parse method is important.  If this change is not agreed upon, let's think of another way to accomplish this.  Thanks!", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "munderseth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3041", "title": "Add Test Reporting to Travis CI", "body": "**Test Reporting for Travis using [Testspace.com](https://testspace.com)**\r\n\r\nHi `scrapy` team. We added test reporting to your repo. We have a blog article on [why we are staging repos](https://blog.testspace.com/testspace-and-open-source).\r\n\r\nNote that we contacted you in the past via an email. We thought it would be easier for you (if interested) by using a Pull Request.   \r\n\r\nFew of the benefits using Testspace:\r\n- view all your test results from a single dashboard\r\n- triage and manage your test failures faster\r\n- add more metrics\r\n- leverage built-in analytics \r\n- get a [test badge](https://help.testspace.com/how-to:get-badge) \r\n[![Space Health](https://open.testspace.com/spaces/74470/badge?token=2f48b759d52023674602ea42e90a5508cad6c953)](https://open.testspace.com/spaces/74470?utm_campaign=badge&utm_medium=referral&utm_source=test \"Test Cases\")\r\n\r\nCheckout **your** test results: https://open.testspace.com/projects/TryTestspace:scrapy from our fork.\r\n\r\n**Why** we are doing this: https://blog.testspace.com/testspace-and-open-source \r\n\r\n----\r\n\r\n**Give Testspace a try?**\r\n\r\n1. Create **Open** (free) account: https://testspace.com/pricing\r\n2. Add a **New Project** from the list of Github repo's\r\n3. Add a Travis **Environment Variable**: Name: `TS_ORG`  Value: `organization name` - based on subdomain selected in step 1\r\n4. Invite others: https://help.testspace.com/how-to:invite-other-users\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslay88": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3040", "title": "Fix for Issue #2919", "body": "Splits out url params and updates them with formdata. Passes local tests.\r\n\r\nFirst PR, please be nice :)\r\n\r\nFixes Issue #2919 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mylh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3028", "title": "Update practice.rst docs", "body": "added tips to avoid getting banned", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chainly": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3027", "title": "fix Spider.log to record right caller information", "body": "Spider.log always log itself in `pathname), funcName, lineno, etc.` format. Because `currentframe = lambda: sys._getframe(3)` and `f = f.f_back` in ``logging.Logger.findCaller`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3016", "title": "Added Scrapy logo", "body": "I replaced the \"Scrapy\" text with the logo shown on the Scrapy website", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web crawling.\nI thought that you guys would be doing that in multithreading but I heard that you guys never use any threads.How could this be possible?can anyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128", "body": "@gmeans code gives warning:\r\n\r\n /usr/local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py:51: builtins.UserWarning:\r\n         'broadd.context.CustomContextFactory' does not accept `method` argument (type OpenSSL.SSL method, e.g. OpenSSL.SSL.SSLv23_METHOD). Please upgrade your context factory class to handle it or ignore it.\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454", "body": "@redapple I had errors:\r\n`<twisted.python.failure.Failure OpenSSL.SSL.Error: ('SSL routines', 'SSL3_READ_BYTES', 'sslv3 alert handshake failure'), ('SSL routines', 'SSL3_WRITE_BYTES', 'ssl handshake failure')>` \r\nwith all packages up-to-date", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745", "body": "Scrapy    : 1.4.0\r\nlxml      : 3.8.0.0\r\nlibxml2   : 2.9.4\r\ncssselect : 1.0.1\r\nparsel    : 1.2.0\r\nw3lib     : 1.17.0\r\nTwisted   : 17.5.0\r\nPython    : 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 17:54:52) [MSC v.1900 32 bit (Intel)]\r\npyOpenSSL : 17.1.0 (OpenSSL 1.1.0f  25 May 2017)\r\nPlatform  : Windows-10-10.0.15063-SP0\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674", "body": "@redapple Thank you", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web \ncrawling.\nI thought that you guys would be doing that in multithreading but I \nheard that you guys never use threading.How could this be possible?can \nanyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160", "body": "Thank you, missconfiguration.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683", "body": "https://github.com/kjd/idna/issues/50#issuecomment-312908539", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "isra17": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2996", "title": "Add signals to handle error from downloader or pipeline", "body": "As of right now, there's is no simple way to handle exception coming from items pipeline or a the `process_response` of a downloader middleware.\r\n\r\nThis PR adds two signals for those use case. Note that the downloader signals also catch any exception from the downloader engine, including `process_request` and `process_exception`.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2995", "title": "Allow passing Failure object to middlewares", "body": "Trying to handle errors coming from middleware, it happens that Twisted strip traceback from an exception returned from a completed deferred (Needed to avoid GC issues). This mean that trying to use `exception.__traceback__` always yield `None`.\r\n\r\nThis PR adds a decorator that can be used on middleware `process_exception` or `process_spider_exception` to get a Failure object instead of an Exception. The Failure object provides a `getTracebackObject` to get a `Traceback`-like object which come really handy when trying to pinpoint an issue.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2962", "title": "Add signal `request_downloading` called right before the download handler", "body": "It was needed for one of our project, maybe it can be useful for core as well.\r\n\r\nThe `request_downloading` signal is sent when the engine is about to download a request. If one handler raise an exception, the download is aborted. The signal supports returning deferreds from their handlers.\r\n\r\nThis is an alternative to the Downloader middlewares where there might be a significant delay between the middleware call and the download handler in case of slow queue processing.\r\nThis even handler allow some extension to tamper the request right before the download time and possibly cancel the request by raising an exception.\r\n\r\n`signal.send_deferred` was needed for the exception handling raised by signal handler in the download manager.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "starrify": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2988", "title": "Added: Customizing request fingerprint calculation via request meta", "body": "Partially implements #900", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2984", "title": "Added: Making the list of exceptions to retry configurable via settings", "body": "Implements #2701", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "immerrr": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2986", "title": "WIP: CookiesMiddleware: add \"reset_cookies\" meta to clear the jar", "body": "This PR adds a `reset_cookies` meta to clean the active cookiejar.\r\n\r\nWhen I try working with sessions I often find myself in a situation when I'd like to restart the session \"from scratch,\" including cookies. It's doable by just setting `cookiejar` meta to an arbitrary value, but then we'd be accumulating the cookiejars over time.\r\n\r\nI'm not entirely sure about the meta name, as I'd probably like it namespaced, e.g. `cookies_reset`, but there's a precedent already with `dont_merge_cookies`, so I chose to follow that pattern.\r\n\r\n@kmike am I missing something here?", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2985", "title": "utils.curl: add parse_curl_cmd func", "body": "Given that the major browsers are able to export the requests in cURL format, it's logical that we have a utility in scrapy to make it a request.\r\n\r\nThis PR works towards adding a function that creates kwargs that could be passed to a `Request` constructor.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2950", "title": "loader add_* funcs: pass **kw to self.selector.xpath", "body": "I have found out that kwargs that one specifies for `loader.add_xpath` are not getting to the actual `selector.xpath` invocation, which looks like a bug.\r\n\r\nI wonder if there's code out there that would be broken by this fix...", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aitoehigie": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2972", "title": "Add a note to allowed_domains", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NoExitTV": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2955", "title": "Handle \"invalid\" relative URL issue #1304", "body": "Did some minor tweaks on how scrapy handle relative URL's as discussed in #1304 \r\n\r\nTested it with some basic code in the scrapy shell:\r\n```\r\n>>> resp = scrapy.http.response.html.HtmlResponse('http://www.example.com',\r\n...      body='''<html>\r\n...                  <body>\r\n...                      <a href=\"../index1.html\">Link1</a>\r\n...                      <a href=\"../../index2.html\">Link2</a>\r\n...                      <a href=\"index3.html\">Link3</a>\r\n...                      <a href=\"other_html/../index4.html\">Link4</a>\r\n...                      <a href=\"other_html/folder2/../index5.html\">Link5</a>\r\n...                      <a href=\"other_html/index6.html\">Link6</a>\r\n...                      <a href=\"../other_html/index7.html\">Link7</a>\r\n...                 </body>\r\n...          </html>''')\r\n>>> links = scrapy.linkextractors.LinkExtractor().extract_links(resp)\r\n>>> links\r\n[Link(url='http://www.example.com/index1.html', text='Link1', fragment='', nofollow=False), Link(url='http://www.example.com/index2.html', text='Link2', fragment='', nofollow=False), Link(url='http://www.example.com/index3.html', text='Link3', fragment='', nofollow=False), Link(url='http://www.example.com/index4.html', text='Link4', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index5.html', text='Link5', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index6.html', text='Link6', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index7.html', text='Link7', fragment='', nofollow=False)]\r\n```\r\n\r\nPassed the same unit tests as the original code when running tox\r\n```\r\n========================================================= 1679 passed, 6 skipped, 14 xfailed in 458.89 seconds =========================================================\r\n\r\n_______________________________________________________________________________ summary ________________________________________________________________________________\r\n\r\n  py27: commands succeeded\r\n  congratulations :)\r\n```\r\n\r\nI believe that scrapy now handle relative url's as expected in python 2.7.13\r\nWhat are your thoughts?", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kaplun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2954", "title": "spiders: add OAI-PMH support WIP", "body": "Signed-off-by: Samuele Kaplun <samuele.kaplun@cern.ch>", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phnk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2953", "title": "Added debug message in spiders/crawl", "body": "As requested in #2925.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mGalarnyk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2931", "title": "Update install.rst", "body": "Updated installation instructions for installing scrapy using conda.\r\n\r\nYou can now just do: \r\n\r\n```\r\nconda install scrapy\r\n```", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "HarrisonGregg": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2918", "title": "Fix SIGINT handling when using inspect_response", "body": "Currently, after calling `scrapy.shell.inspect_response` and then closing the opened shell, SIGINT (Ctrl-C) no longer works to terminate the spider.  This is because `Shell.start`, called in `inspect_response`, removes the signal handler.  To fix this, save the SIGINT handler before calling `Shell.start` in `inspect_response` and add it again after the shell has closed.\r\n\r\nIt doesn't appear that there are any tests for `inspect_response`, so I haven't added any tests for this fix.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangtua1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2917", "title": "support giantfiles download ,fix CVE-2017-14158", "body": "from the issue https://github.com/scrapy/scrapy/issues/482.\r\nWe can see if we download a file with filespipeline, the whole file is stored into the memory.\r\nIf the file is a giant file or an extremely giant file,(over 1G or more), the crawling thread will be crashed.\r\nWe can use this POC to prove it.\r\n[POC.zip](https://github.com/scrapy/scrapy/files/1289514/POC.zip)\r\n\r\nThis has been asigned CVE-2017-14158.\r\nbut in this new testcase using GiantFilesPipeline in the commit,we can download it correctly.\r\n\r\n[testdownload.zip](https://github.com/scrapy/scrapy/files/1289512/testdownload.zip)\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhaojiedi1992": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2911", "title": "Update exporters.rst", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfdeshom": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118", "body": "Hi, to disable `scrapy.contrib.feedexport.FeedExporter`, I had to set it to `None` in my `EXTENSIONS` dict instead of `0`. I am getting this error when using scrapyd.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185", "body": "off-topic but: in general to disable any extension, middleware, etc I have to set it to `None`, which can be a little confusing since you would think that `0` would work too\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375", "body": "@dangra unfortunately, I don't see a documented way to disable an extension and I consider myself pretty familiar with scrapy and its docs. Maybe it's already there in the docs, but simply needs to be more prominent.\n\nFor me there is a larger issue: some extensions have settings associated with them that make it confusing to know exactly how/where to disable them. For example, the cookies extension seems to have 2 ways to disable it:\n-  set `COOKIES_ENABLED` to `False` (https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/cookies.py)\n- set the priority to `None`, ie set `scrapy.contrib.downloadermiddleware.CookiesMiddleware` to `None`\n\nIt would be nice to have just one way to disable extensions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "NicolasP": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831", "body": "Hi, any chance for a review of this PR?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "knaveofdiamonds": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951", "body": "@kmike - ah, ok. I'm working on an inherited codebase that doesn't use the `LinkExtractors` directly, so this was obviously just a misunderstanding of the API/using something non-public. I'll close this issue - agree with backwards incompatible change reasons.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nside": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706", "body": "Seeing the same issue\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703", "body": "@dangra crawl() fixed it for me. Still I'd expect any requests scheduled to have the same treatment, whatever their \"entry point\" in the pipeline is. Feel free to close if you disagree.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941", "body": "I'm on 0.21 (dev). These are good subtleties to know!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606", "body": "Hi,\n\nI've copied a URL below which failed with the original function because it has no url argument and the arguments are in an unexpected order. I can certainly add some unit tests, I'll have a look into it now. It's the end of the day and I want some food!\n\nCheers,\nRob\n\nhttp://www.firstchoice.co.uk/fcsun/page/search/searchresults?sttrkr=mthyr:02/2011_durT:7/n_ls:true_tuidesc:000832_day:15_mps:9_isvid:false_pconfig:1|2|0|0|0|_tchd:0_rating:0_act:0_jsen:true_resc:_attrstr:||||||||null|null_mdest:false_tinf:0_mnth:02_desc:_bc:17_margindt:7_tadt:2_numr:1_spp:mainSearch_depm:7_tuiresc:004287_dur:7_dtx:0_df:false_dxsel:0_imgsel:0_dac:MAN_loct:0_tsnr:0_year:2011_dta:false_tuiacc:028367_acc:\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615", "body": "Which distro are you using? Here in ubuntu, works fine. Try:\n\napt-get install -y build-essential python-dev\napt-get install -y libssl-dev libxml2-dev libxslt1-dev libssl-dev libffi-dev\n\nthen pip install scrapy in a brand new virtualenv\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982", "body": "Let me try again:\n\nI want to use scrapy like this:\n\n```\n# prints options\nscrapy crawl -t csv|sql|mongo|etc -h\n\n# uses a particular item exporter\nscrapy crawl -t csv -f somefile.csv\nscrapy crawl -t mongo --db somedb --col somecollection\n```\n\nWhat do you guys think of this? Is it desired behavior? Does not feel like a whole lot of code to modify. I could do it. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ariddell": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661", "body": "@nramirezuy there's a reference implementation for pep 3156 here: https://code.google.com/p/tulip/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sabren": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629", "body": "Here you go: \n\nhttps://github.com/scrapy/scrapy/pull/45\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490", "body": "Thanks, Scotty! \n\nI'm sure my client would appreciate it.\n\nCan you make a combined pull request, or should I pull from you and open another pull request here?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "mvj3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848", "body": "Thanks @curita for the careful review! I correct it in a new commit.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "RFDAJE": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937", "body": "I had been facing the same issue, so far the simplest solution I found is inside `item_completed`, after getting all things done, reset downloaded to empty. `self.spiderinfo.downloaded = {}`, memory leak issue resolved. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "paulproteus": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573", "body": "I did some work refreshing these patches against current origin/master.\n\nhttps://github.com/paulproteus/scrapy/tree/revise-pullrequest-109\n\nSome notes:\n\nSee 3077ac8b6f592b044ad67f15af3b065d06f27cf7 for a fix where Http11DownloadHandler needs to accept second argument, since that's how the tests use it.\n\nSee 888dfadd24f7d325bffab367acc9dd5a7405e5bc for a fix where the call to log.err() creates an exception in the log that test runner notices, so tests that call this method begin to fail. For now I've disabled the call to log.err(). If we want to keep logging the error, we'll need to call flushLoggedErrors() -- see http://twistedmatrix.com/documents/current/core/howto/trial.html#auto11 . (The test this makes fail is scrapy.tests.test_downloader_handlers.Http11TestCase.test_timeout_download_from_spider )\n\nCommit bdd2a1b02c944845fec4d6142fcb63367b17cc11 (rebased from the original, but otherwise the same) makes many tests fail because there is work left in the reactor. One test you can see this with is scrapy.tests.test_engine.EngineTest.test_crawler\n\nFor now I can't promise I'll have time to figure out what's going on with leaving the reactor unclean, but I thought I'd leave a comment with what I have found in a few hours of looking into this pull request!\n\n-- Asheesh.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rmax": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800", "body": "#62 is a duplicated report of this issue but includes a pull request with a fix.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083", "body": "See issue #58\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355", "body": "As a workaround you can use `process_value` argument:\n\n`SgmlLinkExtractor(..., process_value=lambda v: v.strip())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580", "body": "I have reverted the redirect middleware order and added tests for gzipped meta redirection. Although I'm not sure about leaving the body compressed if it fails, I think there could be more errors (ie. connection drop) that could make fail the decompression and leaving the body as is could produce misbehavior in other components.\n\nAdding more tests for the middlewares integration could help us to identify the best solution.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823", "body": "without any fix fails test_gzipped_redirect_30x\nwith the redirect reorder fails test_gzipped_meta_redirect\nwith dangra's suggestion passes all tests (See https://gist.github.com/1718659)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882", "body": "Nice!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "djm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/426681", "body": "@pablohoffman Cheers!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/426681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aalvarado": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/749296", "body": "```\nfound in the dmoz directory\n```\n\nI think this wasn't updated. Should say `tutorial` now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/749296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "mohsinhijazee": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2180902", "body": "The settings have FEED_URI whereas here it is picking up from SCRAPY_FEED_URI. I think this is kind of conflicting  here.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2180902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "pablohoffman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2184054", "body": "When you access `settings['FEED_URI']` Scrapy ends up looking at the `SCRAPY_FEED_URI` environment variable.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2184054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379561", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701990", "body": "Indeed, shorter and faster. Change pushed, thanks!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701990/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863", "body": "Is there a way to avoid calling a protected method of lxml.html?. As this may raise some compatibility issues on future/past lxml versions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702", "body": "touple -> tuple\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727", "body": "should we make these settings dicts (like extension and middlewares) and use `scrapy.utils.conf.build_component_list` to load them?.\n\nany particular reason why you went with lists instead?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737", "body": "I think this should inherit from `AssertionFailure`, to appear as test failures (and not errors) in some testing frameworks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755", "body": "I think I would prefer two contracts for these:\n\n```\n@minitems 1\n@minrequests 2\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772", "body": "Why do we need this method for?. I don't see it used, and we already have `adjust_request_args`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784", "body": "what about just ignoring those who don't?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786", "body": "perhaps in the future we can add a way to list contracts per spider\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791", "body": "I'm not sure a separte register method is needed. How about the constructor receiving the contracts? Like middlewares and extension do. See `scrapy.middleware` module.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796", "body": "these changes should go into a separate PR\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460", "body": "I think we should reuse the convention of using dicts. (we were planning to port pipelines too).\n\nIt's true that priorities won't add much in in this case, but it's not only priorities what the dict mechanism provides, but also being to disable specific components of the (presumably most sensible default). Priorioties wont' add much to this (as they don't to extensions) but don't harm and it could be useful if someone ever needs them. We should make sure internal code in an intuitive behaviour. Like pre_process being called in priority-order.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558", "body": "I wasn't considering all cases. In that case, how about this:\n\n```\n@returns <type> [M [N]]\n```\n\nHere are some examples to illustrate:\n- `@returns request` - returns at least one request\n- `@returns request 2` - returns at least two requests\n- `@returns request 2 10` - returns at least two requests and no more than 10 requests\n- `@returns request 0 10` - returns no more than 10 requests\n\nSame goes for items.\n\nMaybe make aliases (`request` -> `requests` , `item` -> `items`) so that both plurals and singulars work.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903", "body": "I can't see that comment on github now, did you remove it?\n\nOn Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi\nnotifications@github.comwrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> \n> @returns requests 2 - returns at least two requests\n> \n> -- wouldn't this be considered unexpected behaviour?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/167/files#r1570864.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908", "body": "this one looks very similar to `scrapy.utils.get_func_args` (which is tested and supports methods, function, classes, etc) - could that one be reused?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916", "body": "`from scrapy.conf import settings` is deprecated  API, use `self.settings` within the command methods,  that attribute is assigned by the `scrapy.cmdline` mechanics.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "nuklea": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701979", "body": "`isinstance(arg, (dict, BaseItem))` short.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155", "body": "What about pep8?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brunsgaard": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2853556", "body": "Ahh yeah.. good point. I will write up another commit in the near future taking the settings instance from the crawler. Had totally forgotten about overrides :/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2853556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "archerhu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2965291", "body": "can you explain why you remove the try catch?\n\nat 1.6 version, using ImagePipeline may raise a lot \"exceptions.IOError: image file is truncated\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2965291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976024", "body": "I get your\u00a0motivation,thanks very much\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "nramirezuy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048968", "body": "ssh! :dancer: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104492", "body": "It is 2014-01-17 on UTC :P\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "scottyallen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154", "body": "Thanks for the patch - I was in the process of trying to fix this, and it saved me a ton of time:)  However, I don't think line 191 is quite right for the tunnel case.  It results in sending a GET request with the full url to the destination webserver, which is technically wrong and some sites refuse to handle.  Instead, self.path should remain unchanged for the tunnel case.  I can send a patch to your patch, if you like...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "alexcepoi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975", "body": "what about maxitems, maxrequests? Or the case where you expect to receive exactly one request (the original sep describes a returns_request contract which checks this).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004", "body": "Ignoring sounded unfriendly to me at first (i.e. you may wonder why it does not work). Also eliminating the assertion gives a very misleading error.\nListing contracts sounds good to me maybe a \"--list / -l\" option?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025", "body": "This is a reminiscence from the original implementation. I thought it could prove useful, but now that I think about it's hard to find a scenario in which adjust_request_args is insufficient.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066", "body": "I did not think priorities would be useful, or that we should encourage users to rely on contracts priorities.\nEspecially since for some hooks (pre_process comes into mind) the last contract hooked in is the first one to be executed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090", "body": "note: also change docstring\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864", "body": "`@returns requests 2` - returns at least two requests\n\n-- wouldn't this be considered unexpected behaviour?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929", "body": "It's an outdated diff (i.e. comment on a line of code which has been modified). You should still be able to find it in the issue\n\nOn Sep 10, 2012, at 11:30 PM, Pablo Hoffman notifications@github.com wrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> >   I can't see that comment on github now, did you remove it? On Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi notifications@github.comwrote:\n> >   \u2026\n> >   \u2014\n> >   Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}, "3": {"FredEnglish": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3079", "title": "Connection Lost in a non-clean fashion for some URLs on a particular domain, but not others", "body": "I have created a basic spider to scrape a small group of job listings from totaljobs.com. I have set up the spider with a single start URL, to bring up the list of jobs I am interested in. From there, I launch a separate request for each page of the results. Within each of these requests, I launch a separate request calling back to a different parse method, to handle the individual job URLs.\r\n\r\nWhat I'm finding is that the start URL and all of the results page requests are handled fine - scrapy connects to the site and returns the page content. However, when it attempts to follow the URLs for each individual job page, scrapy isn't able to form a connection. Within my log file, it states:\r\n\r\n`[<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]`\r\n\r\nI'm afraid that I don't have a huge amount of programming experience or knowledge of internet protocols etc. so please forgive me for not being able to provide more information on what might be going on here. I have tried changing the TLS connection type; updating to the latest version of scrapy, twisted and OpenSSL; rolling back to previous versions of scrapy, twisted and OpenSSL; rolling back the cryptography version, creating a custom Context Factory and trying various browser agents and proxies. I get the same outcome every time: whenever the URL relates to a specific job page, scrapy cannot connect and I get the above log file output.\r\n\r\nIt may be likely that I am overlooking something very obvious to seasoned scrapers, that is preventing me from connecting with scrapy. I have tried following some of the the advice in these threads:\r\n\r\n[https://github.com/scrapy/scrapy/issues/1429](https://github.com/scrapy/scrapy/issues/1429)\r\n[https://github.com/requests/requests/issues/4458](https://github.com/requests/requests/issues/4458)\r\n[https://github.com/scrapy/scrapy/issues/2717](https://github.com/scrapy/scrapy/issues/2717)\r\n\r\nHowever, some of it is a bit over my head e.g. how to update cipher lists etc. I presume that it is some kind of certification issue, but then again scrapy is able to connect to other URLs on that domain, so I don't know.\r\n\r\nThe code that I've been using to test this is very basic, but here it is anyway:\r\n\r\n```\r\nimport scrapy\r\n\r\nclass Test(scrapy.Spider):\r\n\t\r\n\r\n\tstart_urls = [\r\n\t\t\t\t\t'https://www.totaljobs.com/job/welder/jark-wakefield-job79229824'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/elliott-wragg-ltd-job78969310'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/exo-technical-job79019672'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/exo-technical-job79074694'\r\n\t\t\t\t\t\t]\r\n\t\r\n\tname = \"test\"\r\n\r\n\tdef parse(self, response):\r\n\t\tprint 'aaaa'\r\n                yield {'a': 1}\r\n```\r\n\r\nThe URLs in the above code **are not** being connected to successfully.\r\n\r\nThe URLs in the below code **are** being connected to successfully.\r\n\r\n```\r\nimport scrapy\r\n\r\nclass Test(scrapy.Spider):\r\n\t\r\n\r\n\tstart_urls = [\r\n\t\t\t\t\t'https://www.totaljobs.com/jobs/permanent/welder/in-uk'\r\n\t\t\t\t\t,'https://www.totaljobs.com/jobs/permanent/mig-welder/in-uk'\r\n\t\t\t\t\t,'https://www.totaljobs.com/jobs/permanent/tig-welder/in-uk'\r\n\t\t\t\t\t\t]\r\n\t\r\n\tname = \"test\"\r\n\r\n\tdef parse(self, response):\r\n\t\tprint 'aaaa'\r\n                yield {'a': 1}\r\n```\r\n\r\n\r\nIt'd be great if someone could replicate this behavior (or not as the case may be) and let me know. Please let me know if I should submit additional details. I apologise, if I have overlooked something really obvious. I am using:\r\n\r\nWindows 7 64 bit\r\nPython 2.7\r\nscrapy version 1.5.0\r\ntwisted version 17.9.0\r\nopenSSL version 17.5.0\r\nlxml version 4.1.1", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shanmuga-cv": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077", "title": "scrapy selector fails when large lines are present response", "body": "Originally encoutered when scraping [Amazon restaurant](https://www.amazon.com/restaurants/zzzuszimbos0015gammaloc1name-new-york/d/B01HH7CS44?ref_=amzrst_pnr_cp_b_B01HH7CS44_438).  \r\nThis page contains multiple script tag with lines greater then 64,000 character in one line. \r\nThe selector (xpath and css) does not search beyond these lines. \r\n\r\nDue to this the following xpath `'//h1[contains(@class, \"hw-dp-restaurant-name\")]/text()'` to extract name of the restaurant returns empty even though there is a matching tag is present.\r\n\r\n\r\nPFA the response text at [original_response.html.txt.gz](https://github.com/scrapy/scrapy/files/1631425/original_response.html.txt.gz)\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "crisfan": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075", "title": "Telnet Console ", "body": "hi, @kmike,I use telnetlib to pause scrapy engine, there is some error! although it pause the scrapy engine.\r\nthis is my poor code:\r\n``` javascript\r\nimport telnetlib\r\nhost = '127.0.0.1'\r\ntn = telnetlib.Telnet(host, 6023)\r\ntn.write(\"engine.pause()\\n\") \r\n``` \r\nerror:\r\n``` javascript\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n``` \r\n\r\ncould you fix my problem, thansk ~~~\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Caleb-Wade": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074", "title": " Cxfreeze+python3.4+scrapy1.4 failed to bundle to executables(AttributeError: module object has no attribute '_fix_up_module')", "body": "After I failed to bundle .py document into .exe document by pyinstaller, I tried cxfreeze. Similar error happened. Someting went wrong when importing scrapy module, and AttributeError: module object has no attribute '_fix_up_module' appeared in the command window. Would someone tell me what happened. Can I make it? Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073", "title": "Pyinstaller+Python3.4+Scrapy1.4 Failed to bundled exe (ImportError:No module named \"XXXX\")", "body": "I have tried to bundle my spider .py into .exe, but weeks pasted, I didn't make it. It pointed that some modules were missing. Even I used --hidden-import to put the missing modules into my bundled exe, it still didn't work. It's amazing that I can see the modules in the HTML document under build file. Pls help me ,Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lifei1245": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069", "title": "about the signal retry_complete", "body": "I didn't find the singnal in the singnal list,how can I use it", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "3xp10it": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068", "title": "scrapy always Starting new HTTP connection after crawl finished ", "body": "I reopen [#3066][1] here,there are more details [here][2].\r\n\r\n[1]: https://github.com/scrapy/scrapy/issues/3066\r\n[2]: https://stackoverflow.com/questions/48182204/scrapy-always-starting-new-http-connection-after-crawl", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "theduman": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067", "title": "scrapy.Request doesnt wait for loop", "body": "I fetch url and other parameters from database and use them in for loop and pass to scrapy.request() function but when i run the code i succesfully pass first element only. Scrapy can't get other elements of list. Here is my code\r\n\r\n```py\r\nclass QuotesSpider(scrapy.Spider):\r\n    name = \"quotes\"\r\n    custom_settings = {\r\n        'CONCURRENT_REQUESTS': 1,\r\n    }\r\n    def start_requests(self):\r\n        sourceArr = []\r\n        connection = db.connect()\r\n        try:\r\n            with connection.cursor() as cursor:\r\n                # Read a single record\r\n                sql = \"SELECT `code`, `url`, `target`,`next` FROM `source`\"\r\n                cursor.execute(sql)\r\n                result = cursor.fetchall()\r\n        finally:\r\n            connection.close()\r\n        print(result)\r\n        for i in result:\r\n            source = Source(i['code'], i['url'], i['target'], i['next'])\r\n            sourceArr.append(source)\r\n        print(sourceArr)\r\n        #for url in urls:\r\n           #yield scrapy.Request(url=url, callback=self.parse)\r\n        for s in sourceArr:\r\n            print(s.target)\r\n            yield scrapy.Request(url=s.url, meta={'target': s.target, 'next': s.next})\r\n            print(\"slept\")\r\n\r\n\r\n    def parse(self, response):\r\n        titlearr = []\r\n        count = 0\r\n        print(response.meta)\r\n        for title in response.css(response.meta['target']):\r\n            count += 1\r\n            titlearr.append(title.css('p a::text').extract_first())\r\n            #yield {'title': title.css('p a::text').extract_first()}\r\n        print(\"total count \" + str(count))\r\n        print(titlearr)\r\n        for next_page in response.css(response.meta['next']):\r\n            yield response.follow(next_page, self.parse)\r\n```\r\n\r\nWhen i print response.meta i got target and next values for only first item. List contains more than 1 item. How can i make scrapy.Request() function wait for the next element to run?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "benjolitz": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065", "title": "`Connection to the other side was lost` for older French site", "body": "Hi,\r\n\r\nI'm attempting to run scrapy on `https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`.\r\n\r\nDespite upgrading all my brew packages, scrapy installation via `pip install -U scrapy`, the website disconnects uncleanly. Specifying `-s DOWNLOADER_CLIENT_TLS_METHOD=TLSv1.0` makes no difference to the following run. Neither does a user-agent clear things up with `-s USER_AGENT=\"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\"`\r\n\r\nI am uncertain as how to debug this further.\r\n\r\n`scrapy shell https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`\r\n\r\n```\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Darwin-16.7.0-x86_64-i386-64bit\r\n2018-01-09 18:03:33 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOADER_CLIENT_TLS_METHOD': 'TLSv1.0', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2018-01-09 18:03:33 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2018-01-09 18:03:33 [scrapy.core.engine] INFO: Spider opened\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\nTraceback (most recent call last):\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/commands/shell.py\", line 73, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 48, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 115, in fetch\r\n    reactor, self._schedule, request, spider)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/python/failure.py\", line 385, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n```\r\n\r\n\r\n`scrapy version -v`:\r\n```\r\nScrapy       : 1.5.0\r\nlxml         : 4.1.1.0\r\nlibxml2      : 2.9.7\r\ncssselect    : 1.0.3\r\nparsel       : 1.3.1\r\nw3lib        : 1.18.0\r\nTwisted      : 17.9.0\r\nPython       : 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)]\r\npyOpenSSL    : 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017)\r\ncryptography : 2.1.4\r\nPlatform     : Darwin-16.7.0-x86_64-i386-64bit\r\n```\r\n\r\n`curl -sLv https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences 2>&1 | head -20`:\r\n```\r\n*   Trying 160.92.134.3...\r\n* TCP_NODELAY set\r\n* Connected to agences.creditfoncier.fr (160.92.134.3) port 443 (#0)\r\n* TLS 1.0 connection using TLS_RSA_WITH_3DES_EDE_CBC_SHA\r\n* Server certificate: agences.creditfoncier.fr\r\n* Server certificate: RapidSSL RSA CA 2018\r\n* Server certificate: DigiCert Global Root CA\r\n> GET /credit-immobilier/toutes-nos-agences HTTP/1.1\r\n> Host: agences.creditfoncier.fr\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 200 OK\r\n< Date: Wed, 10 Jan 2018 02:06:54 GMT\r\n< Server: Microsoft-IIS/6.0\r\n< X-Powered-By: ASP.NET\r\n< Content-Length: 35884\r\n< Content-Type: text/html\r\n< Set-Cookie: ASPSESSIONIDQQBACDTS=JHOJNNCCJNIFABOPLNCEEAFH; path=/\r\n< Cache-control: private\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "reaCodes": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064", "title": "I ran into a problem when I used ipython in a scrapy shell", "body": "I used this command, `scrapy shell 'www.baidu.com'`, and then used `response.body`. \r\n\r\nWhen I input `resp` and click `Tab`\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660511-e159fe1a-f47d-11e7-8499-b6589ef7de3f.png)\r\n\r\nAs you can see, there was a display error\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660548-16f0b794-f47e-11e7-996b-9862f788a771.png)\r\n\r\nUse the command completion function is encountered a display error", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmagonski": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060", "title": "HTTPERROR_ALLOWED_CODES can't parse response with 301", "body": "In scrapy.downloadermiddlewares.redirect.RedirectMiddleware#process_response \r\ndosn't get anything from settings like HTTPERROR_ALLOWED_CODES only in HttpErrorMiddleware we have \r\nhandle_httpstatus_list.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NewUserHa": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057", "title": "configure_logging unable to handle GBK", "body": "I added \r\n`import logging\r\nfrom scrapy.utils.log import configure_logging\r\n\r\nconfigure_logging(install_root_handler=False)\r\nlogging.basicConfig(\r\n    filename='log.txt',\r\n    format='%(levelname)s: %(message)s',\r\n    level=logging.INFO\r\n)`\r\nfrom scrapy documents blow my class define.\r\n\r\nthen:\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\logging\\__init__.py\", line 982, in emit\r\n    stream.write(msg)\r\nUnicodeEncodeError: 'gbk' codec can't encode character '\\ufe0f' in position 190: illegal multibyte sequence\r\nCall stack:\r\n....\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\core\\scraper.py\", line 237, in _itemproc_finished\r\n    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\r\nMessage: 'Scraped from %(src)s\\r\\n%(item)s'\r\nArguments: {'src': <200 http://...>, 'item': {'date': '12-02', 'floor': '...\\n    \u7535\\ufe0f', 'pics': ['...', ...]}}\r\n\r\nI also have 'LOG_LEVEL': 'INFO' in the spider.\r\n\r\nI googled and have no idea how to fix this.\r\nany help, please?\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056", "title": "[suggest] add a option to pipeline for some sites requiring reference in heads", "body": "it's usual case and it's ugly to override get_media_requests method of pipelines.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055", "title": "[bug] pillow will always recode images in imagepipieline", "body": "https://github.com/scrapy/scrapy/blob/aa83e159c97b441167d0510064204681bbc93f21/scrapy/pipelines/images.py#L151\r\n\r\nthis line will always recode images silently and damage the image quality.\r\n\r\nplease add an option to avoid this.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elacuesta": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054", "title": "Request serialization should fail for non-picklable objects", "body": "The Pickle-based disk queues silently serialize requests that shouldn't be serialized in Python<=3.5. I found this problem when dumping a request with an `ItemLoader` object in its `meta` dict. Python 3.6 fails in [this line](https://github.com/scrapy/scrapy/blob/1.4/scrapy/squeues.py#L27) with `TypeError: can't pickle HtmlElement objects`, because the loader contains a `Selector`, which in turns contains an `HtmlElement` object.\r\n\r\nI tested this using the https://github.com/scrapinghub/scrapinghub-stack-scrapy repository, and found that `pickle.loads(pickle.dumps(selector))` doesn't fail, but generates a broken object.\r\n\r\n#### Python 2.7, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3)\r\n```\r\nroot@04bfc6cf84cd:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 2.7.14 (default, Dec 12 2017, 16:55:09) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@04bfc6cf84cd:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:49:27 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath=u'descendant-or-self::a' data=u'<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 140144569743064\r\n```\r\n\r\n\r\n#### Python 3.5, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@1945e2154919:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.5.4 (default, Dec 12 2017, 16:43:39) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@1945e2154919:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:52:37 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath='descendant-or-self::a' data='<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 139862544625976\r\n```\r\n\r\n\r\n#### Python 3.6, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@43e690443ca7:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.6.4 (default, Dec 21 2017, 01:35:12) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@43e690443ca7:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:54:49 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\nTypeError: can't pickle HtmlElement objects\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3082", "title": "[WIP] Do not serialize unpickable objects (py3)", "body": "This addresses #3054 partially, as it catches the exception raised by `pickle`.\r\nHowever, since this exception is only raised in python >= 3.6, for earlier versions it's still not able to realize it shouldn't serialize some requests.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2956", "title": "Add from_crawler support to dupefilters", "body": "Fixes #2940\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stummjr": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046", "title": "Item loader missing values from base item", "body": "ItemLoaders behave oddly when they get a pre-populated item as an argument and `get_output_value()` gets called for one of the pre-populated fields before calling `load_item()`.\r\n\r\nCheck this out:\r\n\r\n```python\r\n>>> from scrapy.loader import ItemLoader\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': 'http://example.com'}\r\n\r\n# so far, so good... what about now?\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.get_output_value('url')\r\n[]\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': []}\r\n```\r\n\r\nThere are **2** unexpected behaviors in this snippet (at least from my point of view):\r\n\r\n**1)** `loader.get_output_value()` doesn't return the pre-populated values, even though they end up in the final item.\r\n\r\nIt seems to be like this on purpose, though. The `get_output_value()` method only queries the `_local_values` defaultdict ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\n**2)** once we call `loader.get_output_value('url')`, that field is not included in the `load_item()` result anymore.\r\n\r\nThis one doesn't look right, IMHO.\r\n\r\nIt happens because when we call `loader.get_output_value('url')` for the first time, such value is not available on `_local_values`, and so a new entry in the `_local_values` defaultdict will be created with an empty list on it ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L121)). Then, when `loader.load_item()` gets called, [these lines](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L116-L117) overwrite the current value from the internal item because the value returned by `get_output_value()` is `[]` and not `None`.\r\n\r\nAny thoughts on this?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3047", "title": "Avoid missing base item fields in item loaders", "body": "This is an attempt to fix the behavior described in #3046.\r\n\r\nInstead of just checking if the value inside the loader is not None in order to decide if a field from the initial item should be overwritten or not, `load_item()` should also make sure that the value returned by `get_output_value()` is not an empty list.\r\n\r\nThat is because `self._local_values` , which stores the new values included via `add_*` or `replace_*` methods, is a[`defaultdict(list)`](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L37). Then, when we call `get_output_value()` for a field only available in the initial item, an empty list will be set for that field in `self._local_values` (because of [this](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\nThis way, we make sure we don't miss fields from the initial item, in case `get_output_value()` gets called for one of the pre-populated fields before `load_item()`, as described on #3046.", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "exotfboy": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036", "title": "File downloaded by pipeline are blank", "body": "I have deployed a spider in my remote server, and I am using `FilePipeline` to download images for `item`, however I found that the downloaded image have size of  zero, which is not expected.\r\n\r\nThen I test the project in my local machine, it worked. So I think maybe my remote server has been banned, however when I tried `wget ..` I can download the image normally, and I also tried `scrapy sheel image_src` it still work.\r\n\r\nNow I have no idea what's going on , I just want to intercept the response of the request re-sent by the pipeline, is it possible?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ReLLL": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034", "title": "Line-ends (unnecessary blank lines) problem in CSV export on Windows ", "body": "CSV export on Windows create unnecessary blank lines after each line.\r\n\r\nYou can fix the problem just by adding \r\nnewline='' \r\nas parameter to io.TextIOWrapper in the __init__ method of the CsvItemExporter class in scrapy.exporters\r\n\r\nDetails are over here:\r\nhttps://stackoverflow.com/questions/39477662/scrapy-csv-file-has-uniform-empty-rows/43394566#43394566", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3039", "title": "Fix for #3034, CSV export unnecessary blank lines problem on Windows", "body": "Fixed the issue I've mentioned there, this is the pull request to merge, (added one line), hope all is fine. \r\nhttps://github.com/scrapy/scrapy/issues/3034\r\n\r\nCloses #3034 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cp2587": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029", "title": "Invalid DNS pattern", "body": "Hello,\r\n\r\nWe are using https proxies to crawl some website and sometimes i get the following stack trace:\r\n\r\n```\r\nError during info_callback\r\nTraceback (most recent call last):\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 315, in dataReceived\r\n    self._checkHandshakeStatus()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 235, in _checkHandshakeStatus\r\n    self._tlsConnection.do_handshake()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1442, in do_handshake\r\n    result = _lib.SSL_do_handshake(self._ssl)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 933, in wrapper\r\n    callback(Connection._reverse_mapping[ssl], where, return_code)\r\n--- <exception caught here> ---\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1102, in infoCallback\r\n    return wrapped(connection, where, ret)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/scrapy/core/downloader/tls.py\", line 67, in _identityVerifyingInfoCallback\r\n    verifyHostname(connection, self._hostnameASCII)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 44, in verify_hostname\r\n    cert_patterns=extract_ids(connection.get_peer_certificate()),\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 73, in extract_ids\r\n    ids.append(DNSPattern(n.getComponent().asOctets()))\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/_common.py\", line 156, in __init__\r\n    \"Invalid DNS pattern {0!r}.\".format(pattern)\r\nservice_identity.exceptions.CertificateError: Invalid DNS pattern '194.167.13.105'.\r\n```\r\n\r\nI think this issue is somewhat similar to https://github.com/scrapy/scrapy/issues/2092 and this 'invalid DNS pattern' error should be caught similarly as the 'Invalid DNS-ID'. What do you think ?\r\n\r\nIn the meantime, how can i catch it myself and silent it ?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmike": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/7c9e32213db2ce757c784e7c29e30caa57dc3d48", "message": "Merge pull request #3059 from jesuslosada/fix-typo\n\nFix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/786144e0c7b25a02151b6d3135451da90e912719", "message": "Merge pull request #3058 from jesuslosada/fix-link\n\nFix link in news.rst"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/aa83e159c97b441167d0510064204681bbc93f21", "message": "Bump version: 1.4.0 \u2192 1.5.0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d07fe11981a07e493faf7454db79b98c02a53118", "message": "set release date"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c107059ef82a4b7b491b23b740b71353f03ab891", "message": "DOC fix rst syntax"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d4e5671d07a8dcf18b665ed3ce4136dccae222fb", "message": "make release docs more readable, add highlights"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/45b0e1a0e4c51a773b39be14334a999cc5f0fe56", "message": "DOC draft 1.5 release notes"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/930f6ed8002e27c9d51f5d5abbb28c36460eb1bb", "message": "Merge pull request #3050 from lopuhin/pypy3\n\nAdd PyPy3 support"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/632f1cc07305d6967c9e8ce3bf150337e2bf3ecd", "message": "Merge pull request #3049 from scrapy/trove-classifiers\n\n[MRG+1] setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9f9edeadfc9d8d3422d4ca15b2b13d5b502ca70e", "message": "Merge pull request #3048 from lopuhin/pypy-install-docs\n\n[MRG+1] Mention PyPy support, add PyPy to install docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1058169f0e3a8646dbd20f9b4c0b599ed9f6d08e", "message": "setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/bdc12f39949731e69aaa32c36490e1b6e56ca98b", "message": "Merge pull request #3045 from hugovk/rm-3.3\n\n[MRG+1] Drop support for EOL Python 3.3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9aa9dd8d45a2ce0c8e6ae0732e610f020735df7e", "message": "DOC mention an easier way to track pull requests locally.\nThanks @eliasdorneles!"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f716843a66829350063e55f8df768eb538c6b05c", "message": "DOC update \"Contributing\" docs:\n\n* suggest Stack Overflow for Scrapy usage questions;\n* encourage users to submit test-only pull requests with reproducable examples;\n* encourage users to pick up stalled pull requests;\n* we don't use AUTHORS file as a main acknowledgement source;\n* suggest using Sphinx autodocs extension"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dangra": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/b170e9fb96dc763b9b78916e1557021e5e004d59", "message": "Merge pull request #2609 from otobrglez/extending-s3-files-store\n\nS3FilesStore support for other S3 providers (botocore options)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/2dee191374e7fb7f2ed35ab9eea07ba7d1ee8b89", "message": "Merge branch 'master' into extending-s3-files-store"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9b4d6a40a6d56acbd9e068e15e6717dc06aee79b", "message": "Merge pull request #3053 from scrapy/release-notes-1.5\n\nRelease notes for the upcoming 1.5.0 version"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/57d04aa9601bc237da4b08777327df241483b389", "message": "Merge pull request #2767 from redapple/http-proxy-endpoint-key\n\n[MRG+1] Use HTTP pool and proper endpoint key for ProxyAgent"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/86c322c3a819405020cd884f128246ae55b6eaeb", "message": "Merge pull request #3038 from scrapy/update-contributing-docs\n\nDOC update \"Contributing\" docs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796", "body": "Can we avoid checking for \" xmlns \" in every loop iteration? it is an invariant check. move it to the top of the function.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810", "body": "alternative and less boilerplate: `inputs.extend(formdata.iteritems())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942", "body": "I tried removing the call to `_nons()` and still passed all tests.\nWhat about removing it completely of figuring out a test case that justifies its use? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002", "body": "not testing for string before splitting?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "jesuslosada": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/61c0b1478284b02a4fcfd2cc4931587c348c5d3a", "message": "Fix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a0836b8fd9720a9439cb3b940aca53b6844a094b", "message": "Fix link in news.rst"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "redapple": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/461f9daff5747728e26cd60e9dfe531092f58132", "message": "Update release notes for upcoming 1.4.1 version"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2906", "title": "[WIP] Downloader timings in request.meta", "body": "While working on a requests log plugin for Scrapy, I felt the need to get more fine-grained timing information on the various steps a `Request` goes through from reaching the `Downloader` to getting the associated response body.\r\nOne can work with `request.meta['download_delay']` and timing inserted from a downloader middleware but I found this additional information interesting for example for HAR output.\r\n\r\nNote: This does not address DNS lookup time nor TCP/TLS connection time.\r\n\r\nI would even set this to ON by default myself.\r\n\r\nMissing:\r\n\r\n- [ ] Documentation\r\n- [ ] How does this play with proxies?\r\n- [x] Handle download exceptions (DNS errors, dowload timeouts, connection errors etc.)", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "raphapassini": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/a1cc5a63d3e253c325159fdc6ebf4cd3faa37c49", "message": "Add mention to dont_merge_cookies in CookiesMiddlewares docs (#2999) (#3030)\n\nAdd mention to dont_merge_cookies in CookiesMiddlewares docs (#2999)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lopuhin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/bb1f31189128cb2272c1302350387075fbbb730a", "message": "Add PyPy3 support to faq and install doc"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/041308afe7c40de7088f75b0e0c312ecd5de428a", "message": "Fix get_func_args test for pypy3\n\nThese built-in functions are exposed as methods in PyPy3.\nFor scrapy this does not matter as:\n1) they do not work for CPython at all\n2) get_func_args is checked for presense of an argument in scrapy,\n   extra \"self\" does not matter.\nBut it still makes sense to leave these tests so that we know we\nshouldn't use get_func_args for built-in functions/methods."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f71df6f9addca10b562bb22890b5ea1c37efde5c", "message": "Run tests for PyPy3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/ea41114cf0ab2782650792ad204cf43fc148c749", "message": "Mention PyPy support, add PyPy to install docs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hugovk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/cbcf80b98ff66db1ccf625fa52c4de8935331972", "message": "Fix typo\n\n[CI skip]"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f11c21c6fc62b64a2bbee0e19e2098ed6257cf19", "message": "Test on Python 3.4"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/44623687ab8936c5696f68f74e438a2891880c82", "message": "Drop support for EOL Python 3.3"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tchiotludo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3072", "title": "Handle Webp Image transparency", "body": "Webp transparent image have a dark background : \r\n\r\nreference image : https://www.lavera.de/typo3temp/fl_realurl_image/105115-4021457609482-glossylips-deliciouspeach09-1294b-43402bf213-6cf9c.png", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BrandonSmithJ": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3062", "title": "Use as a library: Reactor decorator code + MWE", "body": "First, let me say thanks for the extremely useful tool - it really has made a lot of things much simpler than they otherwise would have been. \r\n\r\nWith that in mind, I'd like to give back what I can if it's useful. The problem I'm having is I don't know exactly where to post this. I'm not familiar enough with scrapy internals to suggest where to put the feature, or if it's even necessary enough to put into the main code base. It also likely needs one more modification before being ready for a main branch contribution. \r\n\r\nThe problem this solves is something at least a few people seem to have run into:\r\n\r\n- https://stackoverflow.com/questions/35289054/scrapy-crawl-multiple-times-in-long-running-process\r\n\r\n- https://stackoverflow.com/questions/22116493/run-a-scrapy-spider-in-a-celery-task/22202877#22202877\r\n\r\nMy use case is a website which requires a virtually endless process able to use various scrapy functionalities at whim. To solve the problem of restarting the twisted reactor, I essentially took the solutions I could find and rolled everything into an extremely simple decorator. Basically all the end-user needs to do is label the function they'd like to run as a scrapy function, and the decorator handles everything necessary in order to use scrapy in a library capacity:\r\n\r\n```\r\n\t@reactor_process(timeout=10)\r\n\tdef execute(self, keyword_list):\r\n\t\t''' Crawl the site specifically for certain keywords '''\r\n\t\tfrom crawlers.backend.spider_interface import construct_spider\r\n\t\tfrom scrapy.crawler import CrawlerProcess\r\n\r\n\t\tspider = construct_spider(self)\r\n\t\tspider.create_start_urls(keyword_list)\r\n\t\t\r\n\t\tcrawler  = CrawlerProcess(self.settings)\r\n\t\tdeferred = crawler.crawl(spider)\r\n\t\tdeferred.addBoth(lambda _: crawler._stop_reactor())\r\n```\r\n\r\nWithout the decorator, the function above would only be able to run once - the reactor engine would complain about being restarted. The one problem is that the necessary imports have to be made *within* the function itself - though I think this can be solved with a multiprocessing manager (or worst case, global inspection). \r\n\r\nI also have an interface which allows dynamic creation of contract docstrings / crawlers with different settings inside the same process, which might be useful; but that's something that would probably be better as its own discussion. \r\n  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Matthijsy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3061", "title": "Make autothrottle slow down on HTTP 429 response", "body": "When a response has status 429 (Too Many Requests) it is ignored by the AutoThrottle (because the latency is low, so otherwise the spider will speed up). I think that the wanted behaviour is that the spider will slow down when a 429 is received, but currently the spider will stay at a constant scraping speed. \r\n\r\nThis PR adds a new setting `AUTOTHROTTLE_429_DELAY` when it is set the download delay will be increased with this value every time a 429 is received. It still respects the 'MAX_DOWNLOAD_DELAY` value.  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "parlays": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3051", "title": "The URI gets set again before feed export store gets called.  This al\u2026", "body": "\u2026lows the spider to change settings in the parse method based on the content scraped.  The IFeedStorage interface has been changed for the store method to allow uri to be passed.  The BlockingFeedStorage class now sets the new uri settings in the store method.\r\n\r\nIn my personal scraper I overwrite the FeedExporter object and S3FeedStorage object.  But it would be great if this was part of the core.  It may be a tough change though because it requires a change to the IFeedStorage interface and that is a breaking change for older code.\r\n\r\nIt is common for me to scrape a site and then decide the folder name to use in the S3 bucket.  To be able to change the bucket name or directory in the parse method is important.  If this change is not agreed upon, let's think of another way to accomplish this.  Thanks!", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "munderseth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3041", "title": "Add Test Reporting to Travis CI", "body": "**Test Reporting for Travis using [Testspace.com](https://testspace.com)**\r\n\r\nHi `scrapy` team. We added test reporting to your repo. We have a blog article on [why we are staging repos](https://blog.testspace.com/testspace-and-open-source).\r\n\r\nNote that we contacted you in the past via an email. We thought it would be easier for you (if interested) by using a Pull Request.   \r\n\r\nFew of the benefits using Testspace:\r\n- view all your test results from a single dashboard\r\n- triage and manage your test failures faster\r\n- add more metrics\r\n- leverage built-in analytics \r\n- get a [test badge](https://help.testspace.com/how-to:get-badge) \r\n[![Space Health](https://open.testspace.com/spaces/74470/badge?token=2f48b759d52023674602ea42e90a5508cad6c953)](https://open.testspace.com/spaces/74470?utm_campaign=badge&utm_medium=referral&utm_source=test \"Test Cases\")\r\n\r\nCheckout **your** test results: https://open.testspace.com/projects/TryTestspace:scrapy from our fork.\r\n\r\n**Why** we are doing this: https://blog.testspace.com/testspace-and-open-source \r\n\r\n----\r\n\r\n**Give Testspace a try?**\r\n\r\n1. Create **Open** (free) account: https://testspace.com/pricing\r\n2. Add a **New Project** from the list of Github repo's\r\n3. Add a Travis **Environment Variable**: Name: `TS_ORG`  Value: `organization name` - based on subdomain selected in step 1\r\n4. Invite others: https://help.testspace.com/how-to:invite-other-users\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslay88": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3040", "title": "Fix for Issue #2919", "body": "Splits out url params and updates them with formdata. Passes local tests.\r\n\r\nFirst PR, please be nice :)\r\n\r\nFixes Issue #2919 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mylh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3028", "title": "Update practice.rst docs", "body": "added tips to avoid getting banned", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chainly": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3027", "title": "fix Spider.log to record right caller information", "body": "Spider.log always log itself in `pathname), funcName, lineno, etc.` format. Because `currentframe = lambda: sys._getframe(3)` and `f = f.f_back` in ``logging.Logger.findCaller`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3016", "title": "Added Scrapy logo", "body": "I replaced the \"Scrapy\" text with the logo shown on the Scrapy website", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web crawling.\nI thought that you guys would be doing that in multithreading but I heard that you guys never use any threads.How could this be possible?can anyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128", "body": "@gmeans code gives warning:\r\n\r\n /usr/local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py:51: builtins.UserWarning:\r\n         'broadd.context.CustomContextFactory' does not accept `method` argument (type OpenSSL.SSL method, e.g. OpenSSL.SSL.SSLv23_METHOD). Please upgrade your context factory class to handle it or ignore it.\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454", "body": "@redapple I had errors:\r\n`<twisted.python.failure.Failure OpenSSL.SSL.Error: ('SSL routines', 'SSL3_READ_BYTES', 'sslv3 alert handshake failure'), ('SSL routines', 'SSL3_WRITE_BYTES', 'ssl handshake failure')>` \r\nwith all packages up-to-date", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745", "body": "Scrapy    : 1.4.0\r\nlxml      : 3.8.0.0\r\nlibxml2   : 2.9.4\r\ncssselect : 1.0.1\r\nparsel    : 1.2.0\r\nw3lib     : 1.17.0\r\nTwisted   : 17.5.0\r\nPython    : 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 17:54:52) [MSC v.1900 32 bit (Intel)]\r\npyOpenSSL : 17.1.0 (OpenSSL 1.1.0f  25 May 2017)\r\nPlatform  : Windows-10-10.0.15063-SP0\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674", "body": "@redapple Thank you", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web \ncrawling.\nI thought that you guys would be doing that in multithreading but I \nheard that you guys never use threading.How could this be possible?can \nanyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160", "body": "Thank you, missconfiguration.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683", "body": "https://github.com/kjd/idna/issues/50#issuecomment-312908539", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "isra17": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2996", "title": "Add signals to handle error from downloader or pipeline", "body": "As of right now, there's is no simple way to handle exception coming from items pipeline or a the `process_response` of a downloader middleware.\r\n\r\nThis PR adds two signals for those use case. Note that the downloader signals also catch any exception from the downloader engine, including `process_request` and `process_exception`.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2995", "title": "Allow passing Failure object to middlewares", "body": "Trying to handle errors coming from middleware, it happens that Twisted strip traceback from an exception returned from a completed deferred (Needed to avoid GC issues). This mean that trying to use `exception.__traceback__` always yield `None`.\r\n\r\nThis PR adds a decorator that can be used on middleware `process_exception` or `process_spider_exception` to get a Failure object instead of an Exception. The Failure object provides a `getTracebackObject` to get a `Traceback`-like object which come really handy when trying to pinpoint an issue.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2962", "title": "Add signal `request_downloading` called right before the download handler", "body": "It was needed for one of our project, maybe it can be useful for core as well.\r\n\r\nThe `request_downloading` signal is sent when the engine is about to download a request. If one handler raise an exception, the download is aborted. The signal supports returning deferreds from their handlers.\r\n\r\nThis is an alternative to the Downloader middlewares where there might be a significant delay between the middleware call and the download handler in case of slow queue processing.\r\nThis even handler allow some extension to tamper the request right before the download time and possibly cancel the request by raising an exception.\r\n\r\n`signal.send_deferred` was needed for the exception handling raised by signal handler in the download manager.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "starrify": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2988", "title": "Added: Customizing request fingerprint calculation via request meta", "body": "Partially implements #900", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2984", "title": "Added: Making the list of exceptions to retry configurable via settings", "body": "Implements #2701", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "immerrr": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2986", "title": "WIP: CookiesMiddleware: add \"reset_cookies\" meta to clear the jar", "body": "This PR adds a `reset_cookies` meta to clean the active cookiejar.\r\n\r\nWhen I try working with sessions I often find myself in a situation when I'd like to restart the session \"from scratch,\" including cookies. It's doable by just setting `cookiejar` meta to an arbitrary value, but then we'd be accumulating the cookiejars over time.\r\n\r\nI'm not entirely sure about the meta name, as I'd probably like it namespaced, e.g. `cookies_reset`, but there's a precedent already with `dont_merge_cookies`, so I chose to follow that pattern.\r\n\r\n@kmike am I missing something here?", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2985", "title": "utils.curl: add parse_curl_cmd func", "body": "Given that the major browsers are able to export the requests in cURL format, it's logical that we have a utility in scrapy to make it a request.\r\n\r\nThis PR works towards adding a function that creates kwargs that could be passed to a `Request` constructor.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2950", "title": "loader add_* funcs: pass **kw to self.selector.xpath", "body": "I have found out that kwargs that one specifies for `loader.add_xpath` are not getting to the actual `selector.xpath` invocation, which looks like a bug.\r\n\r\nI wonder if there's code out there that would be broken by this fix...", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aitoehigie": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2972", "title": "Add a note to allowed_domains", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NoExitTV": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2955", "title": "Handle \"invalid\" relative URL issue #1304", "body": "Did some minor tweaks on how scrapy handle relative URL's as discussed in #1304 \r\n\r\nTested it with some basic code in the scrapy shell:\r\n```\r\n>>> resp = scrapy.http.response.html.HtmlResponse('http://www.example.com',\r\n...      body='''<html>\r\n...                  <body>\r\n...                      <a href=\"../index1.html\">Link1</a>\r\n...                      <a href=\"../../index2.html\">Link2</a>\r\n...                      <a href=\"index3.html\">Link3</a>\r\n...                      <a href=\"other_html/../index4.html\">Link4</a>\r\n...                      <a href=\"other_html/folder2/../index5.html\">Link5</a>\r\n...                      <a href=\"other_html/index6.html\">Link6</a>\r\n...                      <a href=\"../other_html/index7.html\">Link7</a>\r\n...                 </body>\r\n...          </html>''')\r\n>>> links = scrapy.linkextractors.LinkExtractor().extract_links(resp)\r\n>>> links\r\n[Link(url='http://www.example.com/index1.html', text='Link1', fragment='', nofollow=False), Link(url='http://www.example.com/index2.html', text='Link2', fragment='', nofollow=False), Link(url='http://www.example.com/index3.html', text='Link3', fragment='', nofollow=False), Link(url='http://www.example.com/index4.html', text='Link4', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index5.html', text='Link5', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index6.html', text='Link6', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index7.html', text='Link7', fragment='', nofollow=False)]\r\n```\r\n\r\nPassed the same unit tests as the original code when running tox\r\n```\r\n========================================================= 1679 passed, 6 skipped, 14 xfailed in 458.89 seconds =========================================================\r\n\r\n_______________________________________________________________________________ summary ________________________________________________________________________________\r\n\r\n  py27: commands succeeded\r\n  congratulations :)\r\n```\r\n\r\nI believe that scrapy now handle relative url's as expected in python 2.7.13\r\nWhat are your thoughts?", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kaplun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2954", "title": "spiders: add OAI-PMH support WIP", "body": "Signed-off-by: Samuele Kaplun <samuele.kaplun@cern.ch>", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phnk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2953", "title": "Added debug message in spiders/crawl", "body": "As requested in #2925.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mGalarnyk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2931", "title": "Update install.rst", "body": "Updated installation instructions for installing scrapy using conda.\r\n\r\nYou can now just do: \r\n\r\n```\r\nconda install scrapy\r\n```", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "HarrisonGregg": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2918", "title": "Fix SIGINT handling when using inspect_response", "body": "Currently, after calling `scrapy.shell.inspect_response` and then closing the opened shell, SIGINT (Ctrl-C) no longer works to terminate the spider.  This is because `Shell.start`, called in `inspect_response`, removes the signal handler.  To fix this, save the SIGINT handler before calling `Shell.start` in `inspect_response` and add it again after the shell has closed.\r\n\r\nIt doesn't appear that there are any tests for `inspect_response`, so I haven't added any tests for this fix.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangtua1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2917", "title": "support giantfiles download ,fix CVE-2017-14158", "body": "from the issue https://github.com/scrapy/scrapy/issues/482.\r\nWe can see if we download a file with filespipeline, the whole file is stored into the memory.\r\nIf the file is a giant file or an extremely giant file,(over 1G or more), the crawling thread will be crashed.\r\nWe can use this POC to prove it.\r\n[POC.zip](https://github.com/scrapy/scrapy/files/1289514/POC.zip)\r\n\r\nThis has been asigned CVE-2017-14158.\r\nbut in this new testcase using GiantFilesPipeline in the commit,we can download it correctly.\r\n\r\n[testdownload.zip](https://github.com/scrapy/scrapy/files/1289512/testdownload.zip)\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhaojiedi1992": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2911", "title": "Update exporters.rst", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfdeshom": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118", "body": "Hi, to disable `scrapy.contrib.feedexport.FeedExporter`, I had to set it to `None` in my `EXTENSIONS` dict instead of `0`. I am getting this error when using scrapyd.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185", "body": "off-topic but: in general to disable any extension, middleware, etc I have to set it to `None`, which can be a little confusing since you would think that `0` would work too\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375", "body": "@dangra unfortunately, I don't see a documented way to disable an extension and I consider myself pretty familiar with scrapy and its docs. Maybe it's already there in the docs, but simply needs to be more prominent.\n\nFor me there is a larger issue: some extensions have settings associated with them that make it confusing to know exactly how/where to disable them. For example, the cookies extension seems to have 2 ways to disable it:\n-  set `COOKIES_ENABLED` to `False` (https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/cookies.py)\n- set the priority to `None`, ie set `scrapy.contrib.downloadermiddleware.CookiesMiddleware` to `None`\n\nIt would be nice to have just one way to disable extensions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "NicolasP": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831", "body": "Hi, any chance for a review of this PR?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "knaveofdiamonds": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951", "body": "@kmike - ah, ok. I'm working on an inherited codebase that doesn't use the `LinkExtractors` directly, so this was obviously just a misunderstanding of the API/using something non-public. I'll close this issue - agree with backwards incompatible change reasons.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nside": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706", "body": "Seeing the same issue\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703", "body": "@dangra crawl() fixed it for me. Still I'd expect any requests scheduled to have the same treatment, whatever their \"entry point\" in the pipeline is. Feel free to close if you disagree.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941", "body": "I'm on 0.21 (dev). These are good subtleties to know!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606", "body": "Hi,\n\nI've copied a URL below which failed with the original function because it has no url argument and the arguments are in an unexpected order. I can certainly add some unit tests, I'll have a look into it now. It's the end of the day and I want some food!\n\nCheers,\nRob\n\nhttp://www.firstchoice.co.uk/fcsun/page/search/searchresults?sttrkr=mthyr:02/2011_durT:7/n_ls:true_tuidesc:000832_day:15_mps:9_isvid:false_pconfig:1|2|0|0|0|_tchd:0_rating:0_act:0_jsen:true_resc:_attrstr:||||||||null|null_mdest:false_tinf:0_mnth:02_desc:_bc:17_margindt:7_tadt:2_numr:1_spp:mainSearch_depm:7_tuiresc:004287_dur:7_dtx:0_df:false_dxsel:0_imgsel:0_dac:MAN_loct:0_tsnr:0_year:2011_dta:false_tuiacc:028367_acc:\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615", "body": "Which distro are you using? Here in ubuntu, works fine. Try:\n\napt-get install -y build-essential python-dev\napt-get install -y libssl-dev libxml2-dev libxslt1-dev libssl-dev libffi-dev\n\nthen pip install scrapy in a brand new virtualenv\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982", "body": "Let me try again:\n\nI want to use scrapy like this:\n\n```\n# prints options\nscrapy crawl -t csv|sql|mongo|etc -h\n\n# uses a particular item exporter\nscrapy crawl -t csv -f somefile.csv\nscrapy crawl -t mongo --db somedb --col somecollection\n```\n\nWhat do you guys think of this? Is it desired behavior? Does not feel like a whole lot of code to modify. I could do it. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ariddell": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661", "body": "@nramirezuy there's a reference implementation for pep 3156 here: https://code.google.com/p/tulip/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sabren": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629", "body": "Here you go: \n\nhttps://github.com/scrapy/scrapy/pull/45\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490", "body": "Thanks, Scotty! \n\nI'm sure my client would appreciate it.\n\nCan you make a combined pull request, or should I pull from you and open another pull request here?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "mvj3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848", "body": "Thanks @curita for the careful review! I correct it in a new commit.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "RFDAJE": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937", "body": "I had been facing the same issue, so far the simplest solution I found is inside `item_completed`, after getting all things done, reset downloaded to empty. `self.spiderinfo.downloaded = {}`, memory leak issue resolved. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "paulproteus": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573", "body": "I did some work refreshing these patches against current origin/master.\n\nhttps://github.com/paulproteus/scrapy/tree/revise-pullrequest-109\n\nSome notes:\n\nSee 3077ac8b6f592b044ad67f15af3b065d06f27cf7 for a fix where Http11DownloadHandler needs to accept second argument, since that's how the tests use it.\n\nSee 888dfadd24f7d325bffab367acc9dd5a7405e5bc for a fix where the call to log.err() creates an exception in the log that test runner notices, so tests that call this method begin to fail. For now I've disabled the call to log.err(). If we want to keep logging the error, we'll need to call flushLoggedErrors() -- see http://twistedmatrix.com/documents/current/core/howto/trial.html#auto11 . (The test this makes fail is scrapy.tests.test_downloader_handlers.Http11TestCase.test_timeout_download_from_spider )\n\nCommit bdd2a1b02c944845fec4d6142fcb63367b17cc11 (rebased from the original, but otherwise the same) makes many tests fail because there is work left in the reactor. One test you can see this with is scrapy.tests.test_engine.EngineTest.test_crawler\n\nFor now I can't promise I'll have time to figure out what's going on with leaving the reactor unclean, but I thought I'd leave a comment with what I have found in a few hours of looking into this pull request!\n\n-- Asheesh.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rmax": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800", "body": "#62 is a duplicated report of this issue but includes a pull request with a fix.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083", "body": "See issue #58\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355", "body": "As a workaround you can use `process_value` argument:\n\n`SgmlLinkExtractor(..., process_value=lambda v: v.strip())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580", "body": "I have reverted the redirect middleware order and added tests for gzipped meta redirection. Although I'm not sure about leaving the body compressed if it fails, I think there could be more errors (ie. connection drop) that could make fail the decompression and leaving the body as is could produce misbehavior in other components.\n\nAdding more tests for the middlewares integration could help us to identify the best solution.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823", "body": "without any fix fails test_gzipped_redirect_30x\nwith the redirect reorder fails test_gzipped_meta_redirect\nwith dangra's suggestion passes all tests (See https://gist.github.com/1718659)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882", "body": "Nice!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "scottyallen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154", "body": "Thanks for the patch - I was in the process of trying to fix this, and it saved me a ton of time:)  However, I don't think line 191 is quite right for the tunnel case.  It results in sending a GET request with the full url to the destination webserver, which is technically wrong and some sites refuse to handle.  Instead, self.path should remain unchanged for the tunnel case.  I can send a patch to your patch, if you like...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "pablohoffman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863", "body": "Is there a way to avoid calling a protected method of lxml.html?. As this may raise some compatibility issues on future/past lxml versions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702", "body": "touple -> tuple\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727", "body": "should we make these settings dicts (like extension and middlewares) and use `scrapy.utils.conf.build_component_list` to load them?.\n\nany particular reason why you went with lists instead?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737", "body": "I think this should inherit from `AssertionFailure`, to appear as test failures (and not errors) in some testing frameworks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755", "body": "I think I would prefer two contracts for these:\n\n```\n@minitems 1\n@minrequests 2\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772", "body": "Why do we need this method for?. I don't see it used, and we already have `adjust_request_args`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784", "body": "what about just ignoring those who don't?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786", "body": "perhaps in the future we can add a way to list contracts per spider\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791", "body": "I'm not sure a separte register method is needed. How about the constructor receiving the contracts? Like middlewares and extension do. See `scrapy.middleware` module.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796", "body": "these changes should go into a separate PR\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460", "body": "I think we should reuse the convention of using dicts. (we were planning to port pipelines too).\n\nIt's true that priorities won't add much in in this case, but it's not only priorities what the dict mechanism provides, but also being to disable specific components of the (presumably most sensible default). Priorioties wont' add much to this (as they don't to extensions) but don't harm and it could be useful if someone ever needs them. We should make sure internal code in an intuitive behaviour. Like pre_process being called in priority-order.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558", "body": "I wasn't considering all cases. In that case, how about this:\n\n```\n@returns <type> [M [N]]\n```\n\nHere are some examples to illustrate:\n- `@returns request` - returns at least one request\n- `@returns request 2` - returns at least two requests\n- `@returns request 2 10` - returns at least two requests and no more than 10 requests\n- `@returns request 0 10` - returns no more than 10 requests\n\nSame goes for items.\n\nMaybe make aliases (`request` -> `requests` , `item` -> `items`) so that both plurals and singulars work.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903", "body": "I can't see that comment on github now, did you remove it?\n\nOn Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi\nnotifications@github.comwrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> \n> @returns requests 2 - returns at least two requests\n> \n> -- wouldn't this be considered unexpected behaviour?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/167/files#r1570864.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908", "body": "this one looks very similar to `scrapy.utils.get_func_args` (which is tested and supports methods, function, classes, etc) - could that one be reused?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916", "body": "`from scrapy.conf import settings` is deprecated  API, use `self.settings` within the command methods,  that attribute is assigned by the `scrapy.cmdline` mechanics.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "nuklea": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155", "body": "What about pep8?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "alexcepoi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975", "body": "what about maxitems, maxrequests? Or the case where you expect to receive exactly one request (the original sep describes a returns_request contract which checks this).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004", "body": "Ignoring sounded unfriendly to me at first (i.e. you may wonder why it does not work). Also eliminating the assertion gives a very misleading error.\nListing contracts sounds good to me maybe a \"--list / -l\" option?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025", "body": "This is a reminiscence from the original implementation. I thought it could prove useful, but now that I think about it's hard to find a scenario in which adjust_request_args is insufficient.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066", "body": "I did not think priorities would be useful, or that we should encourage users to rely on contracts priorities.\nEspecially since for some hooks (pre_process comes into mind) the last contract hooked in is the first one to be executed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090", "body": "note: also change docstring\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864", "body": "`@returns requests 2` - returns at least two requests\n\n-- wouldn't this be considered unexpected behaviour?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929", "body": "It's an outdated diff (i.e. comment on a line of code which has been modified). You should still be able to find it in the issue\n\nOn Sep 10, 2012, at 11:30 PM, Pablo Hoffman notifications@github.com wrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> >   I can't see that comment on github now, did you remove it? On Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi notifications@github.comwrote:\n> >   \u2026\n> >   \u2014\n> >   Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}, "4": {"FredEnglish": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3079", "title": "Connection Lost in a non-clean fashion for some URLs on a particular domain, but not others", "body": "I have created a basic spider to scrape a small group of job listings from totaljobs.com. I have set up the spider with a single start URL, to bring up the list of jobs I am interested in. From there, I launch a separate request for each page of the results. Within each of these requests, I launch a separate request calling back to a different parse method, to handle the individual job URLs.\r\n\r\nWhat I'm finding is that the start URL and all of the results page requests are handled fine - scrapy connects to the site and returns the page content. However, when it attempts to follow the URLs for each individual job page, scrapy isn't able to form a connection. Within my log file, it states:\r\n\r\n`[<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]`\r\n\r\nI'm afraid that I don't have a huge amount of programming experience or knowledge of internet protocols etc. so please forgive me for not being able to provide more information on what might be going on here. I have tried changing the TLS connection type; updating to the latest version of scrapy, twisted and OpenSSL; rolling back to previous versions of scrapy, twisted and OpenSSL; rolling back the cryptography version, creating a custom Context Factory and trying various browser agents and proxies. I get the same outcome every time: whenever the URL relates to a specific job page, scrapy cannot connect and I get the above log file output.\r\n\r\nIt may be likely that I am overlooking something very obvious to seasoned scrapers, that is preventing me from connecting with scrapy. I have tried following some of the the advice in these threads:\r\n\r\n[https://github.com/scrapy/scrapy/issues/1429](https://github.com/scrapy/scrapy/issues/1429)\r\n[https://github.com/requests/requests/issues/4458](https://github.com/requests/requests/issues/4458)\r\n[https://github.com/scrapy/scrapy/issues/2717](https://github.com/scrapy/scrapy/issues/2717)\r\n\r\nHowever, some of it is a bit over my head e.g. how to update cipher lists etc. I presume that it is some kind of certification issue, but then again scrapy is able to connect to other URLs on that domain, so I don't know.\r\n\r\nThe code that I've been using to test this is very basic, but here it is anyway:\r\n\r\n```\r\nimport scrapy\r\n\r\nclass Test(scrapy.Spider):\r\n\t\r\n\r\n\tstart_urls = [\r\n\t\t\t\t\t'https://www.totaljobs.com/job/welder/jark-wakefield-job79229824'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/elliott-wragg-ltd-job78969310'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/exo-technical-job79019672'\r\n\t\t\t\t\t,'https://www.totaljobs.com/job/welder/exo-technical-job79074694'\r\n\t\t\t\t\t\t]\r\n\t\r\n\tname = \"test\"\r\n\r\n\tdef parse(self, response):\r\n\t\tprint 'aaaa'\r\n                yield {'a': 1}\r\n```\r\n\r\nThe URLs in the above code **are not** being connected to successfully.\r\n\r\nThe URLs in the below code **are** being connected to successfully.\r\n\r\n```\r\nimport scrapy\r\n\r\nclass Test(scrapy.Spider):\r\n\t\r\n\r\n\tstart_urls = [\r\n\t\t\t\t\t'https://www.totaljobs.com/jobs/permanent/welder/in-uk'\r\n\t\t\t\t\t,'https://www.totaljobs.com/jobs/permanent/mig-welder/in-uk'\r\n\t\t\t\t\t,'https://www.totaljobs.com/jobs/permanent/tig-welder/in-uk'\r\n\t\t\t\t\t\t]\r\n\t\r\n\tname = \"test\"\r\n\r\n\tdef parse(self, response):\r\n\t\tprint 'aaaa'\r\n                yield {'a': 1}\r\n```\r\n\r\n\r\nIt'd be great if someone could replicate this behavior (or not as the case may be) and let me know. Please let me know if I should submit additional details. I apologise, if I have overlooked something really obvious. I am using:\r\n\r\nWindows 7 64 bit\r\nPython 2.7\r\nscrapy version 1.5.0\r\ntwisted version 17.9.0\r\nopenSSL version 17.5.0\r\nlxml version 4.1.1", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3079/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "shanmuga-cv": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077", "title": "scrapy selector fails when large lines are present response", "body": "Originally encoutered when scraping [Amazon restaurant](https://www.amazon.com/restaurants/zzzuszimbos0015gammaloc1name-new-york/d/B01HH7CS44?ref_=amzrst_pnr_cp_b_B01HH7CS44_438).  \r\nThis page contains multiple script tag with lines greater then 64,000 character in one line. \r\nThe selector (xpath and css) does not search beyond these lines. \r\n\r\nDue to this the following xpath `'//h1[contains(@class, \"hw-dp-restaurant-name\")]/text()'` to extract name of the restaurant returns empty even though there is a matching tag is present.\r\n\r\n\r\nPFA the response text at [original_response.html.txt.gz](https://github.com/scrapy/scrapy/files/1631425/original_response.html.txt.gz)\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3077/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "crisfan": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075", "title": "Telnet Console ", "body": "hi, @kmike,I use telnetlib to pause scrapy engine, there is some error! although it pause the scrapy engine.\r\nthis is my poor code:\r\n``` javascript\r\nimport telnetlib\r\nhost = '127.0.0.1'\r\ntn = telnetlib.Telnet(host, 6023)\r\ntn.write(\"engine.pause()\\n\") \r\n``` \r\nerror:\r\n``` javascript\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\r\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n\r\nUnhandled Error\r\nTraceback (most recent call last):\r\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\r\n``` \r\n\r\ncould you fix my problem, thansk ~~~\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Caleb-Wade": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074", "title": " Cxfreeze+python3.4+scrapy1.4 failed to bundle to executables(AttributeError: module object has no attribute '_fix_up_module')", "body": "After I failed to bundle .py document into .exe document by pyinstaller, I tried cxfreeze. Similar error happened. Someting went wrong when importing scrapy module, and AttributeError: module object has no attribute '_fix_up_module' appeared in the command window. Would someone tell me what happened. Can I make it? Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073", "title": "Pyinstaller+Python3.4+Scrapy1.4 Failed to bundled exe (ImportError:No module named \"XXXX\")", "body": "I have tried to bundle my spider .py into .exe, but weeks pasted, I didn't make it. It pointed that some modules were missing. Even I used --hidden-import to put the missing modules into my bundled exe, it still didn't work. It's amazing that I can see the modules in the HTML document under build file. Pls help me ,Thks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3073/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lifei1245": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069", "title": "about the signal retry_complete", "body": "I didn't find the singnal in the singnal list,how can I use it", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3069/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "3xp10it": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068", "title": "scrapy always Starting new HTTP connection after crawl finished ", "body": "I reopen [#3066][1] here,there are more details [here][2].\r\n\r\n[1]: https://github.com/scrapy/scrapy/issues/3066\r\n[2]: https://stackoverflow.com/questions/48182204/scrapy-always-starting-new-http-connection-after-crawl", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3068/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "theduman": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067", "title": "scrapy.Request doesnt wait for loop", "body": "I fetch url and other parameters from database and use them in for loop and pass to scrapy.request() function but when i run the code i succesfully pass first element only. Scrapy can't get other elements of list. Here is my code\r\n\r\n```py\r\nclass QuotesSpider(scrapy.Spider):\r\n    name = \"quotes\"\r\n    custom_settings = {\r\n        'CONCURRENT_REQUESTS': 1,\r\n    }\r\n    def start_requests(self):\r\n        sourceArr = []\r\n        connection = db.connect()\r\n        try:\r\n            with connection.cursor() as cursor:\r\n                # Read a single record\r\n                sql = \"SELECT `code`, `url`, `target`,`next` FROM `source`\"\r\n                cursor.execute(sql)\r\n                result = cursor.fetchall()\r\n        finally:\r\n            connection.close()\r\n        print(result)\r\n        for i in result:\r\n            source = Source(i['code'], i['url'], i['target'], i['next'])\r\n            sourceArr.append(source)\r\n        print(sourceArr)\r\n        #for url in urls:\r\n           #yield scrapy.Request(url=url, callback=self.parse)\r\n        for s in sourceArr:\r\n            print(s.target)\r\n            yield scrapy.Request(url=s.url, meta={'target': s.target, 'next': s.next})\r\n            print(\"slept\")\r\n\r\n\r\n    def parse(self, response):\r\n        titlearr = []\r\n        count = 0\r\n        print(response.meta)\r\n        for title in response.css(response.meta['target']):\r\n            count += 1\r\n            titlearr.append(title.css('p a::text').extract_first())\r\n            #yield {'title': title.css('p a::text').extract_first()}\r\n        print(\"total count \" + str(count))\r\n        print(titlearr)\r\n        for next_page in response.css(response.meta['next']):\r\n            yield response.follow(next_page, self.parse)\r\n```\r\n\r\nWhen i print response.meta i got target and next values for only first item. List contains more than 1 item. How can i make scrapy.Request() function wait for the next element to run?\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3067/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "benjolitz": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065", "title": "`Connection to the other side was lost` for older French site", "body": "Hi,\r\n\r\nI'm attempting to run scrapy on `https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`.\r\n\r\nDespite upgrading all my brew packages, scrapy installation via `pip install -U scrapy`, the website disconnects uncleanly. Specifying `-s DOWNLOADER_CLIENT_TLS_METHOD=TLSv1.0` makes no difference to the following run. Neither does a user-agent clear things up with `-s USER_AGENT=\"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\"`\r\n\r\nI am uncertain as how to debug this further.\r\n\r\n`scrapy shell https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences`\r\n\r\n```\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\r\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Darwin-16.7.0-x86_64-i386-64bit\r\n2018-01-09 18:03:33 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOADER_CLIENT_TLS_METHOD': 'TLSv1.0', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2018-01-09 18:03:33 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2018-01-09 18:03:33 [scrapy.core.engine] INFO: Spider opened\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n2018-01-09 18:03:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\nTraceback (most recent call last):\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/bin/scrapy\", line 11, in <module>\r\n    sys.exit(execute())\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/commands/shell.py\", line 73, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 48, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 115, in fetch\r\n    reactor, self._schedule, request, spider)\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/python/failure.py\", line 385, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n```\r\n\r\n\r\n`scrapy version -v`:\r\n```\r\nScrapy       : 1.5.0\r\nlxml         : 4.1.1.0\r\nlibxml2      : 2.9.7\r\ncssselect    : 1.0.3\r\nparsel       : 1.3.1\r\nw3lib        : 1.18.0\r\nTwisted      : 17.9.0\r\nPython       : 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)]\r\npyOpenSSL    : 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017)\r\ncryptography : 2.1.4\r\nPlatform     : Darwin-16.7.0-x86_64-i386-64bit\r\n```\r\n\r\n`curl -sLv https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences 2>&1 | head -20`:\r\n```\r\n*   Trying 160.92.134.3...\r\n* TCP_NODELAY set\r\n* Connected to agences.creditfoncier.fr (160.92.134.3) port 443 (#0)\r\n* TLS 1.0 connection using TLS_RSA_WITH_3DES_EDE_CBC_SHA\r\n* Server certificate: agences.creditfoncier.fr\r\n* Server certificate: RapidSSL RSA CA 2018\r\n* Server certificate: DigiCert Global Root CA\r\n> GET /credit-immobilier/toutes-nos-agences HTTP/1.1\r\n> Host: agences.creditfoncier.fr\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 200 OK\r\n< Date: Wed, 10 Jan 2018 02:06:54 GMT\r\n< Server: Microsoft-IIS/6.0\r\n< X-Powered-By: ASP.NET\r\n< Content-Length: 35884\r\n< Content-Type: text/html\r\n< Set-Cookie: ASPSESSIONIDQQBACDTS=JHOJNNCCJNIFABOPLNCEEAFH; path=/\r\n< Cache-control: private\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3065/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "reaCodes": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064", "title": "I ran into a problem when I used ipython in a scrapy shell", "body": "I used this command, `scrapy shell 'www.baidu.com'`, and then used `response.body`. \r\n\r\nWhen I input `resp` and click `Tab`\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660511-e159fe1a-f47d-11e7-8499-b6589ef7de3f.png)\r\n\r\nAs you can see, there was a display error\r\n\r\n![image](https://user-images.githubusercontent.com/13817254/34660548-16f0b794-f47e-11e7-996b-9862f788a771.png)\r\n\r\nUse the command completion function is encountered a display error", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3064/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmagonski": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060", "title": "HTTPERROR_ALLOWED_CODES can't parse response with 301", "body": "In scrapy.downloadermiddlewares.redirect.RedirectMiddleware#process_response \r\ndosn't get anything from settings like HTTPERROR_ALLOWED_CODES only in HttpErrorMiddleware we have \r\nhandle_httpstatus_list.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3060/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NewUserHa": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057", "title": "configure_logging unable to handle GBK", "body": "I added \r\n`import logging\r\nfrom scrapy.utils.log import configure_logging\r\n\r\nconfigure_logging(install_root_handler=False)\r\nlogging.basicConfig(\r\n    filename='log.txt',\r\n    format='%(levelname)s: %(message)s',\r\n    level=logging.INFO\r\n)`\r\nfrom scrapy documents blow my class define.\r\n\r\nthen:\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\logging\\__init__.py\", line 982, in emit\r\n    stream.write(msg)\r\nUnicodeEncodeError: 'gbk' codec can't encode character '\\ufe0f' in position 190: illegal multibyte sequence\r\nCall stack:\r\n....\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\core\\scraper.py\", line 237, in _itemproc_finished\r\n    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\r\nMessage: 'Scraped from %(src)s\\r\\n%(item)s'\r\nArguments: {'src': <200 http://...>, 'item': {'date': '12-02', 'floor': '...\\n    \u7535\\ufe0f', 'pics': ['...', ...]}}\r\n\r\nI also have 'LOG_LEVEL': 'INFO' in the spider.\r\n\r\nI googled and have no idea how to fix this.\r\nany help, please?\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3057/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056", "title": "[suggest] add a option to pipeline for some sites requiring reference in heads", "body": "it's usual case and it's ugly to override get_media_requests method of pipelines.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3056/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055", "title": "[bug] pillow will always recode images in imagepipieline", "body": "https://github.com/scrapy/scrapy/blob/aa83e159c97b441167d0510064204681bbc93f21/scrapy/pipelines/images.py#L151\r\n\r\nthis line will always recode images silently and damage the image quality.\r\n\r\nplease add an option to avoid this.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "elacuesta": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054", "title": "Request serialization should fail for non-picklable objects", "body": "The Pickle-based disk queues silently serialize requests that shouldn't be serialized in Python<=3.5. I found this problem when dumping a request with an `ItemLoader` object in its `meta` dict. Python 3.6 fails in [this line](https://github.com/scrapy/scrapy/blob/1.4/scrapy/squeues.py#L27) with `TypeError: can't pickle HtmlElement objects`, because the loader contains a `Selector`, which in turns contains an `HtmlElement` object.\r\n\r\nI tested this using the https://github.com/scrapinghub/scrapinghub-stack-scrapy repository, and found that `pickle.loads(pickle.dumps(selector))` doesn't fail, but generates a broken object.\r\n\r\n#### Python 2.7, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3)\r\n```\r\nroot@04bfc6cf84cd:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 2.7.14 (default, Dec 12 2017, 16:55:09) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@04bfc6cf84cd:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:49:27 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath=u'descendant-or-self::a' data=u'<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 140144569743064\r\n```\r\n\r\n\r\n#### Python 3.5, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@1945e2154919:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.5.4 (default, Dec 12 2017, 16:43:39) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@1945e2154919:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:52:37 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\n>>> response.selector.css('a')\r\n[<Selector xpath='descendant-or-self::a' data='<a href=\"http://www.iana.org/domains/exa'>]\r\n>>> s2.css('a')\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 227, in css\r\n    return self.xpath(self._css2xpath(query))\r\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 203, in xpath\r\n    **kwargs)\r\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\r\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\r\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\r\nAssertionError: invalid Element proxy at 139862544625976\r\n```\r\n\r\n\r\n#### Python 3.6, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\r\n```\r\nroot@43e690443ca7:/# scrapy version -v\r\nScrapy    : 1.3.3\r\nlxml      : 3.7.2.0\r\nlibxml2   : 2.9.3\r\ncssselect : 1.0.1\r\nparsel    : 1.1.0\r\nw3lib     : 1.17.0\r\nTwisted   : 16.6.0\r\nPython    : 3.6.4 (default, Dec 21 2017, 01:35:12) - [GCC 4.9.2]\r\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\r\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\r\nroot@43e690443ca7:/# scrapy shell \"http://example.org\"\r\n2017-12-29 16:54:49 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\r\n(...)\r\n>>> from six.moves import cPickle as pickle\r\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\nTypeError: can't pickle HtmlElement objects\r\n```", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3054/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3082", "title": "[WIP] Do not serialize unpickable objects (py3)", "body": "This addresses #3054 partially, as it catches the exception raised by `pickle`.\r\nHowever, since this exception is only raised in python >= 3.6, for earlier versions it's still not able to realize it shouldn't serialize some requests.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2956", "title": "Add from_crawler support to dupefilters", "body": "Fixes #2940\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stummjr": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046", "title": "Item loader missing values from base item", "body": "ItemLoaders behave oddly when they get a pre-populated item as an argument and `get_output_value()` gets called for one of the pre-populated fields before calling `load_item()`.\r\n\r\nCheck this out:\r\n\r\n```python\r\n>>> from scrapy.loader import ItemLoader\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': 'http://example.com'}\r\n\r\n# so far, so good... what about now?\r\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\r\n>>> loader = ItemLoader(item)\r\n>>> loader.get_output_value('url')\r\n[]\r\n>>> loader.load_item()\r\n{'summary': 'foo bar', 'url': []}\r\n```\r\n\r\nThere are **2** unexpected behaviors in this snippet (at least from my point of view):\r\n\r\n**1)** `loader.get_output_value()` doesn't return the pre-populated values, even though they end up in the final item.\r\n\r\nIt seems to be like this on purpose, though. The `get_output_value()` method only queries the `_local_values` defaultdict ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\n**2)** once we call `loader.get_output_value('url')`, that field is not included in the `load_item()` result anymore.\r\n\r\nThis one doesn't look right, IMHO.\r\n\r\nIt happens because when we call `loader.get_output_value('url')` for the first time, such value is not available on `_local_values`, and so a new entry in the `_local_values` defaultdict will be created with an empty list on it ([here](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L121)). Then, when `loader.load_item()` gets called, [these lines](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L116-L117) overwrite the current value from the internal item because the value returned by `get_output_value()` is `[]` and not `None`.\r\n\r\nAny thoughts on this?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3046/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3047", "title": "Avoid missing base item fields in item loaders", "body": "This is an attempt to fix the behavior described in #3046.\r\n\r\nInstead of just checking if the value inside the loader is not None in order to decide if a field from the initial item should be overwritten or not, `load_item()` should also make sure that the value returned by `get_output_value()` is not an empty list.\r\n\r\nThat is because `self._local_values` , which stores the new values included via `add_*` or `replace_*` methods, is a[`defaultdict(list)`](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L37). Then, when we call `get_output_value()` for a field only available in the initial item, an empty list will be set for that field in `self._local_values` (because of [this](https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L125)).\r\n\r\n\r\nThis way, we make sure we don't miss fields from the initial item, in case `get_output_value()` gets called for one of the pre-populated fields before `load_item()`, as described on #3046.", "author_association": "MEMBER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "exotfboy": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036", "title": "File downloaded by pipeline are blank", "body": "I have deployed a spider in my remote server, and I am using `FilePipeline` to download images for `item`, however I found that the downloaded image have size of  zero, which is not expected.\r\n\r\nThen I test the project in my local machine, it worked. So I think maybe my remote server has been banned, however when I tried `wget ..` I can download the image normally, and I also tried `scrapy sheel image_src` it still work.\r\n\r\nNow I have no idea what's going on , I just want to intercept the response of the request re-sent by the pipeline, is it possible?", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3036/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ReLLL": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034", "title": "Line-ends (unnecessary blank lines) problem in CSV export on Windows ", "body": "CSV export on Windows create unnecessary blank lines after each line.\r\n\r\nYou can fix the problem just by adding \r\nnewline='' \r\nas parameter to io.TextIOWrapper in the __init__ method of the CsvItemExporter class in scrapy.exporters\r\n\r\nDetails are over here:\r\nhttps://stackoverflow.com/questions/39477662/scrapy-csv-file-has-uniform-empty-rows/43394566#43394566", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3034/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3039", "title": "Fix for #3034, CSV export unnecessary blank lines problem on Windows", "body": "Fixed the issue I've mentioned there, this is the pull request to merge, (added one line), hope all is fine. \r\nhttps://github.com/scrapy/scrapy/issues/3034\r\n\r\nCloses #3034 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cp2587": {"issues": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029", "title": "Invalid DNS pattern", "body": "Hello,\r\n\r\nWe are using https proxies to crawl some website and sometimes i get the following stack trace:\r\n\r\n```\r\nError during info_callback\r\nTraceback (most recent call last):\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 315, in dataReceived\r\n    self._checkHandshakeStatus()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 235, in _checkHandshakeStatus\r\n    self._tlsConnection.do_handshake()\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1442, in do_handshake\r\n    result = _lib.SSL_do_handshake(self._ssl)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 933, in wrapper\r\n    callback(Connection._reverse_mapping[ssl], where, return_code)\r\n--- <exception caught here> ---\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1102, in infoCallback\r\n    return wrapped(connection, where, ret)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/scrapy/core/downloader/tls.py\", line 67, in _identityVerifyingInfoCallback\r\n    verifyHostname(connection, self._hostnameASCII)\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 44, in verify_hostname\r\n    cert_patterns=extract_ids(connection.get_peer_certificate()),\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 73, in extract_ids\r\n    ids.append(DNSPattern(n.getComponent().asOctets()))\r\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/_common.py\", line 156, in __init__\r\n    \"Invalid DNS pattern {0!r}.\".format(pattern)\r\nservice_identity.exceptions.CertificateError: Invalid DNS pattern '194.167.13.105'.\r\n```\r\n\r\nI think this issue is somewhat similar to https://github.com/scrapy/scrapy/issues/2092 and this 'invalid DNS pattern' error should be caught similarly as the 'Invalid DNS-ID'. What do you think ?\r\n\r\nIn the meantime, how can i catch it myself and silent it ?\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/3029/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kmike": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/7c9e32213db2ce757c784e7c29e30caa57dc3d48", "message": "Merge pull request #3059 from jesuslosada/fix-typo\n\nFix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/786144e0c7b25a02151b6d3135451da90e912719", "message": "Merge pull request #3058 from jesuslosada/fix-link\n\nFix link in news.rst"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/aa83e159c97b441167d0510064204681bbc93f21", "message": "Bump version: 1.4.0 \u2192 1.5.0"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d07fe11981a07e493faf7454db79b98c02a53118", "message": "set release date"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/c107059ef82a4b7b491b23b740b71353f03ab891", "message": "DOC fix rst syntax"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/d4e5671d07a8dcf18b665ed3ce4136dccae222fb", "message": "make release docs more readable, add highlights"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/45b0e1a0e4c51a773b39be14334a999cc5f0fe56", "message": "DOC draft 1.5 release notes"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/930f6ed8002e27c9d51f5d5abbb28c36460eb1bb", "message": "Merge pull request #3050 from lopuhin/pypy3\n\nAdd PyPy3 support"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/632f1cc07305d6967c9e8ce3bf150337e2bf3ecd", "message": "Merge pull request #3049 from scrapy/trove-classifiers\n\n[MRG+1] setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9f9edeadfc9d8d3422d4ca15b2b13d5b502ca70e", "message": "Merge pull request #3048 from lopuhin/pypy-install-docs\n\n[MRG+1] Mention PyPy support, add PyPy to install docs"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/1058169f0e3a8646dbd20f9b4c0b599ed9f6d08e", "message": "setup.py: mention that we support PyPy. See GH-2213."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/bdc12f39949731e69aaa32c36490e1b6e56ca98b", "message": "Merge pull request #3045 from hugovk/rm-3.3\n\n[MRG+1] Drop support for EOL Python 3.3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9aa9dd8d45a2ce0c8e6ae0732e610f020735df7e", "message": "DOC mention an easier way to track pull requests locally.\nThanks @eliasdorneles!"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f716843a66829350063e55f8df768eb538c6b05c", "message": "DOC update \"Contributing\" docs:\n\n* suggest Stack Overflow for Scrapy usage questions;\n* encourage users to submit test-only pull requests with reproducable examples;\n* encourage users to pick up stalled pull requests;\n* we don't use AUTHORS file as a main acknowledgement source;\n* suggest using Sphinx autodocs extension"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/4948548", "body": "whoops, I clicked github's Edit button and thought it will become a PR\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4948548/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104443", "body": "For me it is 2014-01-18 :)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": []}, "dangra": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/b170e9fb96dc763b9b78916e1557021e5e004d59", "message": "Merge pull request #2609 from otobrglez/extending-s3-files-store\n\nS3FilesStore support for other S3 providers (botocore options)"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/2dee191374e7fb7f2ed35ab9eea07ba7d1ee8b89", "message": "Merge branch 'master' into extending-s3-files-store"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/9b4d6a40a6d56acbd9e068e15e6717dc06aee79b", "message": "Merge pull request #3053 from scrapy/release-notes-1.5\n\nRelease notes for the upcoming 1.5.0 version"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/57d04aa9601bc237da4b08777327df241483b389", "message": "Merge pull request #2767 from redapple/http-proxy-endpoint-key\n\n[MRG+1] Use HTTP pool and proper endpoint key for ProxyAgent"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/86c322c3a819405020cd884f128246ae55b6eaeb", "message": "Merge pull request #3038 from scrapy/update-contributing-docs\n\nDOC update \"Contributing\" docs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355950", "body": "@pablohoffman: \"a global limit **and** a per-domain limit\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355950/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355969", "body": "\"to run **Scrapy** from a script\" right?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355969/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355981", "body": "\"if possible, use...\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2355981/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379551", "body": "The issue this commit tries to address is real, but the fix introduces a new bug when items contains unicode values not encodeable with default encoding.\n\nIt's a twisted shame that `log.err` doesn't call `_safeFormat` on `_why`  here http://twistedmatrix.com/trac/browser/tags/releases/twisted-12.3.0/twisted/python/log.py#L328\n\nThe good news is that we can call _safeFormat ourself on our `scrapy.log.err` wrapper\n\nwhat do you think?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379551/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379680", "body": "to make it more clear, this is what I am proposing https://gist.github.com/4445963\n\nit has an obvious dislike because of private function import and it doesn't improve on lazy evaluation side\n\nafter all, I am not sure. :sake: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379680/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379712", "body": "Now I figured out that previous Gist was an implementation of `format` instead of `_why`\n\nhere is the `_why` version https://gist.github.com/4446084 \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379712/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2966628", "body": "I guess you refer to Scrapy 0.16. is it the same case on Scrapy 0.18? \n\nThe motivation was to avoid hiding the real error that makes debugging images error a bit harder.\nCan you provide an test case that reproduces \"image file is truncated\" error and that is valid case to silence instead of propagating and logging the full traceback?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2966628/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976147", "body": "hey @a7ch3r, you welcome.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976147/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048440", "body": "unused import?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048440/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3049106", "body": ":beers:\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3049106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647682", "body": "@redapple: what about gzip,x-gzip,deflate?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647682/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647837", "body": "Is it time to move forward and remove `x-gzip` from default `Accept-Encoding`? :)\n\nChrome doesnt send x-gzip anymore:\n\n```\nGET / HTTP/1.1\nHost: example.com\nConnection: keep-alive\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.76 Safari/537.36\nDNT: 1\nAccept-Encoding: gzip,deflate,sdch\nAccept-Language: en-US,en;q=0.8,es-419;q=0.6,es;q=0.4\n```\n\nFirefox neither:\n\n```\nGET / HTTP/1.1\nHost: example.com\nUser-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:24.0) Gecko/20100101 Firefox/24.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nConnection: keep-alive\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4647837/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4956151", "body": "IDEA: as classes are declared in order, with parents been defined before childs, it's possible to set a class attribute into the first class built from this metaclass.\n\nand I think we can simplify finding the new class name by always pointing to the ~~first~~ second element of the MRO.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4956151/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104487", "body": "PYPI says it was released 2014-01-17 :) \n\n![image](https://f.cloud.github.com/assets/37369/1943916/f8783836-7fb1-11e3-9348-0ec972f5b626.png)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104487/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796", "body": "Can we avoid checking for \" xmlns \" in every loop iteration? it is an invariant check. move it to the top of the function.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810", "body": "alternative and less boilerplate: `inputs.extend(formdata.iteritems())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942", "body": "I tried removing the call to `_nons()` and still passed all tests.\nWhat about removing it completely of figuring out a test case that justifies its use? \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/675942/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002", "body": "not testing for string before splitting?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/847002/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "jesuslosada": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/61c0b1478284b02a4fcfd2cc4931587c348c5d3a", "message": "Fix typo in comment"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/a0836b8fd9720a9439cb3b940aca53b6844a094b", "message": "Fix link in news.rst"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "redapple": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/461f9daff5747728e26cd60e9dfe531092f58132", "message": "Update release notes for upcoming 1.4.1 version"}], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2906", "title": "[WIP] Downloader timings in request.meta", "body": "While working on a requests log plugin for Scrapy, I felt the need to get more fine-grained timing information on the various steps a `Request` goes through from reaching the `Downloader` to getting the associated response body.\r\nOne can work with `request.meta['download_delay']` and timing inserted from a downloader middleware but I found this additional information interesting for example for HAR output.\r\n\r\nNote: This does not address DNS lookup time nor TCP/TLS connection time.\r\n\r\nI would even set this to ON by default myself.\r\n\r\nMissing:\r\n\r\n- [ ] Documentation\r\n- [ ] How does this play with proxies?\r\n- [x] Handle download exceptions (DNS errors, dowload timeouts, connection errors etc.)", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/4617865", "body": "@pablohoffman , what would you think if the order was changed to `gzip,deflate,x-gzip`?\nI've found at least 1 web server that doesn't compress responses when `x-gzip` is first\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4617865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4650042", "body": "yes @dangra , `gzip,x-gzip,deflate` also works for this server\n(as does `gzip,deflate,x-gzip`)\n\nFrom wireshark:\n\n```\nGET / HTTP/1.1\nHost: ...\nAccept-Language: en\nAccept-Encoding: gzip,x-gzip,deflate\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Scrapy/0.21.0 (+http://scrapy.org)\n```\n\n```\nHTTP/1.1 200 OK\nCache-Control: no-cache\nPragma: no-cache\nContent-Type: text/html; charset=utf-8\nContent-Encoding: gzip\nExpires: -1\nVary: Accept-Encoding\nServer: Microsoft-IIS/7.5\nSet-Cookie: ASP.NET_SessionId=xyps42n5jtpprtrormg45q45; path=/; HttpOnly\nX-AspNet-Version: 2.0.50727\nX-Powered-By: ASP.NET\nDate: Wed, 20 Nov 2013 09:47:54 GMT\nContent-Length: 11786\n\n.............`.I.%&/m.{.J.J..t...`.$..@.........iG#).*..eVe]f.@......{....{....;.N'...?\\fd.l..J...!....?~|.?\"~..7N....j.^..t...#|.....<-...4}.~.(}...2[^|.Q..(..Y.|..w^?-.lR.3j..G..w.^..........(.....M......&@.....A@[.L..w~.w.:;...#|A....E.$P..]=.{...j|uo\\..ww.>|x...D.....;...lF?...7N~.>h....^..U....y..M.~./?n.l9K.[-..gY3/.e...,.y..>/......y..x|W.0#,.6#&hW../Z...}4..m.l.A.uv..........c?&.-.E..Go.....5^......7...n..w...^........F..|Y,..)..y=Ji.\nQ._..W...w%.j..+. ..,o.u.j.\n.^L9.E$.....~..R.Y..k..d._1.K.xV..eV\n.j..M.5.`-.+.h..2o...:.......j4/.a.(..2%..u.V.y..n...8.a\\.D...jo...IY..5.'.._dK.'.J...&........F3...{.~`...3}.<.....u[y... @O....V$\"....rc...!]S...h...g.`..8[..7.9i:..u^~...u.N.mZ.f..P].}T,h.w.m.g..?k..g..tL..@..m>..\n.pN.l..UuQ...h...i......(.?.r./?y.-.G.;;.3\".......X......x......{......\n...\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/4650042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "raphapassini": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/a1cc5a63d3e253c325159fdc6ebf4cd3faa37c49", "message": "Add mention to dont_merge_cookies in CookiesMiddlewares docs (#2999) (#3030)\n\nAdd mention to dont_merge_cookies in CookiesMiddlewares docs (#2999)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lopuhin": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/bb1f31189128cb2272c1302350387075fbbb730a", "message": "Add PyPy3 support to faq and install doc"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/041308afe7c40de7088f75b0e0c312ecd5de428a", "message": "Fix get_func_args test for pypy3\n\nThese built-in functions are exposed as methods in PyPy3.\nFor scrapy this does not matter as:\n1) they do not work for CPython at all\n2) get_func_args is checked for presense of an argument in scrapy,\n   extra \"self\" does not matter.\nBut it still makes sense to leave these tests so that we know we\nshouldn't use get_func_args for built-in functions/methods."}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f71df6f9addca10b562bb22890b5ea1c37efde5c", "message": "Run tests for PyPy3"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/ea41114cf0ab2782650792ad204cf43fc148c749", "message": "Mention PyPy support, add PyPy to install docs"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hugovk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/scrapy/scrapy/commits/cbcf80b98ff66db1ccf625fa52c4de8935331972", "message": "Fix typo\n\n[CI skip]"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/f11c21c6fc62b64a2bbee0e19e2098ed6257cf19", "message": "Test on Python 3.4"}, {"url": "https://api.github.com/repos/scrapy/scrapy/commits/44623687ab8936c5696f68f74e438a2891880c82", "message": "Drop support for EOL Python 3.3"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tchiotludo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3072", "title": "Handle Webp Image transparency", "body": "Webp transparent image have a dark background : \r\n\r\nreference image : https://www.lavera.de/typo3temp/fl_realurl_image/105115-4021457609482-glossylips-deliciouspeach09-1294b-43402bf213-6cf9c.png", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BrandonSmithJ": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3062", "title": "Use as a library: Reactor decorator code + MWE", "body": "First, let me say thanks for the extremely useful tool - it really has made a lot of things much simpler than they otherwise would have been. \r\n\r\nWith that in mind, I'd like to give back what I can if it's useful. The problem I'm having is I don't know exactly where to post this. I'm not familiar enough with scrapy internals to suggest where to put the feature, or if it's even necessary enough to put into the main code base. It also likely needs one more modification before being ready for a main branch contribution. \r\n\r\nThe problem this solves is something at least a few people seem to have run into:\r\n\r\n- https://stackoverflow.com/questions/35289054/scrapy-crawl-multiple-times-in-long-running-process\r\n\r\n- https://stackoverflow.com/questions/22116493/run-a-scrapy-spider-in-a-celery-task/22202877#22202877\r\n\r\nMy use case is a website which requires a virtually endless process able to use various scrapy functionalities at whim. To solve the problem of restarting the twisted reactor, I essentially took the solutions I could find and rolled everything into an extremely simple decorator. Basically all the end-user needs to do is label the function they'd like to run as a scrapy function, and the decorator handles everything necessary in order to use scrapy in a library capacity:\r\n\r\n```\r\n\t@reactor_process(timeout=10)\r\n\tdef execute(self, keyword_list):\r\n\t\t''' Crawl the site specifically for certain keywords '''\r\n\t\tfrom crawlers.backend.spider_interface import construct_spider\r\n\t\tfrom scrapy.crawler import CrawlerProcess\r\n\r\n\t\tspider = construct_spider(self)\r\n\t\tspider.create_start_urls(keyword_list)\r\n\t\t\r\n\t\tcrawler  = CrawlerProcess(self.settings)\r\n\t\tdeferred = crawler.crawl(spider)\r\n\t\tdeferred.addBoth(lambda _: crawler._stop_reactor())\r\n```\r\n\r\nWithout the decorator, the function above would only be able to run once - the reactor engine would complain about being restarted. The one problem is that the necessary imports have to be made *within* the function itself - though I think this can be solved with a multiprocessing manager (or worst case, global inspection). \r\n\r\nI also have an interface which allows dynamic creation of contract docstrings / crawlers with different settings inside the same process, which might be useful; but that's something that would probably be better as its own discussion. \r\n  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Matthijsy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3061", "title": "Make autothrottle slow down on HTTP 429 response", "body": "When a response has status 429 (Too Many Requests) it is ignored by the AutoThrottle (because the latency is low, so otherwise the spider will speed up). I think that the wanted behaviour is that the spider will slow down when a 429 is received, but currently the spider will stay at a constant scraping speed. \r\n\r\nThis PR adds a new setting `AUTOTHROTTLE_429_DELAY` when it is set the download delay will be increased with this value every time a 429 is received. It still respects the 'MAX_DOWNLOAD_DELAY` value.  ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "parlays": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3051", "title": "The URI gets set again before feed export store gets called.  This al\u2026", "body": "\u2026lows the spider to change settings in the parse method based on the content scraped.  The IFeedStorage interface has been changed for the store method to allow uri to be passed.  The BlockingFeedStorage class now sets the new uri settings in the store method.\r\n\r\nIn my personal scraper I overwrite the FeedExporter object and S3FeedStorage object.  But it would be great if this was part of the core.  It may be a tough change though because it requires a change to the IFeedStorage interface and that is a breaking change for older code.\r\n\r\nIt is common for me to scrape a site and then decide the folder name to use in the S3 bucket.  To be able to change the bucket name or directory in the parse method is important.  If this change is not agreed upon, let's think of another way to accomplish this.  Thanks!", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "munderseth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3041", "title": "Add Test Reporting to Travis CI", "body": "**Test Reporting for Travis using [Testspace.com](https://testspace.com)**\r\n\r\nHi `scrapy` team. We added test reporting to your repo. We have a blog article on [why we are staging repos](https://blog.testspace.com/testspace-and-open-source).\r\n\r\nNote that we contacted you in the past via an email. We thought it would be easier for you (if interested) by using a Pull Request.   \r\n\r\nFew of the benefits using Testspace:\r\n- view all your test results from a single dashboard\r\n- triage and manage your test failures faster\r\n- add more metrics\r\n- leverage built-in analytics \r\n- get a [test badge](https://help.testspace.com/how-to:get-badge) \r\n[![Space Health](https://open.testspace.com/spaces/74470/badge?token=2f48b759d52023674602ea42e90a5508cad6c953)](https://open.testspace.com/spaces/74470?utm_campaign=badge&utm_medium=referral&utm_source=test \"Test Cases\")\r\n\r\nCheckout **your** test results: https://open.testspace.com/projects/TryTestspace:scrapy from our fork.\r\n\r\n**Why** we are doing this: https://blog.testspace.com/testspace-and-open-source \r\n\r\n----\r\n\r\n**Give Testspace a try?**\r\n\r\n1. Create **Open** (free) account: https://testspace.com/pricing\r\n2. Add a **New Project** from the list of Github repo's\r\n3. Add a Travis **Environment Variable**: Name: `TS_ORG`  Value: `organization name` - based on subdomain selected in step 1\r\n4. Invite others: https://help.testspace.com/how-to:invite-other-users\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslay88": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3040", "title": "Fix for Issue #2919", "body": "Splits out url params and updates them with formdata. Passes local tests.\r\n\r\nFirst PR, please be nice :)\r\n\r\nFixes Issue #2919 ", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mylh": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3028", "title": "Update practice.rst docs", "body": "added tips to avoid getting banned", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chainly": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3027", "title": "fix Spider.log to record right caller information", "body": "Spider.log always log itself in `pathname), funcName, lineno, etc.` format. Because `currentframe = lambda: sys._getframe(3)` and `f = f.f_back` in ``logging.Logger.findCaller`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ghost": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/3016", "title": "Added Scrapy logo", "body": "I replaced the \"Scrapy\" text with the logo shown on the Scrapy website", "author_association": "NONE"}], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web crawling.\nI thought that you guys would be doing that in multithreading but I heard that you guys never use any threads.How could this be possible?can anyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135769120/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128", "body": "@gmeans code gives warning:\r\n\r\n /usr/local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py:51: builtins.UserWarning:\r\n         'broadd.context.CustomContextFactory' does not accept `method` argument (type OpenSSL.SSL method, e.g. OpenSSL.SSL.SSLv23_METHOD). Please upgrade your context factory class to handle it or ignore it.\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312946128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454", "body": "@redapple I had errors:\r\n`<twisted.python.failure.Failure OpenSSL.SSL.Error: ('SSL routines', 'SSL3_READ_BYTES', 'sslv3 alert handshake failure'), ('SSL routines', 'SSL3_WRITE_BYTES', 'ssl handshake failure')>` \r\nwith all packages up-to-date", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312947454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745", "body": "Scrapy    : 1.4.0\r\nlxml      : 3.8.0.0\r\nlibxml2   : 2.9.4\r\ncssselect : 1.0.1\r\nparsel    : 1.2.0\r\nw3lib     : 1.17.0\r\nTwisted   : 17.5.0\r\nPython    : 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 17:54:52) [MSC v.1900 32 bit (Intel)]\r\npyOpenSSL : 17.1.0 (OpenSSL 1.1.0f  25 May 2017)\r\nPlatform  : Windows-10-10.0.15063-SP0\r\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312956745/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674", "body": "@redapple Thank you", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312957674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674", "body": "Hey guys I'm new to web crawling.Sorry for disturbing you guys.\nI came to know that scrapy is the fastest web framework module for web \ncrawling.\nI thought that you guys would be doing that in multithreading but I \nheard that you guys never use threading.How could this be possible?can \nanyone give a short explanation to me...Please\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/135768674/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160", "body": "Thank you, missconfiguration.", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312883160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683", "body": "https://github.com/kjd/idna/issues/50#issuecomment-312908539", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/312909683/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "isra17": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2996", "title": "Add signals to handle error from downloader or pipeline", "body": "As of right now, there's is no simple way to handle exception coming from items pipeline or a the `process_response` of a downloader middleware.\r\n\r\nThis PR adds two signals for those use case. Note that the downloader signals also catch any exception from the downloader engine, including `process_request` and `process_exception`.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2995", "title": "Allow passing Failure object to middlewares", "body": "Trying to handle errors coming from middleware, it happens that Twisted strip traceback from an exception returned from a completed deferred (Needed to avoid GC issues). This mean that trying to use `exception.__traceback__` always yield `None`.\r\n\r\nThis PR adds a decorator that can be used on middleware `process_exception` or `process_spider_exception` to get a Failure object instead of an Exception. The Failure object provides a `getTracebackObject` to get a `Traceback`-like object which come really handy when trying to pinpoint an issue.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2962", "title": "Add signal `request_downloading` called right before the download handler", "body": "It was needed for one of our project, maybe it can be useful for core as well.\r\n\r\nThe `request_downloading` signal is sent when the engine is about to download a request. If one handler raise an exception, the download is aborted. The signal supports returning deferreds from their handlers.\r\n\r\nThis is an alternative to the Downloader middlewares where there might be a significant delay between the middleware call and the download handler in case of slow queue processing.\r\nThis even handler allow some extension to tamper the request right before the download time and possibly cancel the request by raising an exception.\r\n\r\n`signal.send_deferred` was needed for the exception handling raised by signal handler in the download manager.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "starrify": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2988", "title": "Added: Customizing request fingerprint calculation via request meta", "body": "Partially implements #900", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2984", "title": "Added: Making the list of exceptions to retry configurable via settings", "body": "Implements #2701", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "immerrr": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2986", "title": "WIP: CookiesMiddleware: add \"reset_cookies\" meta to clear the jar", "body": "This PR adds a `reset_cookies` meta to clean the active cookiejar.\r\n\r\nWhen I try working with sessions I often find myself in a situation when I'd like to restart the session \"from scratch,\" including cookies. It's doable by just setting `cookiejar` meta to an arbitrary value, but then we'd be accumulating the cookiejars over time.\r\n\r\nI'm not entirely sure about the meta name, as I'd probably like it namespaced, e.g. `cookies_reset`, but there's a precedent already with `dont_merge_cookies`, so I chose to follow that pattern.\r\n\r\n@kmike am I missing something here?", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2985", "title": "utils.curl: add parse_curl_cmd func", "body": "Given that the major browsers are able to export the requests in cURL format, it's logical that we have a utility in scrapy to make it a request.\r\n\r\nThis PR works towards adding a function that creates kwargs that could be passed to a `Request` constructor.", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2950", "title": "loader add_* funcs: pass **kw to self.selector.xpath", "body": "I have found out that kwargs that one specifies for `loader.add_xpath` are not getting to the actual `selector.xpath` invocation, which looks like a bug.\r\n\r\nI wonder if there's code out there that would be broken by this fix...", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aitoehigie": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2972", "title": "Add a note to allowed_domains", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "NoExitTV": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2955", "title": "Handle \"invalid\" relative URL issue #1304", "body": "Did some minor tweaks on how scrapy handle relative URL's as discussed in #1304 \r\n\r\nTested it with some basic code in the scrapy shell:\r\n```\r\n>>> resp = scrapy.http.response.html.HtmlResponse('http://www.example.com',\r\n...      body='''<html>\r\n...                  <body>\r\n...                      <a href=\"../index1.html\">Link1</a>\r\n...                      <a href=\"../../index2.html\">Link2</a>\r\n...                      <a href=\"index3.html\">Link3</a>\r\n...                      <a href=\"other_html/../index4.html\">Link4</a>\r\n...                      <a href=\"other_html/folder2/../index5.html\">Link5</a>\r\n...                      <a href=\"other_html/index6.html\">Link6</a>\r\n...                      <a href=\"../other_html/index7.html\">Link7</a>\r\n...                 </body>\r\n...          </html>''')\r\n>>> links = scrapy.linkextractors.LinkExtractor().extract_links(resp)\r\n>>> links\r\n[Link(url='http://www.example.com/index1.html', text='Link1', fragment='', nofollow=False), Link(url='http://www.example.com/index2.html', text='Link2', fragment='', nofollow=False), Link(url='http://www.example.com/index3.html', text='Link3', fragment='', nofollow=False), Link(url='http://www.example.com/index4.html', text='Link4', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index5.html', text='Link5', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index6.html', text='Link6', fragment='', nofollow=False), Link(url='http://www.example.com/other_html/index7.html', text='Link7', fragment='', nofollow=False)]\r\n```\r\n\r\nPassed the same unit tests as the original code when running tox\r\n```\r\n========================================================= 1679 passed, 6 skipped, 14 xfailed in 458.89 seconds =========================================================\r\n\r\n_______________________________________________________________________________ summary ________________________________________________________________________________\r\n\r\n  py27: commands succeeded\r\n  congratulations :)\r\n```\r\n\r\nI believe that scrapy now handle relative url's as expected in python 2.7.13\r\nWhat are your thoughts?", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kaplun": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2954", "title": "spiders: add OAI-PMH support WIP", "body": "Signed-off-by: Samuele Kaplun <samuele.kaplun@cern.ch>", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phnk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2953", "title": "Added debug message in spiders/crawl", "body": "As requested in #2925.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mGalarnyk": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2931", "title": "Update install.rst", "body": "Updated installation instructions for installing scrapy using conda.\r\n\r\nYou can now just do: \r\n\r\n```\r\nconda install scrapy\r\n```", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "HarrisonGregg": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2918", "title": "Fix SIGINT handling when using inspect_response", "body": "Currently, after calling `scrapy.shell.inspect_response` and then closing the opened shell, SIGINT (Ctrl-C) no longer works to terminate the spider.  This is because `Shell.start`, called in `inspect_response`, removes the signal handler.  To fix this, save the SIGINT handler before calling `Shell.start` in `inspect_response` and add it again after the shell has closed.\r\n\r\nIt doesn't appear that there are any tests for `inspect_response`, so I haven't added any tests for this fix.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wangtua1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2917", "title": "support giantfiles download ,fix CVE-2017-14158", "body": "from the issue https://github.com/scrapy/scrapy/issues/482.\r\nWe can see if we download a file with filespipeline, the whole file is stored into the memory.\r\nIf the file is a giant file or an extremely giant file,(over 1G or more), the crawling thread will be crashed.\r\nWe can use this POC to prove it.\r\n[POC.zip](https://github.com/scrapy/scrapy/files/1289514/POC.zip)\r\n\r\nThis has been asigned CVE-2017-14158.\r\nbut in this new testcase using GiantFilesPipeline in the commit,we can download it correctly.\r\n\r\n[testdownload.zip](https://github.com/scrapy/scrapy/files/1289512/testdownload.zip)\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhaojiedi1992": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/2911", "title": "Update exporters.rst", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfdeshom": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118", "body": "Hi, to disable `scrapy.contrib.feedexport.FeedExporter`, I had to set it to `None` in my `EXTENSIONS` dict instead of `0`. I am getting this error when using scrapyd.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392118/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185", "body": "off-topic but: in general to disable any extension, middleware, etc I have to set it to `None`, which can be a little confusing since you would think that `0` would work too\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/5392185/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375", "body": "@dangra unfortunately, I don't see a documented way to disable an extension and I consider myself pretty familiar with scrapy and its docs. Maybe it's already there in the docs, but simply needs to be more prominent.\n\nFor me there is a larger issue: some extensions have settings associated with them that make it confusing to know exactly how/where to disable them. For example, the cookies extension seems to have 2 ways to disable it:\n-  set `COOKIES_ENABLED` to `False` (https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/cookies.py)\n- set the priority to `None`, ie set `scrapy.contrib.downloadermiddleware.CookiesMiddleware` to `None`\n\nIt would be nice to have just one way to disable extensions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/6111375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "NicolasP": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831", "body": "Hi, any chance for a review of this PR?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/179992831/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "knaveofdiamonds": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951", "body": "@kmike - ah, ok. I'm working on an inherited codebase that doesn't use the `LinkExtractors` directly, so this was obviously just a misunderstanding of the API/using something non-public. I'll close this issue - agree with backwards incompatible change reasons.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/220300951/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "nside": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706", "body": "Seeing the same issue\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/29349706/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703", "body": "@dangra crawl() fixed it for me. Still I'd expect any requests scheduled to have the same treatment, whatever their \"entry point\" in the pipeline is. Feel free to close if you disagree.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32618703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941", "body": "I'm on 0.21 (dev). These are good subtleties to know!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/32620941/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606", "body": "Hi,\n\nI've copied a URL below which failed with the original function because it has no url argument and the arguments are in an unexpected order. I can certainly add some unit tests, I'll have a look into it now. It's the end of the day and I want some food!\n\nCheers,\nRob\n\nhttp://www.firstchoice.co.uk/fcsun/page/search/searchresults?sttrkr=mthyr:02/2011_durT:7/n_ls:true_tuidesc:000832_day:15_mps:9_isvid:false_pconfig:1|2|0|0|0|_tchd:0_rating:0_act:0_jsen:true_resc:_attrstr:||||||||null|null_mdest:false_tinf:0_mnth:02_desc:_bc:17_margindt:7_tadt:2_numr:1_spp:mainSearch_depm:7_tuiresc:004287_dur:7_dtx:0_df:false_dxsel:0_imgsel:0_dac:MAN_loct:0_tsnr:0_year:2011_dta:false_tuiacc:028367_acc:\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/736606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615", "body": "Which distro are you using? Here in ubuntu, works fine. Try:\n\napt-get install -y build-essential python-dev\napt-get install -y libssl-dev libxml2-dev libxslt1-dev libssl-dev libffi-dev\n\nthen pip install scrapy in a brand new virtualenv\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/65951615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982", "body": "Let me try again:\n\nI want to use scrapy like this:\n\n```\n# prints options\nscrapy crawl -t csv|sql|mongo|etc -h\n\n# uses a particular item exporter\nscrapy crawl -t csv -f somefile.csv\nscrapy crawl -t mongo --db somedb --col somecollection\n```\n\nWhat do you guys think of this? Is it desired behavior? Does not feel like a whole lot of code to modify. I could do it. \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/66092982/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "ariddell": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661", "body": "@nramirezuy there's a reference implementation for pep 3156 here: https://code.google.com/p/tulip/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/18024661/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "sabren": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629", "body": "Here you go: \n\nhttps://github.com/scrapy/scrapy/pull/45\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/2305629/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490", "body": "Thanks, Scotty! \n\nI'm sure my client would appreciate it.\n\nCan you make a combined pull request, or should I pull from you and open another pull request here?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/165490/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "mvj3": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848", "body": "Thanks @curita for the careful review! I correct it in a new commit.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/156869848/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "RFDAJE": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937", "body": "I had been facing the same issue, so far the simplest solution I found is inside `item_completed`, after getting all things done, reset downloaded to empty. `self.spiderinfo.downloaded = {}`, memory leak issue resolved. ", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/314142937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "paulproteus": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573", "body": "I did some work refreshing these patches against current origin/master.\n\nhttps://github.com/paulproteus/scrapy/tree/revise-pullrequest-109\n\nSome notes:\n\nSee 3077ac8b6f592b044ad67f15af3b065d06f27cf7 for a fix where Http11DownloadHandler needs to accept second argument, since that's how the tests use it.\n\nSee 888dfadd24f7d325bffab367acc9dd5a7405e5bc for a fix where the call to log.err() creates an exception in the log that test runner notices, so tests that call this method begin to fail. For now I've disabled the call to log.err(). If we want to keep logging the error, we'll need to call flushLoggedErrors() -- see http://twistedmatrix.com/documents/current/core/howto/trial.html#auto11 . (The test this makes fail is scrapy.tests.test_downloader_handlers.Http11TestCase.test_timeout_download_from_spider )\n\nCommit bdd2a1b02c944845fec4d6142fcb63367b17cc11 (rebased from the original, but otherwise the same) makes many tests fail because there is work left in the reactor. One test you can see this with is scrapy.tests.test_engine.EngineTest.test_crawler\n\nFor now I can't promise I'll have time to figure out what's going on with leaving the reactor unclean, but I thought I'd leave a comment with what I have found in a few hours of looking into this pull request!\n\n-- Asheesh.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/8343573/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "rmax": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800", "body": "#62 is a duplicated report of this issue but includes a pull request with a fix.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3100800/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083", "body": "See issue #58\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3011083/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355", "body": "As a workaround you can use `process_value` argument:\n\n`SgmlLinkExtractor(..., process_value=lambda v: v.strip())`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3418355/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580", "body": "I have reverted the redirect middleware order and added tests for gzipped meta redirection. Although I'm not sure about leaving the body compressed if it fails, I think there could be more errors (ie. connection drop) that could make fail the decompression and leaving the body as is could produce misbehavior in other components.\n\nAdding more tests for the middlewares integration could help us to identify the best solution.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764580/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823", "body": "without any fix fails test_gzipped_redirect_30x\nwith the redirect reorder fails test_gzipped_meta_redirect\nwith dangra's suggestion passes all tests (See https://gist.github.com/1718659)\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/3764823/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882", "body": "Nice!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/issues/comments/12075882/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "djm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/426681", "body": "@pablohoffman Cheers!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/426681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "aalvarado": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/749296", "body": "```\nfound in the dmoz directory\n```\n\nI think this wasn't updated. Should say `tutorial` now.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/749296/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "mohsinhijazee": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2180902", "body": "The settings have FEED_URI whereas here it is picking up from SCRAPY_FEED_URI. I think this is kind of conflicting  here.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2180902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "pablohoffman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2184054", "body": "When you access `settings['FEED_URI']` Scrapy ends up looking at the `SCRAPY_FEED_URI` environment variable.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2184054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379561", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2379561/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701990", "body": "Indeed, shorter and faster. Change pushed, thanks!\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701990/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863", "body": "Is there a way to avoid calling a protected method of lxml.html?. As this may raise some compatibility issues on future/past lxml versions.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/670863/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/676433/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702", "body": "touple -> tuple\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455702/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727", "body": "should we make these settings dicts (like extension and middlewares) and use `scrapy.utils.conf.build_component_list` to load them?.\n\nany particular reason why you went with lists instead?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455727/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737", "body": "I think this should inherit from `AssertionFailure`, to appear as test failures (and not errors) in some testing frameworks.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455737/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755", "body": "I think I would prefer two contracts for these:\n\n```\n@minitems 1\n@minrequests 2\n```\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455755/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772", "body": "Why do we need this method for?. I don't see it used, and we already have `adjust_request_args`\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455772/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784", "body": "what about just ignoring those who don't?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455784/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786", "body": "perhaps in the future we can add a way to list contracts per spider\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455786/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791", "body": "I'm not sure a separte register method is needed. How about the constructor receiving the contracts? Like middlewares and extension do. See `scrapy.middleware` module.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455791/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796", "body": "these changes should go into a separate PR\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1455796/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460", "body": "I think we should reuse the convention of using dicts. (we were planning to port pipelines too).\n\nIt's true that priorities won't add much in in this case, but it's not only priorities what the dict mechanism provides, but also being to disable specific components of the (presumably most sensible default). Priorioties wont' add much to this (as they don't to extensions) but don't harm and it could be useful if someone ever needs them. We should make sure internal code in an intuitive behaviour. Like pre_process being called in priority-order.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465460/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558", "body": "I wasn't considering all cases. In that case, how about this:\n\n```\n@returns <type> [M [N]]\n```\n\nHere are some examples to illustrate:\n- `@returns request` - returns at least one request\n- `@returns request 2` - returns at least two requests\n- `@returns request 2 10` - returns at least two requests and no more than 10 requests\n- `@returns request 0 10` - returns no more than 10 requests\n\nSame goes for items.\n\nMaybe make aliases (`request` -> `requests` , `item` -> `items`) so that both plurals and singulars work.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1465558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903", "body": "I can't see that comment on github now, did you remove it?\n\nOn Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi\nnotifications@github.comwrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> \n> @returns requests 2 - returns at least two requests\n> \n> -- wouldn't this be considered unexpected behaviour?\n> \n> \u2014\n> Reply to this email directly or view it on GitHubhttps://github.com/scrapy/scrapy/pull/167/files#r1570864.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570903/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908", "body": "this one looks very similar to `scrapy.utils.get_func_args` (which is tested and supports methods, function, classes, etc) - could that one be reused?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614908/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916", "body": "`from scrapy.conf import settings` is deprecated  API, use `self.settings` within the command methods,  that attribute is assigned by the `scrapy.cmdline` mechanics.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1614916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "nuklea": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701979", "body": "`isinstance(arg, (dict, BaseItem))` short.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2701979/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155", "body": "What about pep8?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1245155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brunsgaard": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2853556", "body": "Ahh yeah.. good point. I will write up another commit in the near future taking the settings instance from the crawler. Had totally forgotten about overrides :/\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2853556/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "archerhu": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/2965291", "body": "can you explain why you remove the try catch?\n\nat 1.6 version, using ImagePipeline may raise a lot \"exceptions.IOError: image file is truncated\"\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2965291/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976024", "body": "I get your\u00a0motivation,thanks very much\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/2976024/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "nramirezuy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048968", "body": "ssh! :dancer: \n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/3048968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104492", "body": "It is 2014-01-17 on UTC :P\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/comments/5104492/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}], "review_comments": []}, "scottyallen": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154", "body": "Thanks for the patch - I was in the process of trying to fix this, and it saved me a ton of time:)  However, I don't think line 191 is quite right for the tunnel case.  It results in sending a GET request with the full url to the destination webserver, which is technically wrong and some sites refuse to handle.  Instead, self.path should remain unchanged for the tunnel case.  I can send a patch to your patch, if you like...\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/163154/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "alexcepoi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975", "body": "what about maxitems, maxrequests? Or the case where you expect to receive exactly one request (the original sep describes a returns_request contract which checks this).\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1457975/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004", "body": "Ignoring sounded unfriendly to me at first (i.e. you may wonder why it does not work). Also eliminating the assertion gives a very misleading error.\nListing contracts sounds good to me maybe a \"--list / -l\" option?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458004/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025", "body": "This is a reminiscence from the original implementation. I thought it could prove useful, but now that I think about it's hard to find a scenario in which adjust_request_args is insufficient.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458025/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066", "body": "I did not think priorities would be useful, or that we should encourage users to rely on contracts priorities.\nEspecially since for some hooks (pre_process comes into mind) the last contract hooked in is the first one to be executed.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458066/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090", "body": "note: also change docstring\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1458090/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864", "body": "`@returns requests 2` - returns at least two requests\n\n-- wouldn't this be considered unexpected behaviour?\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570864/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929", "body": "It's an outdated diff (i.e. comment on a line of code which has been modified). You should still be able to find it in the issue\n\nOn Sep 10, 2012, at 11:30 PM, Pablo Hoffman notifications@github.com wrote:\n\n> In scrapy/contracts/default.py:\n> \n> > +# contracts\n> > +class UrlContract(Contract):\n> > -    \"\"\" Contract to set the url of the request (mandatory)\n> > -        @url http://scrapy.org\n> > -    \"\"\"\n> >   +\n> > -    name = 'url'\n> >   +\n> > -    def adjust_request_args(self, args):\n> > -        args['url'] = self.args[0]\n> > -        return args\n> >   +\n> >   +class ReturnsContract(Contract):\n> > -    \"\"\" Contract to check the output of a callback\n> > -        @returns items, 1\n> > -        @returns requests, 1+\n> >   I can't see that comment on github now, did you remove it? On Mon, Sep 10, 2012 at 6:24 PM, Alexandru Cepoi notifications@github.comwrote:\n> >   \u2026\n> >   \u2014\n> >   Reply to this email directly or view it on GitHub.\n", "reactions": {"url": "https://api.github.com/repos/scrapy/scrapy/pulls/comments/1570929/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}}}