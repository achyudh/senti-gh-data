{"_default": {"1": {"lvc": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/484", "title": "API changes report for Elephant Bird Core", "body": "Hi,\r\n\r\nI'd like to share my results on API changes analysis for the library: https://abi-laboratory.pro/java/tracker/timeline/elephant-bird-core/\r\n\r\nThe report is generated by the https://github.com/lvc/japi-compliance-checker tool for jars found at http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-core/ according to https://wiki.eclipse.org/Evolving_Java-based_APIs_2.\r\n\r\nThank you.\r\n\r\n![elephant-bird-core-2](https://user-images.githubusercontent.com/1517837/31162064-f249ba5c-a8e3-11e7-82c4-53da2655da25.png)\r\n![elephant-bird-core-1](https://user-images.githubusercontent.com/1517837/31162065-f24e666a-a8e3-11e7-8ce8-9d41580f9f4c.png)\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "sreekuppa": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/475", "title": "Build Issue. ", "body": "I am trying to install twitter's Elephant Bird parser and I am following the instructions on the official github page. But everytime, the build fails. \n1. I used this link https://leveragebigdata.wordpress.com/2015/10/08/install-protocol-buffer-2-5-0/  to install protobuf 2.4.1.\n2. mvn -Dmaven.test.skip=true package inside elephant-bird directory.\n\n```\n[INFO] Building Elephant Bird Pig 4.15-SNAPSHOT\n[INFO] ------------------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] Elephant Bird ..................................... SUCCESS [0.304s]\n[INFO] Elephant Bird Hadoop Compatibility ................ SUCCESS [0.598s]\n[INFO] Elephant Bird Core ................................ SUCCESS [2.513s]\n[INFO] Elephant Bird Cascading2 .......................... SUCCESS [0.042s]\n[INFO] Elephant Bird Cascading3 .......................... SUCCESS [0.034s]\n[INFO] Elephant Bird Cascading Protobuf .................. SUCCESS [0.024s]\n[INFO] Elephant Bird Crunch .............................. SUCCESS [0.101s]\n[INFO] Elephant Bird Hive ................................ SUCCESS [0.114s]\n[INFO] Elephant Bird Pig ................................. FAILURE [0.045s]\n[INFO] Elephant Bird Mahout .............................. SKIPPED\n[INFO] Elephant Bird RCFile .............................. SKIPPED\n[INFO] Elephant Bird Lucene .............................. SKIPPED\n[INFO] Elephant Bird Pig Lucene .......................... SKIPPED\n[INFO] Elephant Bird Examples ............................ SKIPPED\n[INFO] --------------------------------------------------------------------- ---\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------ \n[INFO] Total time: 4.104s\n[INFO] Finished at: Mon Oct 17 13:43:13 EDT 2016\n[INFO] Final Memory: 24M/750M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal on project elephant-bird-pig: Could not resolve dependencies for project com.twitter.elephantbird:elephant-bird-pig:jar:4.15-SNAPSHOT: Failure to find com.twitter.elephantbird:elephant-bird-core:jar:tests:4.15-SNAPSHOT in http://maven.twttr.com was cached in the local repository, resolution will not be reattempted until the update interval of twitter has elapsed or updates are forced -> [Help 1]\n\nNot sure what to do. Someone pls help. I have thrift version 0.7.0 and protobuf 2.4.1. I am running all this on centos 7\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/475/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "heapxor": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/474", "title": "compilation issue", "body": "Hi,\nIm desperately trying to install the git version of elephant-bird; latest thrift+protobuf were installed accordingly. \n\nI executed mvn package and got error below\n\n[ERROR] Failed to execute goal com.github.igor-petruk.protobuf:protobuf-maven-plugin:0.6.5:run (default) on project elephant-bird-core: 'protoc' failed. Exit code 0 -> [Help 1]\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.github.igor-petruk.protobuf:protobuf-maven-plugin:0.6.5:run (default) on project elephant-bird-core: 'protoc' failed. Exit code 0\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:217)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)\n        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)\n        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)\n        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)\n        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)\n        at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)\nCaused by: org.apache.maven.plugin.MojoExecutionException: 'protoc' failed. Exit code 0\n        at com.github.igor_petruk.protobuf.maven.plugin.RunMojo.printErrorAndThrow(RunMojo.java:430)\n        at com.github.igor_petruk.protobuf.maven.plugin.RunMojo.printErrorAndThrow(RunMojo.java:434)\n        at com.github.igor_petruk.protobuf.maven.plugin.RunMojo.detectProtobufVersion(RunMojo.java:405)\n        at com.github.igor_petruk.protobuf.maven.plugin.RunMojo.execute(RunMojo.java:230)\n        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)\n        ... 19 more\n[ERROR]\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n\nThanks\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/474/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "manoharballia": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/473", "title": "Lucene merge multiple indexes into single using HdfsMergeTool Failing", "body": "I am trying to merge multiple solr collections into single using provided tool HdfsMergeTool. But i get the bellow error:\nUsed command :  hadoop jar HdfMergeSolr.jar /work/slr_merged_1_and_2 2 /solr/collection_1 /solr/collection_2\n\nError Log:\n16/09/21 09:28:01 INFO mapreduce.Job: Job job_1469767030336_14546 running in uber mode : false\n16/09/21 09:28:01 INFO mapreduce.Job:  map 0% reduce 0%\n16/09/21 09:28:09 INFO mapreduce.Job: Task Id : attempt_1469767030336_14546_m_000000_0, Status : FAILED\nError: org.apache.lucene.index.LogByteSizeMergePolicy.setUseCompoundFile(Z)V\n16/09/21 09:28:17 INFO mapreduce.Job: Task Id : attempt_1469767030336_14546_m_000000_1, Status : FAILED\nError: org.apache.lucene.index.LogByteSizeMergePolicy.setUseCompoundFile(Z)V\n16/09/21 09:28:21 INFO mapreduce.Job: Task Id : attempt_1469767030336_14546_m_000000_2, Status : FAILED\nError: org.apache.lucene.index.LogByteSizeMergePolicy.setUseCompoundFile(Z)V\n16/09/21 09:28:27 INFO mapreduce.Job:  map 100% reduce 0%\n16/09/21 09:28:27 INFO mapreduce.Job: Job job_1469767030336_14546 failed with state FAILED due to: Task failed task_1469767030336_14546_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n\n16/09/21 09:28:27 INFO mapreduce.Job: Counters: 11\n        Job Counters\n                Failed map tasks=4\n                Launched map tasks=4\n                Other local map tasks=4\n                Total time spent by all maps in occupied slots (ms)=18101\n                Total time spent by all reduces in occupied slots (ms)=0\n                Total time spent by all map tasks (ms)=18101\n                Total vcore-seconds taken by all map tasks=18101\n                Total megabyte-seconds taken by all map tasks=74141696\n        Map-Reduce Framework\n                CPU time spent (ms)=0\n                Physical memory (bytes) snapshot=0\n                Virtual memory (bytes) snapshot=0\nException in thread \"main\" java.lang.RuntimeException: Merge tool exited with code: -1\n        at com.sears.MergeSolrIndex.HdfsMergeTool.main(HdfsMergeTool.java:105)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "krausb": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/472", "title": "com.twitter.elephantbird.util.ThriftUtils unable to find fields with _ or - in its name", "body": "In my current case the ThriftUtils.getFieldType(...) is not able to successful reflect a field named signal_name out of the thrift class.\n\nExample thrift struct:\n\n> struct TSignal {\n>     1: required i32 signalId\n>     2: required string signal_name\n>     3: required i64 timestamp\n>     4: required i32 raw_value\n>     5: required double physical_value\n>     6: optional string note\n> }\n\nThe Problem here is, that the thrift compiler itself camel cases the getter and the setter to \nTSignal.getSignalName and TSignal.setSignalName without _ or -.\n\nOn line 142 of latest Version of ThriftUtils.getFieldType(...) you are looking for:\n`Method method = containingClass.getDeclaredMethod(prefix + suffix);`\n\nThe variable suffix evaluates as a result of line 135:\n`String suffix = // uppercase first letter\n        fieldName.substring(0, 1).toUpperCase() + fieldName.substring(1);` \nto \"getSignal_name\" or \"isSignal_name\" which does not exist in the thrift class.\n\nWould be nice if you could recognize this behaviour and add that eventuality into ThriftUtils.getFieldType.\n\nPossible solutions would be changing the discovery of the fields by using Class.getDeclaredField(...) so you would'nt need the prefix + suffix and may use the field name directly here or additionaly camel case the field name.\n\nRegards,\nBastian\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MohanBunny": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/471", "title": "PIG LOAD JSON FILE using Elephant-bird gives ERROR", "body": "Trying to load the json file which is having null values in it by using elephant-bird JsonLoader.\n\nsample.json\n\n`{\"created_at\":\"Mon Aug 22 10:48:23 +0000 2016\",\"id\":767674772662607873,\"id_str\":\"767674772662607873\",\"text\":\"KPIT Image Result for https:\\/\\/t.co\\/Nas2ZnF1zZ... https:\\/\\/t.co\\/9TnelwtIvm\",\"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\\" rel=\\\"nofollow\\\"\\u003eTwitter Web Client\\u003c\\/a\\u003e\",\"truncated\":false,\"in_reply_to_status_id\":123,\"in_reply_to_status_id_str\":null,\"in_reply_to_user_id\":null,\"in_reply_to_user_id_str\":null,\"in_reply_to_screen_name\":null,\"geo\":null,\"coordinates\":null,\"place\":null,\"contributors\":null,\"is_quote_status\":false,\"retweet_count\":0,\"favorite_count\":0,\"entities\":{\"hashtags\":[],\"urls\":[{\"url\":\"https:\\/\\/t.co\\/Nas2ZnF1zZ\",\"expanded_url\":\"http:\\/\\/miltonious.com\\/\",\"display_url\":\"miltonious.com\",\"indices\":[24,47]}],\"user_mentions\":[],\"symbols\":[]},\"favorited\":false,\"retweeted\":false,\"possibly_sensitive\":false,\"filter_level\":\"low\",\"lang\":\"en\",\"timestamp_ms\":\"1471862903167\"}`\n\nscript:\n\n```\nREGISTER piggybank.jar\nREGISTER json-simple-1.1.1.jar\nREGISTER elephant-bird-pig-4.3.jar\nREGISTER elephant-bird-core-4.1.jar\nREGISTER elephant-bird-hadoop-compat-4.3.jar\n\njson = LOAD 'sample.json' USING JsonLoader('created_at:chararray, id:chararray, id_str:chararray, text:chararray, source:chararray, in_reply_to_status_id:chararray, in_reply_to_status_id_str:chararray, in_reply_to_user_id:chararray, in_reply_to_user_id_str:chararray, in_reply_to_screen_name:chararray, geo:chararray, coordinates:chararray, place:chararray, contributors:chararray, is_quote_status:bytearray, retweet_count:long, favorite_count:chararray, entities:map[], favorited:bytearray, retweeted:bytearray, possibly_sensitive:bytearray, lang:chararray');\ndescribe json; \ndump json;\n```\n\nWhen I dump json,I am getting the following **output** and the **worning**\n\n**(Mon Aug 22 10:48:23 +0000 2016,767674772662607873,767674772662607873,google Image Result for Twitter Web Client,false,1234,12345,3214,43215,,,,,,,,,,,,,,)**\n\n**WARN org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger - org.apache.pig.builtin.JsonLoader(UDF_WARNING_1): Bad record, returning null for {complete json}**\n\nBy warning i guess it is because of getting NULL values. So **how can we load a Json which is having null values in it.**\n\nAnd I have tried in another way i.e\n\n`json = LOAD 'sample.json' USING com.twitter.elephantbird.pig.load.JsonLoader('created_at:chararray, id:chararray, id_str:chararray, text:chararray, source:chararray, in_reply_to_status_id:chararray, in_reply_to_status_id_str:chararray, in_reply_to_user_id:chararray, in_reply_to_user_id_str:chararray, in_reply_to_screen_name:chararray, geo:chararray, coordinates:chararray, place:chararray, contributors:chararray, is_quote_status:bytearray, retweet_count:long, favorite_count:chararray, entities:map[], favorited:bytearray, retweeted:bytearray, possibly_sensitive:bytearray, lang:chararray');`\n\n`describe json;`\n**Output**\n\n**Schema for json unknown.**\n\nPlease suggest me.\n\nThanks.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gzm55": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/470", "title": "Add support thrift TCompactProtocol in ThriftConverter", "body": "Hi,\n\nIn many general cases, compact protocol bytes in lzo-block file can save 10%~15% size than binary protocol bytes, but the ThriftConverter can't choose other protocol factory than TBinaryProtocol.Factory when serialize() or deserialize(). \n\nCan we add two optional parameters,\n- first is to set a TProtocolFactory class name, used as output protocol when calling serialize();\n- second  is to enable detecting the protocol via TProtocolUtil.guessProtocolFactory() when calling deserialize().\n\nif neither is set, all behaviors of previous codes (MR/pig/hive, etc.) should be preserved.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/470/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "SonaliShrivastava": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/468", "title": "TestThriftToPig for Elephantbird piggybank getting failed", "body": "Hi All,\n\nI am trying to port Elephant Bird Stable Release \"2.2.3\" on RHEL 7.2 ppc64le using open jdk 1.8\nGot success in running ant, ant compile, ant resolve, ant jar and for other targets too except got\nfailure for ant test.\n\nIt gives me Test Failure in below line:\n[junit] Running com.twitter.elephantbird.pig.piggybank.TestThriftToPig\n    [junit] Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.081 sec\n    [junit] TEST com.twitter.elephantbird.pig.piggybank.TestThriftToPig FAILED\n\nBy debugging failures,\nI see, it is failing on line no. 160 i.e. \nassertEquals(\"{(1,0,34,27000,16777216,6000000000,3.141592653589793,JSON THIS! \\\"^A,\" + ooe.zomg_unicode +\n        \",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1),(2),(3)}),(1,0,35,27000,16777216,6000000000,3.141592653589793,JSON THIS! \\\"^A,\" +\n        ooe.zomg_unicode + \",0,base64,{(1),(2),(3)},{(1),(2),(3)},{(1),(2),(3)})}-{({}),({(and a one),(and a two)}),({(then a one, two),(three!),(FOUR!!)})}-{zero={}, three={}, two={(1,Wait.),(2,What?)}}\",\n        (toTuple(type, hm).toDelimitedString(\"-\")));\n\nFor debugging purpose, i have added logs and could get below clarity for string comparison failing as a result they are not equal and so assert error, following are details log i printed:\n        FAILED\nexpected:<...,{(1),(2),(3)})}-{({[}),({(and a one),(and a two)}),({(then a one, two),(three!),(FOUR!!)})}-{zero={}, three={}, two={(1,Wait.),(2,What?)]}}> \nbut was:<...,{(1),(2),(3)})}-{({[(and a one),(and a two)}),({}),({(then a one, two),(three!),(FOUR!!)})}-{zero={}, two={(1,Wait.),(2,What?)}, three={]}}>\njunit.framework.AssertionFailedError.\n\nIf I comment above line, this happens similar to line below too:\nassertEquals(\"(bob,jenkins),42,foo@bar.com,{(415-555-5555,HOME)}\", toTuple(type, person).toDelimitedString(\",\"));\n\nas well as also for line i.e.\nassertTrue( // the order of elements in map could vary because of HashMap\n        tupleString.equals(\"(bob,jenkins)-{MOBILE=650-555-5555, WORK=415-555-5555, HOME=408-555-5555}\") ||\n        tupleString.equals(\"(bob,jenkins)-{MOBILE=650-555-5555, HOME=408-555-5555, WORK=415-555-5555}\"));\n\nWhen i comment all 3 lines above, test passes for all other modules of it and also for other projects of elephant-bird.\n\nIt seems this is coding issue.\n@Team: Can you please let me know for reason it is failing beside other all test are passing except failing for com.twitter.elephantbird.pig.piggybank.TestThriftToPig.\n\nThanks & Regards,\nSonali Shrivastava\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/468/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "immo-huneke-zuhlke": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/467", "title": "lucene-queryparser compatibility?", "body": "In `com.twitter.elephantbird.mapreduce.output.LuceneIndexOutputFormat.createIndexWriter`, there is a call to the API `org.apache.lucene.index.LogByteSizeMergePolicy.setUseCompoundFile`, which was removed after version 4.0.0 of `org.apache.lucene:lucene-queryparser`. This prevents me from using any features of later versions of the query parser in my program (specifically, 4.7.2) if I want to continue to use `elephant-bird-pig-lucene`.\n\nWhat are your plans for upgrading this dependency? Currently, the `org.apache.lucene` components are at version 6.0.1 (35 releases beyond 4.0.0, released in October 2012).\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/467/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "aksbhatia": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/466", "title": "JsonStringToMap cannot handle JsonArrays", "body": "**Error:**\nCaused by: java.lang.ClassCastException: org.json.simple.JSONArray cannot be cast to org.json.simple.JSONObject\n    at com.twitter.elephantbird.pig.piggybank.JsonStringToMap.parseStringToMap(JsonStringToMap.java:63)\n    at com.twitter.elephantbird.pig.piggybank.JsonStringToMap.exec(JsonStringToMap.java:53)\n\n**Possible Reason:**\n  protected Map<String, String> parseStringToMap(String line) {\n    try {\n      Map<String, String> values = Maps.newHashMap();\n      **_JSONObject jsonObj = (JSONObject) jsonParser.parse(line);**_\n\ni.e. Since it is trying to cast JSONArray to JSONObject, it fails.\n\n**Eg. Input:**\n[{\"discountLevel\":\"SUBTOTAL\",\"itemIDs\":[\"2523445\"]},{\"discountLevel\":\"SUBTOTAL\",\"itemIDs\":[\"1119882\"]}]\n\n**My Pig Script:**\na = FOREACH applied GENERATE JsonStringToMap(input);\n\nSome googling led me here : https://groups.google.com/forum/#!topic/elephantbird-dev/mxCcAh4WEXY \nSimilar?\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/466/reactions", "total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 1, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mtsgrd": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/462", "title": "Protobuf 3.x compatibility?", "body": "Has anyone tested compatibility with protobuf 3.x?\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/462/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gerashegalov": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/453", "title": "SplitUtils consistently misnames min as maxCombinedSplitSize", "body": "the logic in SplitUtils#getCombinedSplits clearly treats maxCombinedSplitSize as the min, and the actual split size may well exceed it. \n\n```\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/453/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/comments/6468832", "body": "5 should be a configurable number of locations\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/6468832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/6468858", "body": "Alternatively, for better data locality, it could make sense to consider locations with most bytes rather than most splits  in case the splits are numerous but tiny.  \n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/6468858/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/6469014", "body": "If you use some custom class with a mutable int field,  you can avoid auto-boxing on line 94-98, and a second unnecessary lookup for update on line 98. Instead you could increment the int field in the object returned by hosts.get.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/6469014/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/14628891", "body": "+1\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/14628891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "rubanm": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/449", "title": "Add a parameter for MAX_COLLECTION_SIZE to ThriftBinaryProtocol", "body": "Follow up from https://github.com/twitter/elephant-bird/pull/448\n\nThis could be used for ignoring corrupt records irrespective of the readLength check.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/twitter/elephant-bird/commits/2761b313a7ee1df880e3c0cc164d0183848c738b", "message": "[Release] - 4.14, prepare for next development iteration 4.15-SNAPSHOT"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/380f5aa9f034024ca19237359c1727e8b105c414", "message": "[Release] - Prepare release 4.14"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/fbe07c4f34e72772d498e5d85d95869b24e3c8d7", "message": "Merge pull request #465 from twitter/rubanm/cascading3_followup_fixes\n\nFollow-up fix for cascading3"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/1aaaa1dd5194094b91d0df4e77d91d2c34199955", "message": "use deprecated IF wrappers for LzoText"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/c557b2d204bca118a6355b9ab760301507de71f1", "message": "fix LzoText schemes"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/7c29a806f67dffc375983318b6cf20b10bd419a8", "message": "Fix output formats to work in hadoop1 context"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/b598d321b6cff990f1a7acc80c19da24c0495227", "message": "Follow-up fixes for cascading3"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/61f3a8f1ba939b4fe1d43d76be181af7b5e55c51", "message": "[Release] - 4.13, prepare for next development iteration 4.14-SNAPSHOT"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/25c0b184edfe7b18889e0e948f0f19f38563240a", "message": "[Release] - Prepare release 4.13"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/9c4d0917e478e9fa3143f106319b42ff5c993155", "message": "Merge pull request #463 from rubanm/rubanm/try_cascading3\n\nCompatibility with Cascading 3.0 take two"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/c77ee1f7c310169346ea97035f66e46ea22ecb69", "message": "add comment re cascading versions"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/b19c1d60516d22ea99e74060b9e4915642677679", "message": "undo whitespace only changes, minor comment update"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/80c34c47992665ff82f495bfa29fd329f61ba39d", "message": "Input format fixes"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/0f6b769293ba89d5a48c01cd33ceb1877aa91233", "message": "add new subproject to pom file"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/fe4004213585ac80cd1c06da1c7bc93feb86908b", "message": "do not port deprecated schemes to cascading3"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/941db7979930f2cdff7296be9154f80af624b103", "message": "remove common protobuf code from cascading2"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/5accfdb05d7d3aa19f433a040bbd5d7229ceb4f7", "message": "move common classes to cascading-protobuf"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/c7fd5e950821632cdff7c3427b112bf6da8b8962", "message": "gitignore swp files"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/04803b5dced17c70c5e2244968805923fc779d5d", "message": "fix package namespace in cascading3"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/d3bd3dbe8bc2e0a44e2c664e2a1ce73b3158f64f", "message": "Merge branch 'try_cascading3' of https://github.com/cchepelov/elephant-bird into try_cascading3"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/be20fa68b3052babbb0dcfd48afc6f06257c551b", "message": "Merge pull request #5 from twitter/master\n\nMerge from twitter/elephant-bird"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "justmytwospence": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/445", "title": "OrderedLoadFunc for JsonLoader", "body": "Is there a particular reason the Pig JsonLoader can't implement the OrderedLoadFunc rather than LoadFunc interface? OrderedLoadFunc would allow JSON data to be merge sorted later.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/445/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bluezone2015": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/438", "title": "RCFileProtobufInputFormat not work in hive 0.13. I need help, thanks a lot~~~", "body": "--------------------------create table first:--------------------------\ncreate external table test_rc\nrow format serde \"com.twitter.elephantbird.hive.serde.ProtobufDeserializer\"\nwith serdeproperties (\n\"serialization.class\"=\"com.galaxy.example.seqfile.proto.AddressBookProtos$Person\")\nstored as\ninputformat \"com.twitter.elephantbird.mapred.input.RCFileProtobufInputFormat\"\noutputformat \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\";\n\n--------------------------then I load data to hive table:--------------------------\nload data local inpath '/root/galaxy-cdh-client/hadoop/bin/part-r-00000.rc' overwrite into table test_rc;\n\n--------------------------exec the hql:--------------------------\nselect id,name from test_rc;\n\n--------------------------then I encounter the exception:--------------------------\nDiagnostic Messages for this Task:\nError: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing writable org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable@141ed711\n        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing writable org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable@141ed711\n        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:533)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)\n        ... 8 more\nCaused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable cannot be cast to org.apache.hadoop.io.BytesWritable\n        at com.twitter.elephantbird.hive.serde.ProtobufDeserializer.deserialize(ProtobufDeserializer.java:56)\n        at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.readRow(MapOperator.java:154)\n        at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.access$200(MapOperator.java:127)\n        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:508)\n        ... 9 more\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "wonlay": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/432", "title": "DeprecatedOutputFormatWrapper does not work well with PartitionTap", "body": "When using DeprecatedOutputFormatWrapper with PartitionTap, the output won't be committed to the final location but discarded in the temporary directory.\n\nThere are two issues here:\n1. TapOutputCollector used by PartitionTag is testing if the output format is a FileOutputFormat or not, after wrapped, it's not a FileOutputFormat anymore, the filenames won't be created.\n1. The commit never happen when used in this case, the test for commit will always return false, since the real output format is writing to a different temp dir, and no intermedium commit to bridge them up.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nellore": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/426", "title": "LzoInputFormat's listStatus() can take prohibitively long on S3 because it invokes FileInputFormat's listStatus() implementation", "body": "LzoInputFormat's listStatus begins with\n\n``` java\nList<FileStatus> files = super.listStatus(job);\n```\n\nwhere super refers to FileInputFormat. FileInputFormat's listStatus() calls singleThreadedListStatus() (when LIST_STATUS_NUM_THREADS == 1), which is not optimized for S3. This becomes an issue when listing a directory with many files when a job begins; I've observed that a single listStatus() call can take 17 minutes when there are 50k files in an input path.\n\nProposed solution: use the listStatus method of the FileSystem of the appropriate input path (obtained from getInputPaths(job)). This will call the listStatus method of whatever class is specified by fs.s3[n].impl in core-site.xml.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/428", "title": "uses FileSystem's listStatus rather than FileInputFormat's in LzoInputFormat", "body": "Proposed resolution to #426, which describes how FileInputFormat's listStatus is slow on S3 for input paths spanning many files.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gsteelman": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/419", "title": "DelegateCombineFileInputFormat Doesn't Honor CombineFileInputFormat.maxSplitSize", "body": "I'm trying to use DelegateCombineFileInputFormat + LzoTextInputFormat + LzoTextOutputFormat. I'm also trying to specify the maxSplitSize for combining files. I've found that DelegateCombineFileInputFormat doesn't honor maxSplitSize, minSplitSizeNode, or minSplitSizeRack if they are configured before the job is run.\n\nPer @jcoveney \"If there is a maxInputSplitSize in Hadoop's CombineFileInputFormat no, it is not honored.\":\nhttps://github.com/kevinweil/elephant-bird/blob/master/core/src/main/java/com/twitter/elephantbird/util/SplitUtil.java#L35\n\nI can see a couple approaches for a fix:\n1) SplitUtil.getCombinedSplitSize(Configuration): Change it so it tries to getLong from COMBINE_SPLIT_SIZE, if it can't it'll try to get from CombineFileInputFormat \"mapreduce.input.fileinputformat.split.maxsize\" which apparently isn't a static constant, but a hard coded string...\n\n2) DelegateCombineFileInputFormat could set SplitUtil.COMBINE_SPLIT_SIZE equal to CombineFileInputFormat max split size if it was set. This same approach could be used for minSplitSizeNode and minSplitSizeRack. Where in DelegateCombineFileInputFormat would this go?\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/419/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "snehotosh": {"issues": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/415", "title": "error: no suitable constructor found while doing building jar (mvn package)", "body": "Hi,\n\nI am facing below error though all pre-requisite are met including Thrift 0.7.0,protoc and jdk 1.7.55 version.\n\n[INFO]\n[INFO] Elephant Bird ...................................... SUCCESS [01:06 min]\n[INFO] Elephant Bird Hadoop Compatibility ................. SUCCESS [03:09 min]\n[INFO] Elephant Bird Core ................................. FAILURE [01:12 min]\n[INFO] Elephant Bird Cascading2 ........................... SKIPPED\n[INFO] Elephant Bird Crunch ............................... SKIPPED\n[INFO] Elephant Bird Hive ................................. SKIPPED\n[INFO] Elephant Bird Pig .................................. SKIPPED\n[INFO] Elephant Bird Mahout ............................... SKIPPED\n[INFO] Elephant Bird RCFile ............................... SKIPPED\n[INFO] Elephant Bird Lucene ............................... SKIPPED\n[INFO] Elephant Bird Pig Lucene ........................... SKIPPED\n[INFO] Elephant Bird Examples ............................. SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 05:28 min\n[INFO] Finished at: 2014-08-29T13:41:54+05:30\n[INFO] Final Memory: 26M/161M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:testCompile (default-testCompile) on project elephant-bird-core: Compilation failure: Compilation failure:\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/OneOfEach.java:[189,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/com/twitter/elephantbird/thrift/test/TestUnion.java:[110,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/Base64.java:[129,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/Base64.java:[131,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/Base64.java:[133,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/Base64.java:[135,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/Base64.java:[137,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/Base64.java:[139,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/CompactProtoTestStruct.java:[355,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/CompactProtoTestStruct.java:[382,12] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/CompactProtoTestStruct.java:[409,12] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/CompactProtoTestStruct.java:[442,12] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/CompactProtoTestStruct.java:[471,12] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/thrift/test/ComparableUnion.java:[96,8] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte,String) is not applicable\n[ERROR](actual argument boolean cannot be converted to String by method invocation conversion)\n[ERROR] constructor FieldValueMetaData.FieldValueMetaData(byte) is not applicable\n[ERROR](actual and formal argument lists differ in length)\n[ERROR] /u01/Installables/PIG_Related/elephant-bird-master/core/target/generated-test-sources/thrift/com/twitter/elephantbird/thrift/test/TestBinaryInListMap.java:[107,16] error: no suitable constructor found for FieldValueMetaData(byte,boolean)\n[ERROR] -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR]\n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :elephant-bird-core\n[root@bigdata1 elephant-bird-master]#\n\nRegards,\nSnehotosh\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/415/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "benpence": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/elephant-bird/commits/f5f5baeac4869e6969cddc88ed49f0103e368b39", "message": "Merge pull request #481 from Yaliang/yaliangw/add-presto-hive-query\n\nAdded condition to fetch serialization class for a presto query"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Yaliang": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/elephant-bird/commits/bd5f658e85b85ace26716191204997bcc82c8a59", "message": "Added condition to fetch serialization class for a presto query"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/485", "title": " Check LZO index file existence when check splitability of LZO files", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "isnotinvain": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/elephant-bird/commits/1a6b87410afc4e546d4db86df467b3e14cfb936f", "message": "Merge pull request #479 from dieu/apanasenko/release_4.15\n\nRelease 4.15"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/6b2e45057f1904295cec3291d6b5f3720434813a", "message": "Merge pull request #478 from dieu/apanasenko/writebale_seq_with_combined\n\nAdded CombinedWritableSequenceFile to cascading2"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/87efd8c5358614888791fc2ce6b68675bac32239", "message": "Merge pull request #469 from zmhassan/typo-1\n\nFixed documentation to remove typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dieu": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/elephant-bird/commits/abde522550fd6d9185d3ff3a427dfca0d9f75f43", "message": "[Release] - 4.15, prepare for next development iteration 4.16-SNAPSHOT"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/1e6889fe0100e5b7dbd1ed9384ceb114f760f7f1", "message": "[Release] - Prepare release 4.15"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/commits/62005488404f8bc20ca9f73cb97edad8c47e127f", "message": "Added CombinedWritableSequenceFile to cascading2"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zmhassan": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/elephant-bird/commits/316fe46b3a867b617f349d89da30698ae49447e2", "message": "Fixing documentation to remove typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "steveniemitz": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/483", "title": "Add an option to disable using LZO index files when computing splits", "body": "We have many jobs that read large (>10,000) numbers of LZO input files.  Because of the linear relationship of split calculation time to the number of inputs, some of these jobs can actually take longer to calculate the splits than for the mappers to actually run.\r\n\r\nResults from an trivial test job show a massive speedup when disabling using indexes when calculating splits, with no change in the number of splits actually created.\r\n\r\nWith index files enabled:\r\n``` \r\n * RESULTS:          1\r\n *   input paths:  9708\r\n *   total splits: 4854\r\n *   time [s]:      165\r\n```\r\nand disabled:\r\n```\r\n *  RESULTS:          1\r\n *   total paths:  9708\r\n *   total splits: 4854\r\n *   time [s]:        2\r\n```\r\n\r\nInternally we've been using hacky similar solution which involved overriding `getSplits()` from `MultiInputFormat` in a subclass.  While hacky, it has enabled a very significant speed up in many of our jobs.\r\n\r\nAs a follow up enhancement, we could automatically skip split calculation using LZO indexes if the number of input files is greater than some number N (which could be chosen via sampling many existing jobs).\r\n\r\nFinally, another reason one would want to enable this feature is in cloud environments.  Typically cloud blob stores (S3, GCS) have a significantly higher time-to-first-byte than say, locally hosted HDFS.  Because of this, doing many small (LZO index files are typically only a few MB) sequential reads can be very slow.  I specifically ran into this running a job in Google Dataproc processing ~10,000 files, the time to calculate splits for the job took over 40 minutes!  Disabling splitting dropped the time to ~1 second.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "glennq": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/477", "title": "Make numRecordsPerBlock configurable", "body": "@isnotinvain \r\nAttempt to make numRecordsPerBlock configurable.\r\n\r\nWhen testing locally there's a strange failure for pig even when building from master (at line 166 of TestThriftToPig.java):\r\n```\r\nFailed tests:   test(com.twitter.elephantbird.pig.util.TestThriftToPig): expected:<...,{(1),(2),(3)})}-{({[}),({(and a one),(and a two)}),({(then a one, two),(three!),(FOUR!!)})}-{zero={}, three={}, two={(1,Wait.),(2,What?)]}}> but was:<...,{(1),(2),(3)})}-{({[(and a one),(and a two)}),({}),({(then a one, two),(three!),(FOUR!!)})}-{zero={}, two={(1,Wait.),(2,What?)}, three={]}}>\r\n```\r\n\r\nPassed all tests before this failure.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zhuguangbin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/476", "title": "fix LzoBinaryBlockRecordReader bug: skip bad block rather than throws IOE", "body": "In some scenario, LzoThriftBlock file may be corrupt. For example, in our company, we consume kafka data and sink to HDFS as LzoThriftBlock format using a flink streaming job. If the job crash, the writing file may be corrupt. The mapreduce job reading the corrupt file will fail . \r\n\r\nError logs are as follows: \r\n\r\norg.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: Premature EOF from inputStream\r\n\tat org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:201)\r\n\tat com.twitter.elephantbird.mapreduce.io.BinaryBlockReader.parseNextBlock(BinaryBlockReader.java:145)\r\n\tat com.twitter.elephantbird.mapreduce.io.BinaryBlockReader.setupNewBlockIfNeeded(BinaryBlockReader.java:169)\r\n\tat com.twitter.elephantbird.mapreduce.io.BinaryBlockReader.readNextProtoBytes(BinaryBlockReader.java:87)\r\n\tat com.twitter.elephantbird.mapreduce.io.BinaryBlockReader.readNext(BinaryBlockReader.java:74)\r\n\tat com.twitter.elephantbird.mapreduce.input.LzoBinaryBlockRecordReader.nextKeyValue(LzoBinaryBlockRecordReader.java:138)\r\n\tat com.twitter.elephantbird.pig.load.LzoBaseLoadFunc.getNextBinaryValue(LzoBaseLoadFunc.java:108)\r\n\tat com.twitter.elephantbird.pig.load.ThriftPigLoader.getNext(ThriftPigLoader.java:48)\r\n\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:211)\r\n\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)\r\n\tat org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)\r\n\tat org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)\r\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\r\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\r\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\r\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\r\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\r\n\r\nThe reason is the last block of the split is incomplete because of suddenly quit of the writing steam. I think this could be more tolerant\uff0calthough the last block is corrupt, the preceding blocks of the split is OK. All we should do is skipping the corrupt block.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rjoberon": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/464", "title": "added option to include the raw JSON", "body": "I found no option to \"pipe\" the raw JSON through Pig, i.e., to filter the JSON with Pig based on its content but then output the original JSON. Using JsonStorage does not work, since it does not fully support nested structures. The same is true for LzoJsonStorage. Therefore, I added an option which adds the raw JSON to the returned tuple such that the first tuple can be used for processing with Pig and the second tuple can be stored.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mflood": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/443", "title": "Changed  byteStream.getCount() to byteStream.getLength()", "body": " so that RCFilePigStorage works with hive 0.14 (and is backwards compatible with previous versions)\n\nBecause 'public int getCount()' was removed from org.apache.hadoop.hive.serde2.ByteStream.Output in hive 0.14, and getLength() is available from the parent class.\n\n   See:\n\n   https://groups.google.com/forum/#!msg/elephantbird-dev/jNPK7p_QaNs/XshS_OLXNogJ\n   https://issues.apache.org/jira/browse/PIG-3949\n   https://issues.apache.org/jira/browse/HIVE-6430\n   https://issues.apache.org/jira/secure/attachment/12642763/HIVE-6430.12.patch\n\nHive 0.13:\nhttp://svn.apache.org/repos/asf/hive/branches/branch-0.13/serde/src/java/org/apache/hadoop/hive/serde2/ByteStream.java\n\nHive 0.14:\nhttp://svn.apache.org/repos/asf/hive/branches/branch-0.14/serde/src/java/org/apache/hadoop/hive/serde2/ByteStream.java\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "venugit": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/436", "title": "Fix NPE when iterating over an input split in CompositeRecordReader.java", "body": "When iterating over input splits via DeprecatedInputFormatWrapper, DeprecatedInputFormatWrapper.java always calls mifcReader.setKeyValue(key, value) before nextValue is invoked which can call through to setKeyValue in CompositeRecordReader.java. setKeyValue requires that the currentRecordReader instance be non-null; however currentRecordReader is set to null in line 113 at the end of every input split, leading to an NPE with the next call to setKeyValue after the end of an input split. \n\nThis patch address the situation by having the setKeyValue method doing a null check for currentRecordReader and in the case it is null, invoking nextKeyValue to see if there are any more elements to be found \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mikekap": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/433", "title": "Support fullcamel mode thrift in ThriftStructDescriptor", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "angushe": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/425", "title": "Fix protobuf serde errors", "body": "Hi, \n\nThis is a pull request trying to fix the same problem described in pull request #400,  and the fix has been tested successfully on Hive 0.12/0.13 and Protobuf 2.4.1/2.5.0.\n\nAny comments?\n\nThanks\nAngus\n", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/143", "title": "Protobuf message extension support", "body": "Hi,\n\nHere is an attempt to add protobuf message extension support to elephant-bird. \n\nAny comments would be appreciated. \n\nThanks\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/12443", "body": "Hi Raghu,\n\nIt is intentional, for some users's Hadoop distribution comes up with the commons-codec-1.3.jar which may precede the 1.4 version in classpath. And in that case, Base64(0) fails to compile.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/12443/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "kendaleiv": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/416", "title": "Improve nested JSON pig examples", "body": "Unsure when `elephant-bird-hadoop-compat.jar` is actually required. The Hortonworks Sandbox 2.1 seems to require it, anyhow.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "canojim": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/410", "title": "support mahout version 0.9", "body": "support mahout changes in 0.9\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "xuwenhao": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/404", "title": "Add BooleanWritableConverter to use them in SequenceFileLoader in pig", "body": "I have used BooleanWritable as a key or value in SequenceFile, since BooleanWritable is built inside hadoop, I suggest to have BooleanWritableConverter built into the elephant-bird. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rahulrv1980": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/400", "title": "Fix for ProtobufStructObjectInspector to work correctly with Hive", "body": "Use builder class since hive expects mutable objects while message objects are immutable\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pereferrera": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/387", "title": "Update Readme.md", "body": "Adding beginner's guide to registering all needed jars in a Pig session.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gurmeetsaran": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/378", "title": "Added Support for Handling Invalid JSON Records", "body": "Hi Kevin,\n\nI have added support for handling Invalid JSON Records. Currently It just filters out invalid JSON Records. I have added support to return invalid records which can be segregated into good and bad records. This can help if we want to report invalid records to the upstream user who is generating the records.\n\nThis can be changed by setting the -invalidRecord option in the JsonLoader constructor\n Example: \nsource_data = LOAD '$input' USING com.intuit.iac.pig.udf.JsonLoader('-nestedLoad -invalidRecord') as (json:map[]);\nSPLIT source_data INTO source_data_good_record IF json#'error_string' is null,source_data_bad_record IF json#'error_string' !='';\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ddaniels888": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/310", "title": "Retry file status (ls) operations, which can fail with FileNotFound due to eventual consistency in S3", "body": "Fixes issue #309.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "housejester": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/212", "title": "Json work (Jive Software)", "body": "These commits started with a need to have a JsonConverter with nesting support to be used by the SequenceFileLoader.  There are several places in elephant-bird that do json work, and they seem to do it differently.  It seems reasonable to start to refactor those places to delegate to a single json parsing utility.  \n\nSo, the first commit simply adds JsonToPigParser, which is mostly the same code as what is in JsonLoader, but uses the jackson json parser instead of simple-json.  It supports creating nested maps from nested json, as well as detecting string values that have json data (think avro json structures that have a field that is essentially a json payload).  Has tests.\n\nThe next commit adds the JsonConverter for use with the SequenceFileLoader.  The javadoc shows usage.\n\nThe final commit modifies the JsonStringToMap to delegate to the JsonToPigParser.  I REALLY wish I could get the nesting to work here.  It was easy enough to add, but pig refused to allow me to refer to the nested structures.  What is strange is that the nesting support DOES work in the JsonConverter scenario.\n\nNext steps might be to refactor the existing Json\\* classes to delegate to the JsonToPigParser.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rangadi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/101", "title": "Support for non-LZO files.", "body": "set elephantbird.lzo.input.include.nonlzo to enable it.\nby default only lzo files are read.\n\nAll the existing Lzo\\* input formats and Pig loaders would work with this. \n\n(eventually we might remove \"Lzo\" prefix for many of these loader classes).\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/612522", "body": "Thanks Guys.\n\nAdded another update to the patch. Includes all the suggestions.\n- renamed couple of 'Binary' base classes to remove 'Proto' in the name.\n- Updated names for counters in PigLoaders to include name of the Thrift/Protobuf class name.\n- added stats for InputFormats.\n- PigCounterHelper : changed order of checking for reporter so that there are Map insertions and deletions in common case.\n- Other misc smaller changes.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/612522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/615003", "body": "Updated the equals methods as suggested. \n\nFor now, retained the stats names. can can add '_' once elephant-bird can export these stats to ganglia.. \n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/615003/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/632770", "body": "Only the last three commits are the real commits. The rest of them are already merged.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/632770/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/comments/279754", "body": "I don't think we need these two booleans. Essentially we could have, serialized == (message != null) and deserialized == (messageBytes != null). We will make sure this condition holds when the state of the object changes (in set(), readFields() etc).\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/279754/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/279771", "body": "minor: Do you have auto-reindent set? I think the prev indentation was better.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/279771/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/2807880", "body": "nope. may be just 3.0.0 for min version is good enough.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/2807880/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/5853", "body": "This file got committed in the past by mistake.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/5853/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/7650", "body": "builder.clone()  is create() + mergeFrom() . merge iterates over all the fields.\nmessage.newBuilderForType() is just create(). \n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/7650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/12441", "body": "You can just call Base64(0), right? since EB will have 1.4 in its libs.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/12441/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/12676", "body": "I was able to compile even though my hadoop has 1.3. I don't think hadoop libs are included in classpath while compiling. Not sure if t invoking through reflection is much costlier than invoking directly. Either way, it does not affect EB since it creates only one per file.\n\nIt would be nice to have a comment inside createStandardBase64() as to why we have this work around.\nthanks for the patch.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/12676/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/44630", "body": "Removed this as this was not used any more.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/44630/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/45677", "body": "This was the bug. We were using the same filter for listing directories, which ended up filtering out directories not ending \".lzo\"\n\nThe rest of the code changes is mainly to move file filtering logic into one place (into addInputPath).\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/45677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/45768", "body": "The java doc says it can return null if Schema could not be determined. In fact my test for recursion was using this patch (with JsonLoader).\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/45768/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/105968", "body": "How are splits handled? this should take split start and end into account?\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/105968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/105987", "body": "these properties can be removed since Pig projection will read only the required fields.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/105987/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106035", "body": "scratch the above. these are context properties not pig config properties.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106035/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106042", "body": "eventually we need to let key and value converters to provide the schema so that getSchema() can be implemented. \n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106042/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106055", "body": "It is probably better to expect just two columns 'key, value' in the input table. It would be less error prone and also lets PIG skip the unused fields efficiently. Most of our scripts don't access fields with index. In the above case it would be :\n\nkey_value = foreach key_val generate key, val;\n\nSTORE key_val INTO '$OUTPUT' USING com.twitter.elephantbird.pig.store.SequenceFileStorage (\n  '-t org.apache.hadoop.io.IntWritable -c com.twitter.elephantbird.pig.util.IntWritableConverter',\n '-t org.apache.hadoop.io.Text        -c com.twitter.elephantbird.pig.util.TextConverter'\n );\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106068", "body": "next() would return null if hasNext() is not called? I don't think Iterator contract makes invoking hasNext()  and next() strictly in pairs.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106068/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "kevinweil": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/174861", "body": "Yes, that's a good point.  The LZO native libs must be on your java.library.path in both hadoop and pig to use the code for either.  I will update the readme.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/174861/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/176112", "body": "Fixed.  Thanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/176112/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/256493", "body": "Yeah, it's just not possible to use custom slicers in local mode with pig 0.6.  As Dmitriy says, this is fixed in 0.7, at which point it will begin magically working.  I'm going to close this bug now, because we'll have to do refactoring for 0.7 anyway.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/256493/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/379827", "body": "Hi Scott,\n\nThe goal is to be sure that some corruption in a large file doesn't brick the whole file.  So data is written in chunks, and chunks are delimited by this 16-byte separator.  A more elegant design would allow the separator to be dynamic to ensure that any written block does not contain its separator, but there is a 2^128 chance of this happening, so we were ok with it.  And we'd welcome backwards compatible changes that address it, if you're concerned :)  But that's the overall function of the KNOWN_GOOD_POSITION_MARKER.  It also is used in making the file splittable for Hadoop-based parallel processing, which works the same way.  \n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/379827/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/379832", "body": "Update on this: reached out to Karthik, who wrote the protocol buffers to a file in a different file format than elephant bird uses, so it makes sense that they were unreadable for elephant bird.  Closing this as not an issue.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/379832/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/382232", "body": "Looks good.  Will do...\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/382232/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/382233", "body": "Pulled and integrated into kevinweil/master\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/382233/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/382234", "body": "Thanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/382234/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/431366", "body": "I'm not sure what the issue is here, since it works on Linux/Mac.  I don't have the time right now to make this support the Windows environment, so I'm going to close this request.  If you have time to debug it and patch it for Windows, though, I'd gladly accept it!\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/431366/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/477462", "body": "Empty.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/477462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/478482", "body": "Todd, is this release backwards incompatible with CDH2?\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/478482/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/478485", "body": "Oops, wrong pull request :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/478485/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/643931", "body": "Working on it as we speak!\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/643931/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "voberoi": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/214219", "body": "FWIW, the first id field doesn't have to be part of the message definition to cause this bug. It seems to happen with any repeated nested message.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/214219/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/216377", "body": "I'm currently working on a patch and I'll send you guys a pull request later today or tomorrow.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/216377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/217744", "body": "Patch is in voberoi/elephant-bird/master.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/217744/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/245602", "body": "Just initially puzzled by this behavior and slowed down. This is certainly a very-nice-to-have, though.\n\nWriting tests that run in any reasonable amount of time for large Pig scripts, for example, is impossible.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/245602/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "dvryaboy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/216368", "body": "You are quite right -- at the moment we don't support complex fields in pig to protobuf conversion (we do support them in protobuf to pig conversion). Adding this functionality would basically require taking ProtobufToPig and inverting it to create PigToProtobuf. We'll keep this in mind as a feature to add. \n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/216368/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/220480", "body": "merged, tested, committed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/220480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/245590", "body": "The main issue here is that Pig pre-0.7 does not use Slices in local mode, and slices are where the actual work happens.  This should get taken care of when we branch the current code to a 0.6 branch, and port master to 0.7. It's a big rewrite, though, so won't happen for a bit. Are you blocked by this, or just slowed down?\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/245590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/270904", "body": "Did we not put guava in lib/ ? That would be an oversight. Guava is Apache 2, so we can just distribute it with Elephant Bird.  I tried to set it up using maven/ivy for another project the other day, but looks like the maven deploy for guava is broken :(.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/270904/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/477454", "body": "This has been merged.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/477454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/590076", "body": "Love the refactor. \n\nWe should update the docs to explain that one does not need all that code generation anymore! Nice work.\n\nBinaryProto\\* should just be Binary\\* -- otherwise it's confusing that Thrift objects appear to come from protocol buffer - related classes. Same for the various proto\\* variables in Binary\\* classes (protoConverters become binaryConverters, etc).\n\nI think we had this in the old code too, but I am not terribly happy with the silent failures on decode errors in the Readers.  Perhaps we should wrap them in a DecodeException and rethrow; then allow InputFormat to increment error counters and return null if that's the desired behavior. The counters should be parametrized with the run-time class, not just be generic.\n\nThe LzoThriftB64LinePigLoader.java comments still refer to protocol buffers -- please change.\n\nThe thrift changes make sense.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/590076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/628893", "body": "Merged to master.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/628893/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/7647", "body": "It's better to keep a prototypical builder around and clone it every time than to build a new builder every time.\nSo instead, keep builder_ here, and in writer_.write do tupleToMessage(builder_.clone(), f);\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/7647/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/45713", "body": "I am not confident Pig won't break if we return null here.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/45713/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/90075", "body": "guess we should rethrow here.. RTE?\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/90075/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/90076", "body": "\"can\"\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/90076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/94877", "body": "it would probably be better to change the build so noproto and nothrift are combinable (so we don't have to add noproto_nothrift targets, etc)\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/94877/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/94878", "body": "\"generates\"\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/94878/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/94884", "body": "Oh I see we already have properties. We should be able to just rework the compile target and make sure it respects -Dnoproto, -Dnothrift, etc, and get rid of the specialty targets.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/94884/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/94891", "body": "It seems like we shouldn't need jar-precompiled if we get the dependencies right here -- it should see that certain stages have been done and skip them automatically.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/94891/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mhkt": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/270984", "body": "Ah, I do see google-collect in lib, which covers it. \n\nAnd from the Guava issue-tracker, I'm guessing the Ivy issue is related to IVY-1169 (fixed in SVN):\nhttps://issues.apache.org/jira/browse/IVY-1169\n\nThanks!\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/270984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "karthikswa": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/351091", "body": "Also wanted to mention that my LZO file contains a list of repeated Protobuf objects in them\n\n-Karthik.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/351091/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "johanoskarsson": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/656110", "body": "As Dmitriy mentioned the method is kinda awkward, it's generated by Eclipse and I didn't polish it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/issues/comments/656110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "traviscrawford": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/comments/2807640", "body": "OSX 10.8 ships with:\n\n```\nApache Maven 3.0.3 (r1075438; 2011-02-28 09:31:09-0800)\n```\n\nAny objection to dropping this version requirement to 3.0.3?\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/2807640/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "Piyushbalwani": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/comments/20620925", "body": "when i implement examples/src/main/pig/nested_json_get_distinct_items_from_nested_array.pig \r\n\r\ngrunt> register /home/piyush/Desktop/pro/json-simple-1.1.1.jar ;\r\ngrunt> register /home/piyush/Desktop/pro/elephant-bird-core-4.1.jar ;\r\ngrunt> register /home/piyush/Desktop/pro/elephant-bird-pig-4.1.jar ;\r\ngrunt> json_data = load '/home/piyush/Desktop/sample.json' using com.twitter.elephantbird.pig.load.JsonLoader('-nestedLoad'); \r\n\r\n2017-01-26 08:49:50,123 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. org/slf4j/LoggerFactory\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/comments/20620925/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "sagemintblue": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106037", "body": "On Mon, Aug 29, 2011 at 2:13 PM, rangadi\nreply@reply.github.com\nwrote:\n\n> > - \u00a0 \u00a0 \u00a0 \u00a0default:\n> > - \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// TODO fix Pig's silent ignorance of FrontendExceptions thrown from here\n> > - \u00a0 \u00a0 \u00a0 \u00a0 \u00a0throw new FrontendException(\"Expected field indices in [0, 1] but found index \" + i);\n> > - \u00a0 \u00a0 \u00a0}\n> > - \u00a0 \u00a0}\n> > - \u00a0 \u00a0setContextProperty(READ_KEY_PARAM, Boolean.toString(readKey));\n> > - \u00a0 \u00a0setContextProperty(READ_VALUE_PARAM, Boolean.toString(readValue));\n> > - \u00a0 \u00a0return new RequiredFieldResponse(true);\n> > - \u00a0}\n> >   +\n> > - \u00a0@Override\n> > - \u00a0public void setLocation(String location, Job job) throws IOException {\n> > - \u00a0 \u00a0Preconditions.checkNotNull(location, \"Location is null\");\n> > - \u00a0 \u00a0Preconditions.checkNotNull(location, \"Job is null\");\n> > - \u00a0 \u00a0FileInputFormat.setInputPaths(job, new Path(location));\n> > - \u00a0 \u00a0readKey = Boolean.parseBoolean(getContextProperty(READ_KEY_PARAM, \"true\"));\n> \n> these properties can be removed since Pig projection will read only the required fields.\n\nCould you clarify? Pig signals LoadFunc impls to optimize their\nbehavior based on required projection. These properties support\nimplementation of that optimization in SequenceFileLoader. If I remove\nthem, unnecessary Writable deserialization will take place during\nprojection.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106037/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106039", "body": "On Mon, Aug 29, 2011 at 2:29 PM, rangadi\nreply@reply.github.com\nwrote:\n\n> > - \u00a0 \u00a0 \u00a0 \u00a0default:\n> > - \u00a0 \u00a0 \u00a0 \u00a0 \u00a0// TODO fix Pig's silent ignorance of FrontendExceptions thrown from here\n> > - \u00a0 \u00a0 \u00a0 \u00a0 \u00a0throw new FrontendException(\"Expected field indices in [0, 1] but found index \" + i);\n> > - \u00a0 \u00a0 \u00a0}\n> > - \u00a0 \u00a0}\n> > - \u00a0 \u00a0setContextProperty(READ_KEY_PARAM, Boolean.toString(readKey));\n> > - \u00a0 \u00a0setContextProperty(READ_VALUE_PARAM, Boolean.toString(readValue));\n> > - \u00a0 \u00a0return new RequiredFieldResponse(true);\n> > - \u00a0}\n> >   +\n> > - \u00a0@Override\n> > - \u00a0public void setLocation(String location, Job job) throws IOException {\n> > - \u00a0 \u00a0Preconditions.checkNotNull(location, \"Location is null\");\n> > - \u00a0 \u00a0Preconditions.checkNotNull(location, \"Job is null\");\n> > - \u00a0 \u00a0FileInputFormat.setInputPaths(job, new Path(location));\n> > - \u00a0 \u00a0readKey = Boolean.parseBoolean(getContextProperty(READ_KEY_PARAM, \"true\"));\n> \n> scratch the above. these are context properties not pig config properties.\n\nGot it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106039/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106040", "body": "On Mon, Aug 29, 2011 at 2:05 PM, rangadi\nreply@reply.github.com\nwrote:\n\n> > - \u00a0private final DataInputBuffer kibuf = new DataInputBuffer();\n> > - \u00a0private final DataInputBuffer vibuf = new DataInputBuffer();\n> > - \u00a0private FileSplit fileSplit;\n> > - \u00a0private SequenceFile.Reader reader;\n> > - \u00a0private ValueBytes vbytes;\n> > - \u00a0private boolean more, valueUncompressed;\n> >   +\n> > - \u00a0@Override\n> > - \u00a0public void initialize(InputSplit inputSplit, TaskAttemptContext context) throws IOException,\n> > - \u00a0 \u00a0 \u00a0InterruptedException {\n> > - \u00a0 \u00a0Preconditions.checkNotNull(inputSplit, \"InputSplit is null\");\n> > - \u00a0 \u00a0Preconditions.checkNotNull(context, \"TaskAttemptContext is null\");\n> > - \u00a0 \u00a0Configuration conf = context.getConfiguration();\n> > - \u00a0 \u00a0this.fileSplit = (FileSplit) inputSplit;\n> > - \u00a0 \u00a0Path path = this.fileSplit.getPath();\n> > - \u00a0 \u00a0this.reader = new SequenceFile.Reader(FileSystem.get(conf), path, conf);\n> \n> How are splits handled? this should take split start and end into account?\n\nI think you're right. Look for an update soon...\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106040/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106222", "body": "Fixed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106222/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106224", "body": "Fixed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106224/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106227", "body": "\"-i\" option removed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106227/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106232", "body": "You mean LoadMetadata#getSchema()? I hadn't planned on implementing LoadMetadata in SequenceFileLoader. Should this be added before inclusion of this branch?\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106232/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}, {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106367", "body": "I've added basic LoadMetadata support to SequenceFileLoader with latest commit to this branch.\n", "reactions": {"url": "https://api.github.com/repos/twitter/elephant-bird/pulls/comments/106367/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "MEMBER"}]}}}}