{"_default": {"1": {"unisqu": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/546", "title": "can implement xxhash as hash?", "body": "can implement xxhash as hash?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stanisavs": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/543", "title": "Stats is empty", "body": "Hello!\r\nHow can I see stats?\r\n``src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222``\r\nThen I use this connection on production for 1 minute (5k qps).\r\nLogs:\r\n```...\r\n...\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 10 eof rb 567 sb 124\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 10 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 10 '127.0.0.1:20964' on event 00FF eof 1 done 1 rb 567 sb 124  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76105 done on c 8 req_time 0.226 msec type REQ_REDIS_ZREVRANGEBYSCORE narg 7 req_len 86 rsp_len 97 key0 'DB11' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76107 done on c 11 req_time 0.181 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '456:51:d8437695e7a446210fc356b730d5e909:tmp' peer '127.0.0.1:20968' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76111 done on c 8 req_time 0.140 msec type REQ_REDIS_HGET narg 3 req_len 100 rsp_len 13 key0 'tables:domain' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 11 eof rb 456 sb 159\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 11 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 11 '127.0.0.1:20968' on event 00FF eof 1 done 1 rb 456 sb 159  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76114 done on c 8 req_time 0.156 msec type REQ_REDIS_HINCRBY narg 4 req_len 90 rsp_len 7 key0 'imps:420155' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76116 done on c 8 req_time 0.151 msec type REQ_REDIS_HINCRBY narg 4 req_len 81 rsp_len 8 key0 'campaigns:ads:stats_day:420144' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76118 done on c 8 req_time 0.139 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '459:51:489c90dd70cc6bf3292a1722ac0f46ee:tmp' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_connection.c:360 recv on sd 8 eof rb 436 sb 130\r\n[2017-12-06 11:05:52.735] nc_request.c:430 c 8 is done\r\n....\r\n```\r\n\r\n\r\nStats (I checked it every 5 second):\r\n```\r\nsrc/nutcracker --describe-stats\r\nThis is nutcracker-0.4.1\r\n\r\npool stats:\r\n  client_eof          \"# eof on client connections\"\r\n  client_err          \"# errors on client connections\"\r\n  client_connections  \"# active client connections\"\r\n  server_ejects       \"# times backend server was ejected\"\r\n  forward_error       \"# times we encountered a forwarding error\"\r\n  fragments           \"# fragments created from a multi-vector request\"\r\n\r\nserver stats:\r\n  server_eof          \"# eof on server connections\"\r\n  server_err          \"# errors on server connections\"\r\n  server_timedout     \"# timeouts on server connections\"\r\n  server_connections  \"# active server connections\"\r\n  server_ejected_at   \"timestamp when server was ejected in usec since epoch\"\r\n  requests            \"# requests\"\r\n  request_bytes       \"total request bytes\"\r\n  responses           \"# responses\"\r\n  response_bytes      \"total response bytes\"\r\n  in_queue            \"# requests in incoming queue\"\r\n  in_queue_bytes      \"current request bytes in incoming queue\"\r\n  out_queue           \"# requests in outgoing queue\"\r\n  out_queue_bytes     \"current request bytes in outgoing queue\"\r\n```\r\n\r\n```\r\nps aux | grep nut\r\nroot      5985  1.0  0.0  18584  2660 ?        Sl   11:04   0:44 src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222\r\n```\r\n\r\n```\r\nnetstat -tulpn | grep 22222\r\ntcp        0      0 0.0.0.0:22222           0.0.0.0:*               LISTEN      5985/nutcracker \r\n```\r\n\r\nBTW connection with redis took 0.15ms, connection with twemproxy took 0.03ms, but 4 pipes with redis was faster (1.0-1.15ms) than the same operations with twemproxy (1.0-1.5ms). I was hoping for more acceleration.\r\n\r\nAnd also how to understand the logs?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "filippog": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/540", "title": "Prometheus stats support?", "body": "Hi,\r\nwould you be interested in having native Prometheus stats support? I'm not sure what would be the best way implementation-wise (a separate port?).", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532", "title": "Double quotes in server name result in invalid json stats", "body": "Hi,\r\nwe're using nutcracker/twemproxy with server names with double quotes, though this results in invalid json because the double quotes are not escaped but should (`src/nc_stats.c` at `stats_begin_nesting` if I'm reading the code correctly)", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "praveenbalaji-blippar": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/539", "title": "Twemproxy (Redis) for large batch/small # of connections", "body": "I use Redis in our offline processing (large batch/small # of connections). I want to use twemproxy to partition data. I see two issues which I want to address:\r\n\r\n- I notice that the performance drops ~2x when I use twemproxy.\r\n- Of greater concern to me is that twemproxy seems to drop the connection intermittently. I tried various `--mbuf-size` ranging from the default (16K) to 16M.\r\n\r\nI looked through several Issues on github, but most use cases seem to be large # of connection/online use cases. I want to figure out the configuration to handle large batch/small # of connections instead. Suggestions are much appreciated.\r\n\r\n\r\nOn an 8-core machine, `top` shows that available memory is not a problem. However, twemproxy appears to consume >90% CPU. The Redis instances consume ~10% CPU. When I execute the same commands in one of the Redis instances, the Redis instance consumes >90% CPU.\r\n\r\nThe configuration is as follows:\r\n```\r\ntest:\r\n  listen: 0.0.0.0:6378\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  redis: true\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 1\r\n  servers:\r\n   - 127.0.0.1:7378:1\r\n   - 127.0.0.1:7379:1\r\n```\r\n\r\nRequest payloads have one of the following characteristics:\r\n\r\n- pipelined request with `del`. Each pipelined request has 200,000 - 2,000,000 `del` commands. Each key is ~10 bytes.\r\n- `mget` batches of size 100,000 - 1,000,000. Each key and value are ~10 bytes.\r\n- pipelined request with `set` and `sadd`. Each pipelined request has 200,000 - 2,000,000 commands (set + sadd). Each key and value is ~10 bytes.\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "inter169": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/538", "title": "OOM on the mbuf freelist (100+GB)", "body": "the nutcracker process consumed 100+GB phisycal memory on my production box after a data migration from another redis to this one (nutcracker).\r\nand the gdb console showed below:\r\n```\r\n(gdb) p nfree_mbufq\r\n$1 = 6365614\r\n(gdb) p mbuf_chunk_size\r\n$1 = 16384\r\n```\r\n\r\nthe memory consumption was nfree_mbufq * mbuf_chunk_size = 101GB approx. \r\nI have read some code fixes (pr)s about the similar phenomenon, like:\r\nhttps://github.com/twitter/twemproxy/pull/461\r\nhttps://github.com/twitter/twemproxy/issues/203\r\n\r\nbut such fixes didn't set the limitation of the mbuf chunks, so the OOM was still here, \r\nI coded a fix, and the nutcracker can pass a command param ('-n ',  in my fix) to set the max number of mbuf chunks, once exceeded the limitation it can free the mbuf after processing one req immediately.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jhwillett": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/537", "title": "Twemproxy hangs parsing nested Array responses from Redis", "body": "Twemproxy hangs when processing a Redis Lua script via EVALwhich returns nested sub-arrays. \r\nThis is easy to reproduce.\r\n\r\nSetup:\r\n* On Mac OS 10.12.6.\r\n* Running redis-server 2.8.24 at localhost:7379 (installed via Homebrew).\r\n* Running nutcracker-0.4.1 at localhost:221221 which is configured with:\r\n```\r\ndevelopment:\r\n  redis:            true\r\n  auto_eject_hosts: false\r\n  listen:           127.0.0.1:22121\r\n  servers:\r\n    - 127.0.0.1:7379:1\r\n```\r\n\r\nDemo of direct connection to Redis being happy:\r\n```\r\n$ redis-cli -p 7379\r\n127.0.0.1:7379> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379> EVAL 'return {1,{2},3}' 1 x\r\n1) (integer) 1\r\n2) 1) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379>\r\n```\r\n\r\nDemo of connection through Twemproxy being unhappy:\r\n```\r\n$ redis-cli -p 221221\r\n127.0.0.1:221221> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:221221> EVAL 'return {1,{2},3}' 1 x\r\n^C\r\n```\r\nThe last command hangs as long as I was willing to wait (several minutes), until I cancel it.\r\n\r\nI see no messages in the twemproxy logs during this time.\r\n\r\nI notice that src/proto/nc_redis.c has special cases for nested multi bulk reply element from sscan/hscan/zscan.  Perhaps eval and evalsha are simply missing the special case treatment?\r\n\r\nI apologize for phrasing this as a bug report instead of as a pull request with a fix, but I am afraid it may be a long time before I can dig deeper.\r\n\r\nThank you for a wonderful tool.  It is a critical piece of our infrastructure at ProsperWorks and has made our lives much easier.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/537/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BlueCatFlord": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/535", "title": " Whether the project is in maintenance", "body": "Whether the project is in maintenance", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslusher": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/534", "title": "twemproxy only sees one of the memcached servers in the pool", "body": "It's my understanding that when a server in the twemproxy pool gets ejected, the other server in the pool should still be available for caching. It seems that when I take out *memcached-1 only*, the proxy itself becomes unavailable. If I take out memcached-2 from the pool, everything operates normally, except that there doesn't seem to be any indication in the logs that the server leaves or returns to the pool. \r\n\r\nI have tested that both memcached servers are available directly. If I put one or the other memcached sever by itself in the pool configuration, they're available using the proxy, but only memcached-1 is available if I have them both in the pool. I've tried ordering them differently and it doesn't seem to make a difference. A tcpdump only ever shows traffic to memcached-1 when they are both in the pool. When nutcracker is restarted, I only see arp traffic going to one of the two servers, but never both.\r\n\r\nTo reproduce:\r\n(nutcracker version 0.4.1 on centos 7)\r\n/etc/nutcracker/nutcracker.yml\r\n``` yaml\r\nbad_pool:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  server_retry_timeout: 30000\r\n  server_failure_limit: 3\r\n  servers:\r\n   - 10.10.10.33:11211:1 memcached-1\r\n   - 10.10.10.34:11211:1 memcached-2\r\n```\r\n\r\ntelnet 127.0.0.1 22122\r\n```\r\nTrying 127.0.0.1...\r\nConnected to 127.0.0.1.\r\nEscape character is '^]'.\r\nset testing 1 0 3\r\none\r\nSTORED\r\n```\r\n\r\nssh 10.10.10.33:\r\n```\r\nsudo systemctl stop memcached\r\n```\r\n\r\ntelnet console:\r\n```\r\nget testing\r\nSERVER_ERROR Connection refused\r\nConnection closed by foreign host.\r\n```\r\n\r\nnutcracker logs for sequence:\r\n```\r\n[2017-08-18 11:08:46.894] nc_core.c:43 max fds 1024 max client conns 989 max server conns 3\r\n[2017-08-18 11:08:46.894] nc_stats.c:851 m 4 listening on '0.0.0.0:22222'\r\n[2017-08-18 11:08:46.894] nc_proxy.c:217 p 6 listening on '127.0.0.1:22122' in memcache pool 0 'bad_pool' with 2 servers\r\n[2017-08-18 11:08:56.457] nc_proxy.c:377 accepted c 8 on p 6 from '127.0.0.1:41122'\r\n[2017-08-18 11:09:11.595] nc_request.c:96 req 1 done on c 8 req_time 1160.716 msec type REQ_MC_SET narg 2 req_len 24 rsp_len 8 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:00.115] nc_response.c:118 s 9 active 0 is done\r\n[2017-08-18 11:14:00.116] nc_core.c:237 close s 9 '10.50.20.35:11211' on event 00FF eof 1 done 1 rb 8 sb 24\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close s 9 '10.50.20.35:11211' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n[2017-08-18 11:14:06.887] nc_request.c:96 req 4 done on c 8 req_time 0.597 msec type REQ_MC_GET narg 2 req_len 13 rsp_len 33 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close c 8 '127.0.0.1:41122' on event FF00 eof 0 done 0 rb 37 sb 41: Operation not permitted\r\n```", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/534/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Vibe6": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/531", "title": "Change to grow yr Power", "body": "Hello twitter I what to give you advice please change \"follow\" text on yr button type. Add or only show a + income with profile it will help you believe me.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/531/reactions", "total_count": 3, "+1": 0, "-1": 3, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mishtika": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/530", "title": "Twemproxy \" Connection refused \" on failure of one redis instance", "body": "I have configured twem proxy with 2 redis servers.\r\nWhen one of these redis server fails the twem proxy gives a error saying \" Connection refused\"\r\n\r\nMy twem conf \r\nbeta:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  hash_tag: \"{}\"\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  redis: true\r\n  servers:\r\n   - 127.0.0.1:6381:1\r\n   - 127.0.0.1:6379:1\r\n\r\ni am trying to get a key from redis. If i kill one instance of redis and then try to run the get command then it gives the following error:\r\n[2017-07-26 16:25:24.548] nc_core.c:237 close s 15 '127.0.0.1:6381' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuetianle": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/528", "title": "is it  support the docker redis service ", "body": "when use the docker redis service ,it don't work well.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rposky": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/526", "title": "Add additional timeout option to apply to time between server request and response", "body": "The current \"timeout clock starts ticking the instant the message is enqueued into the server [in queue]\" and not at the time that the message is actually sent to the server. While this may be appropriate for some use cases, I suggest that it is problematic for others that wish to react to server response time irrespective of server demand.\r\n\r\nI have observed that in the presence of many requests using the same key, and thus mapped to the same server, a denial of service scenario is possible due to timeout of requests that may not have even left the server in-queue. Server ejection naturally follows, despite that the proxied service is still running and responding normally. It just could not respond to all queued requests within the configured timeout, again some of which it may not have even technically began servicing.\r\n\r\nIt would be beneficial were there an additional timeout option that could be evaluated specifically against server response times and used to influence server ejections in lieu of server_timeout errors. In the meantime, I have found that adjusting the existing timeout option to a much larger value can help to mitigate this error scenario, though it unfortunately means that the service will be slower to detect actual server issues.\r\n\r\nThanks ahead of time for any consideration and/or suggestions.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "danielkraaijbax": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/525", "title": "Redis PING command is sometimes slow.", "body": "Hi There,\r\n\r\nWe have a setup of Twemproxy and 3 Redis masters with 3 Redis Slaves  and i have found an issue with the PING command that we were using.\r\nFirst of, We use the PING command to check if a Redis server is correctly connected. We found some issues with this in the past where we would connect to Twemproxy, ask for a key and that the server did not respond. We fixed that and is totally not related to this ticket.\r\nIn our codebase (PHP) there was however still a line where we would issue a PING command after each connect. \r\nWe were sometimes experiencing slowness on our pages. Somehow our connection wrapper managed to wait 200ms before continuing.\r\n\r\nI created a couple of test scripts to try and resolve this issue.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->ping();\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times. \r\n\r\n> for i in {1..20}; do php redis-ping.php; sleep 1; done\r\n202.45825195312\r\n0.625\r\n1.968994140625\r\n0.60693359375\r\n1.89501953125\r\n202.28198242188\r\n0.452880859375\r\n1.438720703125\r\n201.91772460938\r\n2.072021484375\r\n201.50170898438\r\n202.705078125\r\n0.783935546875\r\n2.90966796875\r\n1.487060546875\r\n0.831787109375\r\n0.52587890625\r\n0.593017578125\r\n0.634033203125\r\n1.114990234375\r\n\r\nI made some adjustments to the script.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->get('FooBar');\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times.\r\n> for i in {1..20}; do php redis.php; sleep 1; done\r\n1.2978515625\r\n1.47607421875\r\n2.22216796875\r\n2.47509765625\r\n2.26708984375\r\n1.2548828125\r\n1.25\r\n0.85986328125\r\n4.005859375\r\n3.97802734375\r\n0.876953125\r\n1.739990234375\r\n0.784912109375\r\n0.760986328125\r\n1.220947265625\r\n3.2080078125\r\n2.267822265625\r\n0.810791015625\r\n2.806884765625\r\n0.744140625\r\n\r\nSo it seems that the PING command is sometimes slow.\r\nI assume this could be an issue? \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kigster": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/523", "title": "On newer SmartOS Twemproxy chooses epoll instead of event ports", "body": "Looks like SmartOS now [implements `epoll`](http://blog.shalman.org/exploring-epoll-on-smartos/), as well as their native event ports.\r\n\r\nBuild system prefers `epoll` when both are available, but it would be better if it chose native `event ports`. We are currently experiencing random sporadic lockups of twemproxy built with `epoll` support. Currently testing the event ports version.\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vishalsharma13": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/522", "title": "Redis rename command", "body": "hi,\r\n\r\nWhy twemproxy not used rename command? Is there any possibilty to use rename command through twemproxy.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516", "title": "Twemproxy Server Timedout", "body": "Hi,\r\n\r\nCan you please help to identify the reason of error with redis server.\r\n\r\nTwemproxy logs :- \r\n\r\n[2017-02-09 12:06:24.188] nc_request.c:96 req 219840 done on c 27903 req_time 1000.594 msec type REQ_REDIS_HMGET narg 7 req_len 117 rsp_len 27 key0 'USER:SESSION:8c5ab24a-c0b0-4ccd-a377-02c0b1d728ed' peer '10.247.74.50:58653' done 1 error 1\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1\r\n  timeout: 500\r\nbeta:\r\n  auto_eject_hosts: false\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 20000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 1000\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512", "title": "Twemproxy : Server Connections not closed properly", "body": "Hi,\r\n\r\nI am using Redis with 3 servers configuration i.e one master(to write data) and two slaves(to read data) and using Twemproxy for load balancing.\r\n\r\nAnd using Jedis with connection pooling.\r\n\r\nNow i doing load testing but i am facing below errors\r\n\r\n1) Twemproxy Server connections are not properly closed on time and increased upto double or triple of  request(Like for 10k requests server connections opened upto 30k connections)\r\n\r\n2) Getting exception like unable to get resource from pool however my poolsize limit is not exceeded and after one command connection returns back to pool.\r\n\r\nServer Configurations :- \r\n\r\n1) 64GB RAM\r\n2) 16 core processor\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1\r\n  timeout: 5000\r\nbeta:\r\n  auto_eject_hosts: true\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 50000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 5000\r\n\r\n\r\nJedis Pool Configuration :- \r\n\r\npublic class RedisReadManager {\r\n    private static final RedisReadManager instance = new RedisReadManager();\r\n    private static JedisPool pool;\r\n    private RedisReadManager() {}\r\n    public final static RedisReadManager getInstance() {\r\n        return instance;\r\n    }\r\n    public void connect(String host, int port) {\r\n        try {\r\n        \tif(pool==null || pool.isClosed()){\r\n\t\t\t\tJedisPoolConfig poolConfig = new JedisPoolConfig();\r\n\t\t\t\tpoolConfig.setMaxTotal(60000);\r\n\t\t\t\tpoolConfig.setTestOnBorrow(true);\r\n\t\t\t\tpoolConfig.setTestOnReturn(true);\r\n\t\t\t\tpoolConfig.setMaxIdle(3000);\r\n\t\t\t\tpoolConfig.setMinIdle(100);\r\n\t\t\t\tpoolConfig.setTestWhileIdle(true);\r\n\t\t\t\tpoolConfig.setNumTestsPerEvictionRun(10);\r\n\t\t\t\tpoolConfig.setTimeBetweenEvictionRunsMillis(10000);\r\n\t\t\t\tpool = new JedisPool(poolConfig, host, port);\r\n        \t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\t\t\t\r\n\t\t}\r\n    }\r\n    public void releasePool() {\r\n        try {\r\n\t\t\tif(pool!=null&&!pool.isClosed()){\r\n\t\t\t\tpool.destroy();\r\n\t\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n    public Jedis getJedis() {\r\n        return pool.getResource();\r\n    }\r\n\r\n\tpublic void returnJedis(Jedis jedis) {\r\n    \ttry {\r\n\t\t\tif(jedis!=null&jedis.isConnected()){\r\n\t\t\t\tjedis.disconnect();\r\n\t\t\t\tjedis.close();\r\n\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n}\r\n\r\nNutcracker-web :- \r\n\r\n![nutcracker-web](https://cloud.githubusercontent.com/assets/22707039/22018052/ebaf5d92-dcd2-11e6-91c6-463af69e960a.jpg)\r\n\r\nPlease help to resolve the issue.\r\n \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ArcticSnowman": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/521", "title": "Any recommendations For Using KeepaliveD with Twemproxy?", "body": "Anyone got recommendations For Using KeepaliveD with Twemproxy?\r\n\r\nUse a simple tcp check against the port?  Something more sophisticated with a check on the stats port?\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518", "title": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands", "body": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands.\r\n\r\n`redis-cli GET <key>`  does not work for twemproxy as redis-cli issues a `COMMAND` command to check that you have a valid command to send. \r\n\r\nThis works in interactive mode...\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mcanonic": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/519", "title": "Support for BRPOP and BLPOP", "body": "Hi,\r\nI saw the notes/redis.md file and I'm wondering if the unsupported commands are somethin that you are working on it or not. Just to know if the (near?) future these commads like BRPOP e BLPOP will be supported.\r\nThanks\r\nM", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hjhart": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/514", "title": "Twemproxy cluster not accepting clients", "body": "On a machine running twemproxy, it is no longer accepting client connections. We're not totally sure why, at this point. The logs look interesting enough, ending with an `eof`. I think I'm just not understanding 1) how twemproxy would ever \"go down\" and 2) choose not to come back up.\r\n\r\nThe redis machines are responsive, because we have five other twemproxy machines with exactly the same configuration and they are still up. There is a process that runs JUST before this problem occurs, and it is hitting redis incredibly hard. We're just not sure conceptually _why_ twemproxy goes down, and what configuration we can change to make it stop. \r\n\r\nUnderstanding, of course, that we are hitting redis incredibly hard from a single process and we could choose _not_ to do that, but we want to first understand why twemproxy goes down, and then why it _stays_ down.\r\n\r\nAnother thing to note, is that we've recently updated from twemproxy 0.3.0 to 0.4.1, and this didn't appear to happen on 0.3.0.\r\n\r\nThe end of the logs looks like this: \r\n\r\n## Logs\r\n\r\n```\r\n> tail /var/log/twemproxy-daily-sale.log\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782534 done on c 30 req_time 27.163 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20888845:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782535 done on c 30 req_time 27.181 msec type REQ_REDIS_DEL narg 2 req_len 34 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782536 done on c 30 req_time 27.200 msec type REQ_REDIS_RPUSH narg 12 req_len 177 rsp_len 5 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782539 done on c 30 req_time 27.209 msec type REQ_REDIS_EXPIRE narg 3 req_len 49 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782540 done on c 30 req_time 27.227 msec type REQ_REDIS_DEL narg 2 req_len 35 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782541 done on c 30 req_time 27.246 msec type REQ_REDIS_RPUSH narg 3 req_len 51 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782544 done on c 30 req_time 27.256 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:57.393] nc_connection.c:360 recv on sd 30 eof rb 830971428 sb 32948345\r\n[2017-01-27 05:44:57.393] nc_request.c:430 c 30 is done\r\n[2017-01-27 05:44:57.393] nc_core.c:237 close c 30 '10.100.17.85:56791' on event 00FF eof 1 done 1 rb 830971428 sb 32948345\r\n```\r\n\r\n## Configuration\r\n\r\nOur configuration on twemproxy looks like this:\r\n<details>\r\n  <summary>Click to expand configuration</summary>\r\n\r\n\r\n```\r\n> cat /etc/twemproxy/twemproxy-daily-sale.yml\r\ndaily-sale:\r\n  hash: fnv1_32\r\n  hash_tag: \"::\"\r\n  distribution: ketama\r\n  timeout: 1000\r\n  auto_eject_hosts: false\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 3\r\n  redis: true\r\n  listen: 10.100.18.248:22139\r\n  servers:\r\n  - 10.100.17.85:37001:1 daily-sale001\r\n  - 10.100.17.85:37002:1 daily-sale002\r\n  - 10.100.17.85:37003:1 daily-sale003\r\n  - 10.100.17.85:37004:1 daily-sale004\r\n  - 10.100.17.241:37005:1 daily-sale005\r\n  - 10.100.17.241:37006:1 daily-sale006\r\n  - 10.100.17.241:37007:1 daily-sale007\r\n  - 10.100.17.241:37008:1 daily-sale008\r\n  - 10.100.18.19:37009:1 daily-sale009\r\n  - 10.100.18.19:37010:1 daily-sale010\r\n  - 10.100.18.19:37011:1 daily-sale011\r\n  - 10.100.18.19:37012:1 daily-sale012\r\n  - 10.100.18.100:37013:1 daily-sale013\r\n  - 10.100.18.100:37014:1 daily-sale014\r\n  - 10.100.18.100:37015:1 daily-sale015\r\n  - 10.100.18.100:37016:1 daily-sale016\r\n```\r\n\r\n</details>\r\n\r\n## Connections\r\n\r\n`netstat -an` shows that there is a single `CLOSE_WAIT` connection stuck.\r\n\r\n```\r\n> netstat -an | grep 22139\r\n10.100.18.248.22139        *.*                0      0 1048576      0 LISTEN\r\n10.100.18.248.22139  10.100.104.187.23453 1049792      0 1049800      0 CLOSE_WAIT\r\n```\r\n\r\nAnd on the corresponding machine (10.100.104.187) there is a `SYN_SENT` connection stuck:\r\n\r\n```\r\n> netstat -an | grep 10.100.18.248\r\n10.100.104.187.28052 10.100.18.248.22136  1049792      0 1049800      0 ESTABLISHED\r\n10.100.104.187.38585 10.100.18.248.22139      0      0 1049740      0 SYN_SENT\r\n10.100.104.187.34939 10.100.18.248.22135  1049792      0 1049800      0 ESTABLISHED\r\n```\r\n\r\n## Statistics\r\n\r\nOur statistics do not indicate any servers are out of the hash ring...\r\n\r\n<details>\r\n  <summary>Click to expand statistics</summary>\r\n\r\n```\r\n> nc localhost 22227 | json\r\n{\r\n  \"service\": \"nutcracker\",\r\n  \"source\": \"twemproxy100.prod\",\r\n  \"version\": \"0.4.1\",\r\n  \"uptime\": 90618,\r\n  \"timestamp\": 1485566119,\r\n  \"total_connections\": 459796,\r\n  \"curr_connections\": 17,\r\n  \"daily-sale\": {\r\n    \"client_eof\": 3927,\r\n    \"client_err\": 455852,\r\n    \"client_connections\": 0,\r\n    \"server_ejects\": 0,\r\n    \"forward_error\": 0,\r\n    \"fragments\": 0,\r\n    \"daily-sale001\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 499925,\r\n      \"request_bytes\": 52043467,\r\n      \"responses\": 499925,\r\n      \"response_bytes\": 2109629,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale002\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 425855,\r\n      \"request_bytes\": 44208662,\r\n      \"responses\": 425855,\r\n      \"response_bytes\": 1791852,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale003\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 516402,\r\n      \"request_bytes\": 53579106,\r\n      \"responses\": 516402,\r\n      \"response_bytes\": 2170560,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale004\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 571640,\r\n      \"request_bytes\": 59475059,\r\n      \"responses\": 571640,\r\n      \"response_bytes\": 2405288,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale005\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 488641,\r\n      \"request_bytes\": 50937149,\r\n      \"responses\": 488641,\r\n      \"response_bytes\": 2060132,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale006\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 595062,\r\n      \"request_bytes\": 61596411,\r\n      \"responses\": 595062,\r\n      \"response_bytes\": 2514791,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale007\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 539622,\r\n      \"request_bytes\": 56245033,\r\n      \"responses\": 539622,\r\n      \"response_bytes\": 2274943,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale008\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 450732,\r\n      \"request_bytes\": 47025684,\r\n      \"responses\": 450732,\r\n      \"response_bytes\": 1898874,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale009\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 461595,\r\n      \"request_bytes\": 47897394,\r\n      \"responses\": 461595,\r\n      \"response_bytes\": 1938910,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale010\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 502702,\r\n      \"request_bytes\": 52229759,\r\n      \"responses\": 502702,\r\n      \"response_bytes\": 2124118,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale011\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 489467,\r\n      \"request_bytes\": 50950224,\r\n      \"responses\": 489467,\r\n      \"response_bytes\": 2060700,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale012\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 510860,\r\n      \"request_bytes\": 53141344,\r\n      \"responses\": 510860,\r\n      \"response_bytes\": 2145320,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale013\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 446362,\r\n      \"request_bytes\": 46440311,\r\n      \"responses\": 446362,\r\n      \"response_bytes\": 1872555,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale014\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 427993,\r\n      \"request_bytes\": 44477521,\r\n      \"responses\": 427993,\r\n      \"response_bytes\": 1803340,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale015\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 508857,\r\n      \"request_bytes\": 52884727,\r\n      \"responses\": 508857,\r\n      \"response_bytes\": 2141577,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale016\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 564176,\r\n      \"request_bytes\": 58592099,\r\n      \"responses\": 564176,\r\n      \"response_bytes\": 2376014,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n</details>\r\n\r\n## Connecting\r\n\r\nAnd similarly trying to establish a connection to twemproxy hangs indefinitely...\r\n\r\n```\r\n> telnet 10.100.18.248 22139\r\nTrying 10.100.18.248...\r\n```\r\n\r\nAny and all help is appreciated. Thank you!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/514/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jennyfountain": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/513", "title": "Questions about twemproxy and performance", "body": "We are seeing a few issues that I was hoping someone could help me resolve or point me in the right direction.\r\n\r\n1.  During high loads, we are seeing a lot of backup in the out_queue_bytes.  On normal traffic loads, this is 0. \r\n\r\nExample (sometimes goes into 2k/3k range as well):\r\n    \"out_queue_bytes\": 33\r\n    \"out_queue_bytes\": 91\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 174\r\n\r\nIn addition, it shows that our time spent in memcache goes up from 400 ms to 1000-2000 ms. This seriously affects our application. \r\n\r\n2.  auto eject also seems to not work as expected. Server goes down and our app freaks out - saying it cannot access a memcache server.\r\n\r\nhere is an example of a config:\r\n\r\nweb:\r\n  listen: /var/run/nutcracker/web.sock 0777\r\n  auto_eject_hosts: true\r\n  distribution: ketama\r\n  hash: one_at_a_time\r\n  backlog: 65536\r\n  server_connections: 16\r\n  server_failure_limit: 3\r\n  server_retry_timeout: 30000\r\n  servers:\r\n   - 1.2.3.4:11211:1\r\n   - 1.2.3.5:11211:1\r\n   - 1.2.3.6:11211:1\r\n  timeout: 2000\r\n\r\nsomaxconn = 128\r\n\r\nWhat we tried and didn't help.\r\n1.  mbuf to 512\r\n2.  server connection from 1 to 200\r\n\r\nThank you for any guidance on this problem.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "manjuraj": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/34eb60fb977da9aa8bcac784dee1e7fb04c79d47", "message": "Merge pull request #517 from takayamaki/fix_typo\n\nfix typo in README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0f9e1baf06db478065ba43402b975c57a567d00f", "message": "Merge pull request #492 from kalifg/patch-1\n\nFix typo in notes/memcache.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6fd922039bc696e07b7ef18946732df6b7cf2a5d", "message": "Merge pull request #493 from dennismartensson/patch-1\n\nUpdate README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/017d44503344ecdf2439747476556f4b9fee3e21", "message": "Merge pull request #494 from mortonfox/patch-1\n\nUpdate the sensu-metrics link"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e5739338ddf228b3b900aa116646cb9654fa5f65", "message": "Merge pull request #489 from rohitpaulk/update-redis-docs\n\nUpdate redis docs for PING and QUIT"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/74af2fb2d5d3e214d8c0741a4b0ebb7d93572fc8", "message": "Merge pull request #439 from Krinkle/patch-1\n\nreadme: Link to HTTPS for wikimedia.org"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9a2611e5992e65e6c15cd23bce06ffed3901f4f8", "message": "Merge pull request #425 from charsyam/feature/typos\n\nfix typos"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/619b55b6a1dcd21ffe16f312b52cd576c6a4caa0", "message": "Merge pull request #421 from charsyam/feature/upgrade-redis-lib\n\nupgrade redis-py version from 2.9.0 to 2.10.3"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/592c1b6b157721f66789bca6a3d463f32e6c59fc", "message": "Merge pull request #420 from esindril/master\n\nAdd script to build SRPM and fix spec file"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/defda674cdfb94fef31bdf8cc7a1dd766ea0a05a", "message": "Merge pull request #418 from twitter/revert-406-bugfix/redis-error-response-parser\n\nRevert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/65bb2ac5bbf0acce3fa52469752046c388e56ef5", "message": "Revert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/34f369ff91076b5d1b1229eca51bc3a2ed8f6d4a", "message": "Merge pull request #406 from tom-dalton-fanduel/bugfix/redis-error-response-parser\n\nFix parsing bug when error body contains no spaces"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954", "body": "I would be willing to take patches for kqueue support in twemproxy\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968", "body": "Thank you for the patches. I will merge this in the next version\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668", "body": "twemproxy does not do replication. If you are doing replication, you would have to do it on the client side\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106", "body": "> 1. Before you started work on twemproxy, did you consider using moxi: https://github.com/steveyen/moxi? If yes, why did you reject moxi?\n>    with nutcracker we have:  \n>    a) protocol pipelining which works really well for us, as we wanted to introduce minimal latency degration with a proxy sitting between a client and server.\n>    b) mbuf, which essentially enables zero copy when moving data from client to server (and vice versa)\n>    c) observability which was fairly important to us in our production environment\n> 2. Are you planning to support binary protocol in addition to ascii?\n>    no (see 'thoughts' section in notes/memcache.txt)\n> 3. Do you see any need for multi-threaded support? Moxi supports both single and multi-threaded configurations.\n>    no; if a run proxy of proxy (client --> proxy --> (proxy)+ --> server) you can actually make use of all cores and would probably be network bound in this scenario\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681", "body": "This is a good idea.\n\nThe start up time of twemproxy is really small, especially when preconnect is not set. So, if your clients had retries built into it, then you can trivially propagate configuration changes by doing a rolling restarts of twemprox'ies. This is a good enough solution, imo\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486", "body": "> > Would you be open to a patch for this?\n> > I am always open to accepting patches :) \n\nI just think that reloading of configuration file on-the-fly is tricky to get right.\n\n> >  The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n\nI am curious as how this is happening. I set SO_REUSEADDR address option on the twemcache's listening socket, which means that we would reuse ports even if they are busy (See: proxy_reuse() function in nc_proxy.c)\n\nCould you paste me the log file dump with the error that you seeing? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606", "body": "Thanks for the patch using libevent API. \n\nI am willing to accept patches if they don't use libevent but directly call bsd's kqueue API. All you might have to do is to implement abstraction in the event interfaces, which would be in nc_event.[ch]. Let if know if you have any questions. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377", "body": "you can always change the ulimit of the shell from which you launch nutcracker\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857", "body": "blake the patch looks great. One thing I realized is that memcache has some item header overhead and slab header overhead even for the largest item. So, even if you configure memcache with 1MB slab, the largest item that can be stored in the slab is < 1MB. Furthermore the item size not only includes the value length, but also key length. \n\nGiven this, do you think we should have two extra keys in yml configuration\nitem_max_kvlen: (maximum key + value length)\nitem_overhead:\n\nand we discard requests whose key + value length + overhead > item_max_kvlen\n\nthoughts?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944", "body": "the reason I don't use libevent api is because I wanted to build nutcracker without any dependency on 3rd party libraries. furthermore, I use ET semantics, which I believe is not available in 1.4 version of libevent.\n\ncould i get a pull request of your changes?\n\nI also looked at your changes. I think you might want to do few cleanups before submitting a pull request. Few suggestions:\na) abstract out the event interface and wire to the underlying event call using a function pointer\nb) maybe create event/ directory that contains event/nc_epoll.c and event/nc_kqueue.c files. \nc) follow style conventions outlined in: https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133", "body": "> SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nIs there a test case that would reproduce this scenario? I am unable to reproduce this scenario, even if I bombard twemproxy with a constant stream of traffic and restart it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361", "body": "blake, I wonder if this issue would go away if we set l_linger = 0 by calling nc_set_linger(sd, 0) on the listening sockets (see notes/socket.txt for details on it)? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080", "body": "Thanks for the test case; this is really useful\n\nI am still actively working on the redis branch. hopefully we should have a stable build out by end of this month\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159", "body": "should be straight forward if aws conforms to memcache ascii protocol\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813", "body": "is there a reason why you would prefer inline command over unified command besides the obvious ease of issuing such command from telenet;\n\njust supporting unified protocol makes parsing for req / rsp easy and simple in twemproxy.\n\nif you would like to contribute, you could look at nc_parse.[ch]\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651", "body": "twemproxy should just work with kestrel. I guess you might be well off using \"distribution: random\" instead of default of \"distribution: ketama\" for a pool of kestrel severs\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677", "body": "apologies @jsholmes ; I am still working on this\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/1694665", "body": "I am still actively testing this branch...I should have a stable build out in few weeks.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/1694665/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829605", "body": "hmm...this is a wrong commit from my end. ignore this. apologies\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829605/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829681", "body": "I chose the verbose way because the log message prints the line number which helps in deciphering the condition that was triggered\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536", "body": "No need for a py script. You can generate the strings using macro magic called strigificaion :) This way your code never gets outdate\n\nSee this for reference -- \n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L23-L49\n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L117-L122 \n- http://gcc.gnu.org/onlinedocs/cpp/Stringification.html\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553", "body": "s/char */uint_8 */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558", "body": "usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586", "body": "space between the closing paren and curly brace. So `log_loggable(LOG_NOTICE) {`\nHere are the coding guidelines -- https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt. Would really appreciate if you follow it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603", "body": "just call it req_log()\n\nalso format is `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612", "body": "formatting -- `/* a fragment */`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643", "body": "formatting - `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649", "body": "formatting - `if (rsp) {`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660", "body": "call this `int_64_t start_ts /\\* request start timestamp in usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410", "body": "Rename\ntotal_connections to ntotal_conn\ncurr_connections to nconn\ncurr_client_connections to nclient_conn\n\ncurr_connections should be int32_t\ncurr_client_connections should be int32_t\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414", "body": "how did we come up with RESERVED_FDS number as 32\nrename it as NC_NUM_RESERVED_FD\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416", "body": "I think we might still hit this scenario - One instance where I see this happening is that you close the connection() from the code (decrement the counters), but the sockets are still in TIME_WAIT state and hence are not available for accept() sys call.\n\nSo, instead of panic, we should log() and return NC_ERROR and not set p->recv_ready to 0\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121", "body": "what is this for?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123", "body": "why did we bump this up?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127", "body": "[formatting] `status: %d`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172", "body": "is get_mbuf() function required? It seems to be called only in msg_make_reply(). Prefer not having this function\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178", "body": "why are these commented out? Isn't it required to trigger the out event?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216", "body": "Should we move the lines 482 - 495 into server_pool_idx()? It seems that only place where the tag logic is not used is in server_pool_server(), which is surprising, because I think that tag logic should also apply there equally.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257", "body": "In this piece of code, you allocate message structs for every server in the pool - ncontinuum messages, even if incoming messages are destined to < ncontinuum servers.\n\nOnly at line 818-819, do you some of these allocated message structs. Can we refactor this differently - maybe two loops where the first loop aggregates all the keys destined per server and the second loop has the logic\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}]}, "mortonfox": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/fe68175e0200e3c2589139438ff3efa392042aa6", "message": "Update the sensu-metrics link"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dennismartensson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/80ef6a7444fd5ae97fcab9606c1abedc19f00824", "message": "Update README.md\n\nAdded Greta to the list of companys"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kalifg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b87ba1abfe6a814999279e69af7ce07ba0ff6c68", "message": "Fix typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohitpaulk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/eed195341a02fa688b0dfb784d120f337f15a454", "message": "Update redis docs for PING and QUIT\n\nSupport for these was introduced in\n@4175419288ef66d95e082cfa2124e77fe6d4fe6d."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "andyqzb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/330f43a430261aa48d4063771ed70fe191177154", "message": "Merge pull request #486 from deep011/deep011-patch-1\n\nfix a memory leak bug for mset command"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ced2044980e4b9dd0c91b25fc64ce127879a1491", "message": "Merge pull request #484 from postwait/patch-1\n\nFix typo circunous -> circonus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "deep011": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/558e0d40ad79f423c4784565648e6c83cf035777", "message": "fix a memory leak bug for mset command"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/462", "title": "Fix a bug for msg->mlen", "body": "Function msg_append() already added the dst->mlen.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10512396", "body": " I think it would be better to support \u201cdelete key 0\\r\\n\u201d command, because memcached support this command. What is your opinion?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10512396/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "postwait": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1e078e9e9d97560825ae4f1245177a0af29e3c82", "message": "Fix typo circunous -> circonus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Krinkle": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/91a68d3c42638eb8178001f4d67d2606dcd80f51", "message": "readme: Link to HTTPS for wikimedia.org"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "esindril": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/51c5228acdd8a324a43107330f6b936de028dc0c", "message": "Fix spec file to work for RHEL >= 6 and wrong changelog date"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/205323f87deb0f0004963717f2d7a80eed8e9c3b", "message": "Add script to build the SRPM package for RPM-based distributions"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "caniszczyk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/0e8708be4d365163a1061c2a89ecc344e21203d6", "message": "Add Uber to the list of Adopters\n\nhttp://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128", "body": "Any updates on this patch @manjuraj ?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280", "body": "Just to note that this is somewhat of a hack, but it works for now. I didn't find any tests to run via gtest or whatever harness.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "idning": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1f4e0dae62baa379209e4cdfe64f2224dadce632", "message": "Merge pull request #410 from vincentve/master\n\noptimize performance when single key del"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350", "body": "if current msg is a sub-msg, the `msg->frag_owner` may be freed, and reset by `_msg_get`, this is not correct, especially for `req_error`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378", "body": "macro create clean code but break cscope/ctags :( \n\nclean code is more important, I will have a try.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "invalid-email-address": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b6ac1bbc34aa0f470b55893090c7cf6a696ec184", "message": "optimize performance when single key del"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tom-dalton-fanduel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/6f32a928b4830d218fef67aa5d22bc0fd44000f6", "message": "Add testcase"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TysonAndre": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/545", "title": "Always initialize file permissions field for unix domain socket", "body": "It seems like field->perm might be uninitialized memory\r\ndepending on how it is allocated.\r\nThe intended behavior is to only change file permissions from the default if a permission was specified in the config:\r\nhttps://github.com/twitter/twemproxy/pull/311/files#diff-f74ea9da930e79a9573455a0cbe4785d\r\n\r\nI ran into an issue where different sockets had different file\r\npermissions, and some of those sockets weren't readable by the user\r\nwhich created it. (I specified *only* the path to the unix domain socket)\r\n\r\nThis behavior probably started in\r\nhttps://github.com/twitter/twemproxy/pull/311/files", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phamhongviet": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/544", "title": "Add systemd service file", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "essanpupil": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/541", "title": "fix list indentation in README", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "idirouhab": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/536", "title": "Add Foodora as company who uses it in prod", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pataquets": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/524", "title": "Add Docker support", "body": "Add Dockerfile to enable image building.\r\nUsing the official GCC image, latest tag. More info at https://hub.docker.com/_/gcc/\r\n\r\nJust adding files, setting working dir and running make instructions.\r\n\r\nBuild:\r\n\r\n```\r\n$ docker build -t twemproxy .\r\n```\r\n\r\nRun:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro twemproxy -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nFYI, there's a still quicker to test, already built image on my Docker Hub. Test it by running:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro pataquets/twemproxy  -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nUsing `--rm` instead of `-d` makes the container not go background and it to be deleted after stop. Should stop by `CTRL+C`'ing it.\r\nIn order for the Docker container to connect to external memcached or Redis instances, either them should be contactable as external IPs or hosts or be linked to other previously run Docker containers via Docker's ```--link``` option.\r\n\r\nHere it is an example Docker Compose file I'm using (I can submit it with the PR also if you find it useful):\r\n```\r\nredis1:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63791:6379\r\n\r\nredis2:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63792:6379\r\n\r\ntwemproxy:\r\n  image: pataquets/twemproxy\r\n  command: -c /etc/nutcracker.yml --verbose=6\r\n  links:\r\n    - redis1\r\n    - redis2\r\n  ports:\r\n    - 6379:6379\r\n    - 22222:22222\r\n  volumes:\r\n    - ./conf/nutcracker.redis.yml:/etc/nutcracker.yml:ro\r\n```\r\nNotice that it is tuned for two Redis instances (yml file not included, mount yours)\r\n\r\nOptional improvement to come (maybe in another issue):\r\n- Create an 'official', based on your repo, automated build at Docker Hub for the image: https://docs.docker.com/docker-hub/builds/ . Just requires a free Docker Hub account and a following a quick 'Create automated build' process. I'll be happy to help on it, if needed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "galusben": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/520", "title": "JFrog is using twemproxy on production", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "santoshsahoo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/515", "title": "Added CircleHD to companies using Twemproxy", "body": "Updated README.md, we are using of twemproxy on aws.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "voetsjoeba": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/511", "title": "Fix build failure with --disable-stats", "body": "A build with ./configure --disable-stats currently fails with the following error (gcc 4.8.5 on CentOS 6):\r\n\r\n```\r\nnc_server.c: In function \u2018server_failure\u2019:\r\nnc_server.c:291:5: warning: implicit declaration of function \u2018stats_server_set_ts\u2019 [-Wimplicit-function-declaration]\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n     ^\r\nnc_server.c:291:38: error: \u2018server_ejected_at\u2019 undeclared (first use in this function)\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n                                      ^\r\nnc_server.c:291:38: note: each undeclared identifier is reported only once for each function it appears in\r\n```\r\n\r\nThe reason appears to be that some functions are not being nopped out in `nc_stats.h` when `NC_STATS` is 0. This change adds the missing entries. Technically only `stats_server_set_ts` is needed to fix the build error, but from the intent of the code it's clear that `stats_pool_set_ts` should also be nopped out, despite never being called directly.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/510", "title": "Allow negative exptime values in memcached storage commands", "body": "Memcached allows negative exptime values in storage commands to immediately expire the stored values, but this wasn't very well documented until recently (here's the commit that adds the documentation from May 2016: https://github.com/memcached/memcached/commit/e7d4521cd8b27f7ebc6e4c1b9aee9eb3544f6af5).\r\n\r\nThis patch allows the memcached parser to support negative exptime values in storage commands. The server still responds with the usual STORED if the exptime is negative, so no additional response handling is needed.\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ugurengin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/502", "title": "update memcache populate script", "body": "update memcache populate script to improve test cause of memcached for get and set operations", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mahdi-hdi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/497", "title": "Added Geo Functionality", "body": "New Redis functionality for geo date type now supported:\n\n```\nGEOADD\nGEODIST\nGEOHASH\nGEOPOS\nGEORADIUS\nGEORADIUSBYMEMBER\n```\n\n**Mutli bulk array** was implemented in Redis response parser to support response of **GEORADIUS**. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "willfitch": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/480", "title": "Implement ERROR protocol", "body": "Rather than severing connections upon an invalid event, this patch adds the ERROR portion of the Memcached protocol.  This does not, however, add CLIENT_ERROR or SERVER_ERROR.  That can be discussed.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "VishalRocks": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/476", "title": "Update README.md", "body": "Added Codechef in Companies using Twemproxy in Production\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "artursitarski": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/474", "title": "Wikia as twemproxy user", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flygoast": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/463", "title": "Implemented client connections limiting of server pool.", "body": "Implemented client connections limiting of server pool.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "huachaohuang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/461", "title": "Fix memory leak for redis mset.", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charsyam": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/458", "title": "Fix Bug: redis_auth don't work with redis_db", "body": "currently, twemproxy ignore \"redis_db\" when \"redis_auth\" is set.\nselect command is set as noforward because of auth.\n\nthis patch fix this.\nand move add_auth to post_connected.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/428", "title": "rebase heartbeat branch ", "body": "It had a bug when redis is loading.\nbut after patch of https://github.com/twitter/twemproxy/commit/ef453130e321974b332dd99d585ae7285eee4b5d\n(handle loading state as error)\nI think it fixed.\n\nIn my tests. this works fine :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/423", "title": "Fix parsing bug when error body contains no spaces", "body": "This is only copy for https://github.com/twitter/twemproxy/pull/406 (by @tom-dalton-fanduel )\nfor testing. I just PR again.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/395", "title": "support nested multibulk reply", "body": "@manjuraj This is part of https://github.com/twitter/twemproxy/pull/393\n\nfirst. I divided it that is only supporting nested multi bulk reply.\n\nafter merging this :)\n\nI will PR second of them :)\n\nThanks for your reivew :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/393", "title": "support geo commands and nested multibulk reply.", "body": "1. support GEO Commands\n   -> geoadd, geohash, geodist, georadius, georadiusbymember\n2. support nested multibulk reply\n   -> geo commands return nested multibulk reply\n\ncurrently we only support this type of multibulk\n\n```\n                 * - mulit-bulk\n                 *    - cursor\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n```\n\nbut georadius and georadiusbymember returns maximum 3 depth multibulk. so this patch supports this kind of multibulk also.\n\n```\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n```\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031155", "body": "@BrandonBrowning Could you give me a script to test this? Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10032839", "body": "@BrandonBrowning Thank you. I found the reason. and I will fix it soon. Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10032839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10033615", "body": "@BrandonBrowning could you try this version?\nhttps://github.com/charsyam/twemproxy/tree/feature/issue-323\nThank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10033615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572", "body": "How was just initializing with CONF_DEFAULT_SELECT?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575", "body": "I just think that add new conf file for this patch like this: nutcracker.select.yml\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579", "body": "in twemproxy, it is better to change the variable name to is_select_msg\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597", "body": "and conn->sd is nonblocking socket. so write can be failed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "ofirule": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/435", "title": "Added support for listening to same port from multiple twemproxy processes", "body": "My team (from IBM Security Trusteer) has ran into some bottlenecks when using twemproxy and developed this patch.\n\nThis patch allows to use socket option SO_REUSEPORT on twemproxy socket. This option is available on Linux kernel version 3.9+.\n\nAfter stress testing our redis-server we found that handling many clients consumes too much processing from redis but not redis code itself rather its networking code (epoll/etc.).\n\nIt seems that the single thread nature of redis maxes out the networking part with one core in extreme conditions (like many connections scenario, no pipelining, etc.).\n\nUsing this patch we can connect many twemproxy instances (which runs on the same computer) to a redis server using the same socket, we were able to get a much greater throughput and still have one logical redis server. \n\nThese twemproxy instances can all be configured with the same configuration file, and only needs to have a different stats port. (we don't enable SO_REUSEPORT for that socket)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rosmo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/431", "title": "Allow ignoring SELECT from client", "body": "Even if you are just using database 0, some developers want to call Redis' SELECT command always up front. This add a new redis_ignore_select option to pools to simply return OK for any SELECT command.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "umegaya": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/424", "title": "add support of script load command by broadcast", "body": "refs #68 \nthis pull request aims to add support for SCRIPT LOAD command, by following fix.\n- define msg type for SCRIPT LOAD and parse correctly (8158d2e)\n- able to give 'broadcast' attribute to certain kind of msg type, and give broadcast attribute to SCRIPT command (8158d2e)\n- do broadcast correctly (cb50953)\n\nalso original python test seems to be broken, add new test by shell script (dfba9f8), you can run new test like following:\n\n```\ncd tests/test_redis_sh\n./run ./test_script_load.sh\n```\n\nit is only passed limited test case, and I'm very new to twemproxy, so not enough confidence about correctness. can you review and if it looks good, merge to master? thanks in advance.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dec5e": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/412", "title": "Removing duplicate Hootsuite mention in list of companies using Twemproxy", "body": "Hootsuite was added twice: in #291 and #387.\n\nPS. Maybe it's good to sort companies list alphabetically to prevent such duplication in future. I can do it if you agree.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ideal": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/411", "title": "add rbtree_entry to fetch a struct pointer from a rbtree node pointer", "body": "So the code is more generic.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "susman": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/400", "title": "add rhel7 compatibility", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ton31337": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/396", "title": "Add SO_REUSEPORT support to socket", "body": "Add SO_REUSEPORT support to socket. Introduce new feature for Twemproxy running on newer kernels. With this feature you are able to do upgrades, restarts without any downtime. Kernel does load balancing between processes with the same host:port pairs. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nanzhushan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/391", "title": "add", "body": "\u4e2d\u6587\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "travisbot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1743714) (merged bd16a4cc into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1744434) (merged 12b9c896 into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003216) (merged 9a17a01d into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003218) (merged f34559a3 into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "bmatheny": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865", "body": "Would you be open to a patch for this? The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063", "body": "SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nI agree the on the fly reload is tricky, I'll put some thought into it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782", "body": "One additional comment. Based on feedback from Manju I moved the old vlen values to vlen_rem, and made vlen be an immutable value representing the total size of the value. This allows a much cleaner calculation in the req_filter of whether the object value exceeds the configured item_size_max or not.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Ayutthaya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117", "body": "As a school project, I've made a patch to support kqueue, epoll and event ports, using the libevent API. There are also a few additional changes to make it work on my mac os x. (the patch is at github.com/ayutthaya/twemproxy ). I'm ready to improve my work if necessary, so don't hesitate to give feedback. Regards.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583", "body": "I changed the patch. Now it uses kqueue API directly (but doesn't support event ports anymore). It needed changes mainly to nc_event.[ch] and nc_stats.[ch], since stats aggregation was also based on epoll. The patch is still at github.com/ayutthaya/twemproxy . one question: what is the reason for not using libevent API ? Thanks. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593", "body": "I've tried to comply with the suggestions above as much as possible. In particular, I've abstracted out the event interface and used a function pointer to make all #ifdef HAVE_EPOLL/KQUEUE disappear in all files except nc_event.h nc_epoll.c and nc_kqueue.c. Don't hesitate if you think it needs further changes before doing a pull request. \nI also have a question about twemproxy: what is the number of sockets from which point one should use twemproxy / the performance of the cache server starts degrading seriously due to per-connection overhead? 65k? 200k? If you could simply give me a lead, it would help me a lot for my project. Thanks !\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jsholmes": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243", "body": "Any update on that stable build?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "mezzatto": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/1693973", "body": "This is awesome! Any known issues?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/1693973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nitper": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/3668996", "body": "@manjuraj just wanted to let you know that we are fully using twemproxy with redis at bright.com and are loving it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/3668996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "1125449708": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/4628152", "body": "Fantastic.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/4628152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dominis": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/5512362", "body": "lol\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/5512362/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "GOPALYADAV": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924236", "body": "# hash_tag\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924245", "body": "+language:6\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "BrandonBrowning": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10019622", "body": "I seem to have gotten a similar error with the latest code?\n\n```\n[2015-03-03 19:01:55.777] nc_redis.c:2076 parsed bad rsp 2 res 1 type 133 state 11\n00000000  2a 32 0d 0a 24 32 0d 0a  72 30 0d 0a 2b 4f 4b 0d   |*2..$2..r0..+OK.|\n00000010  0a                                                 |.|\n[2015-03-03 19:01:55.777] nc_core.c:198 recv on s 8 failed: Invalid argument\n```\n\nExpected\n\n```\n1) \"r0\"\n2) OK\n```\n\nCaused by running\n\n```\nevalsha 370ca495ee43ef26fae6c1d4dfa16baac375026d 1 a set 5\n```\n\n(it's a wrapper around normal command execution)\n(reference #108)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10019622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031992", "body": "Give twemproxy a single redis instance (only for simplicity; works with multiple obviously)\nBash variables represent ports\n\n```\n$ redis-cli -p $nutcracker set _name r0\n$ redis-cli -p $nutcracker eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a get\n1) \"r0\"\n2) (nil)\n\n# looking good\n\n$ redis-cli -p $nutcracker eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a set 5\n(error) ERR Invalid argument\n\n# oh no\n\n$ redis-cli -p $redis eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a set 5\n1) \"r0\"\n2) OK\n\n# works directly on client\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10071652", "body": "Repro case is fixed!  Thanks :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10071652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "vlm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829590", "body": "Does it make sense to add CONN_KIND_AS_STRING(conn) instead of every `\"s` though?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829594", "body": "What's the reason for this commit?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11831080", "body": "Ack.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11831080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Niteesh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739", "body": "if cp->select  is initlized by CONF_DEFAULT_SELECT, the function conf_set_num will throw error that select is duplicate,\nthus to avoid writing very similar function  and  let conf_set_num work fine, this is done, later if cp->select remains unset default value is  assigned in conf_validate_pool\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748", "body": "i made sure that when i call this funtion, connection is connected.\neven than i should  have kept it in loop to try more than once if EAGAIN error is encountered\n\nor Ill appritiate  your comments if there is a better way to do it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055", "body": "```\nvoid  setSelectDb(struct conn *conn,struct server *server){\n     ASSERT(!conn->client && conn->connected )\n     int n, i;\n     char selectCommand[25];\n     sprintf(selectCommand,\"*2\\r\\n$6\\r\\nSELECT\\r\\n$1\\r\\n%d\\r\\n\",server->owner->select);\n     n = write(conn->sd,selectCommand,strlen(selectCommand));\n if (n < 0) {\n     log_error(\"ERROR selecting db on  socket for socket %d-> error %d \", conn->sd, strerror(errno) );\n     conn->err = errno;\n }\n```\n\nI am really not sure about this  so i want to ask you if this will do the needful        \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "jdi-tagged": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780", "body": "This does the opposite of what the comment above says; with this change having unique server names doesn't help if the pnames match.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "allenlz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133", "body": "All fragements will be freed at the same time if some error happens (`rsp_make_error`). If there is no error, they will also be freed at the same time.\nIf I can't believe `frag_owner` is valid, which you think it may be a wild pointer, that will be a bug of twemproxy upstream.\nWill it happen?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}, "2": {"unisqu": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/546", "title": "can implement xxhash as hash?", "body": "can implement xxhash as hash?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stanisavs": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/543", "title": "Stats is empty", "body": "Hello!\r\nHow can I see stats?\r\n``src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222``\r\nThen I use this connection on production for 1 minute (5k qps).\r\nLogs:\r\n```...\r\n...\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 10 eof rb 567 sb 124\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 10 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 10 '127.0.0.1:20964' on event 00FF eof 1 done 1 rb 567 sb 124  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76105 done on c 8 req_time 0.226 msec type REQ_REDIS_ZREVRANGEBYSCORE narg 7 req_len 86 rsp_len 97 key0 'DB11' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76107 done on c 11 req_time 0.181 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '456:51:d8437695e7a446210fc356b730d5e909:tmp' peer '127.0.0.1:20968' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76111 done on c 8 req_time 0.140 msec type REQ_REDIS_HGET narg 3 req_len 100 rsp_len 13 key0 'tables:domain' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 11 eof rb 456 sb 159\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 11 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 11 '127.0.0.1:20968' on event 00FF eof 1 done 1 rb 456 sb 159  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76114 done on c 8 req_time 0.156 msec type REQ_REDIS_HINCRBY narg 4 req_len 90 rsp_len 7 key0 'imps:420155' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76116 done on c 8 req_time 0.151 msec type REQ_REDIS_HINCRBY narg 4 req_len 81 rsp_len 8 key0 'campaigns:ads:stats_day:420144' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76118 done on c 8 req_time 0.139 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '459:51:489c90dd70cc6bf3292a1722ac0f46ee:tmp' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_connection.c:360 recv on sd 8 eof rb 436 sb 130\r\n[2017-12-06 11:05:52.735] nc_request.c:430 c 8 is done\r\n....\r\n```\r\n\r\n\r\nStats (I checked it every 5 second):\r\n```\r\nsrc/nutcracker --describe-stats\r\nThis is nutcracker-0.4.1\r\n\r\npool stats:\r\n  client_eof          \"# eof on client connections\"\r\n  client_err          \"# errors on client connections\"\r\n  client_connections  \"# active client connections\"\r\n  server_ejects       \"# times backend server was ejected\"\r\n  forward_error       \"# times we encountered a forwarding error\"\r\n  fragments           \"# fragments created from a multi-vector request\"\r\n\r\nserver stats:\r\n  server_eof          \"# eof on server connections\"\r\n  server_err          \"# errors on server connections\"\r\n  server_timedout     \"# timeouts on server connections\"\r\n  server_connections  \"# active server connections\"\r\n  server_ejected_at   \"timestamp when server was ejected in usec since epoch\"\r\n  requests            \"# requests\"\r\n  request_bytes       \"total request bytes\"\r\n  responses           \"# responses\"\r\n  response_bytes      \"total response bytes\"\r\n  in_queue            \"# requests in incoming queue\"\r\n  in_queue_bytes      \"current request bytes in incoming queue\"\r\n  out_queue           \"# requests in outgoing queue\"\r\n  out_queue_bytes     \"current request bytes in outgoing queue\"\r\n```\r\n\r\n```\r\nps aux | grep nut\r\nroot      5985  1.0  0.0  18584  2660 ?        Sl   11:04   0:44 src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222\r\n```\r\n\r\n```\r\nnetstat -tulpn | grep 22222\r\ntcp        0      0 0.0.0.0:22222           0.0.0.0:*               LISTEN      5985/nutcracker \r\n```\r\n\r\nBTW connection with redis took 0.15ms, connection with twemproxy took 0.03ms, but 4 pipes with redis was faster (1.0-1.15ms) than the same operations with twemproxy (1.0-1.5ms). I was hoping for more acceleration.\r\n\r\nAnd also how to understand the logs?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "filippog": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/540", "title": "Prometheus stats support?", "body": "Hi,\r\nwould you be interested in having native Prometheus stats support? I'm not sure what would be the best way implementation-wise (a separate port?).", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532", "title": "Double quotes in server name result in invalid json stats", "body": "Hi,\r\nwe're using nutcracker/twemproxy with server names with double quotes, though this results in invalid json because the double quotes are not escaped but should (`src/nc_stats.c` at `stats_begin_nesting` if I'm reading the code correctly)", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "praveenbalaji-blippar": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/539", "title": "Twemproxy (Redis) for large batch/small # of connections", "body": "I use Redis in our offline processing (large batch/small # of connections). I want to use twemproxy to partition data. I see two issues which I want to address:\r\n\r\n- I notice that the performance drops ~2x when I use twemproxy.\r\n- Of greater concern to me is that twemproxy seems to drop the connection intermittently. I tried various `--mbuf-size` ranging from the default (16K) to 16M.\r\n\r\nI looked through several Issues on github, but most use cases seem to be large # of connection/online use cases. I want to figure out the configuration to handle large batch/small # of connections instead. Suggestions are much appreciated.\r\n\r\n\r\nOn an 8-core machine, `top` shows that available memory is not a problem. However, twemproxy appears to consume >90% CPU. The Redis instances consume ~10% CPU. When I execute the same commands in one of the Redis instances, the Redis instance consumes >90% CPU.\r\n\r\nThe configuration is as follows:\r\n```\r\ntest:\r\n  listen: 0.0.0.0:6378\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  redis: true\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 1\r\n  servers:\r\n   - 127.0.0.1:7378:1\r\n   - 127.0.0.1:7379:1\r\n```\r\n\r\nRequest payloads have one of the following characteristics:\r\n\r\n- pipelined request with `del`. Each pipelined request has 200,000 - 2,000,000 `del` commands. Each key is ~10 bytes.\r\n- `mget` batches of size 100,000 - 1,000,000. Each key and value are ~10 bytes.\r\n- pipelined request with `set` and `sadd`. Each pipelined request has 200,000 - 2,000,000 commands (set + sadd). Each key and value is ~10 bytes.\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "inter169": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/538", "title": "OOM on the mbuf freelist (100+GB)", "body": "the nutcracker process consumed 100+GB phisycal memory on my production box after a data migration from another redis to this one (nutcracker).\r\nand the gdb console showed below:\r\n```\r\n(gdb) p nfree_mbufq\r\n$1 = 6365614\r\n(gdb) p mbuf_chunk_size\r\n$1 = 16384\r\n```\r\n\r\nthe memory consumption was nfree_mbufq * mbuf_chunk_size = 101GB approx. \r\nI have read some code fixes (pr)s about the similar phenomenon, like:\r\nhttps://github.com/twitter/twemproxy/pull/461\r\nhttps://github.com/twitter/twemproxy/issues/203\r\n\r\nbut such fixes didn't set the limitation of the mbuf chunks, so the OOM was still here, \r\nI coded a fix, and the nutcracker can pass a command param ('-n ',  in my fix) to set the max number of mbuf chunks, once exceeded the limitation it can free the mbuf after processing one req immediately.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jhwillett": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/537", "title": "Twemproxy hangs parsing nested Array responses from Redis", "body": "Twemproxy hangs when processing a Redis Lua script via EVALwhich returns nested sub-arrays. \r\nThis is easy to reproduce.\r\n\r\nSetup:\r\n* On Mac OS 10.12.6.\r\n* Running redis-server 2.8.24 at localhost:7379 (installed via Homebrew).\r\n* Running nutcracker-0.4.1 at localhost:221221 which is configured with:\r\n```\r\ndevelopment:\r\n  redis:            true\r\n  auto_eject_hosts: false\r\n  listen:           127.0.0.1:22121\r\n  servers:\r\n    - 127.0.0.1:7379:1\r\n```\r\n\r\nDemo of direct connection to Redis being happy:\r\n```\r\n$ redis-cli -p 7379\r\n127.0.0.1:7379> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379> EVAL 'return {1,{2},3}' 1 x\r\n1) (integer) 1\r\n2) 1) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379>\r\n```\r\n\r\nDemo of connection through Twemproxy being unhappy:\r\n```\r\n$ redis-cli -p 221221\r\n127.0.0.1:221221> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:221221> EVAL 'return {1,{2},3}' 1 x\r\n^C\r\n```\r\nThe last command hangs as long as I was willing to wait (several minutes), until I cancel it.\r\n\r\nI see no messages in the twemproxy logs during this time.\r\n\r\nI notice that src/proto/nc_redis.c has special cases for nested multi bulk reply element from sscan/hscan/zscan.  Perhaps eval and evalsha are simply missing the special case treatment?\r\n\r\nI apologize for phrasing this as a bug report instead of as a pull request with a fix, but I am afraid it may be a long time before I can dig deeper.\r\n\r\nThank you for a wonderful tool.  It is a critical piece of our infrastructure at ProsperWorks and has made our lives much easier.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/537/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BlueCatFlord": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/535", "title": " Whether the project is in maintenance", "body": "Whether the project is in maintenance", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslusher": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/534", "title": "twemproxy only sees one of the memcached servers in the pool", "body": "It's my understanding that when a server in the twemproxy pool gets ejected, the other server in the pool should still be available for caching. It seems that when I take out *memcached-1 only*, the proxy itself becomes unavailable. If I take out memcached-2 from the pool, everything operates normally, except that there doesn't seem to be any indication in the logs that the server leaves or returns to the pool. \r\n\r\nI have tested that both memcached servers are available directly. If I put one or the other memcached sever by itself in the pool configuration, they're available using the proxy, but only memcached-1 is available if I have them both in the pool. I've tried ordering them differently and it doesn't seem to make a difference. A tcpdump only ever shows traffic to memcached-1 when they are both in the pool. When nutcracker is restarted, I only see arp traffic going to one of the two servers, but never both.\r\n\r\nTo reproduce:\r\n(nutcracker version 0.4.1 on centos 7)\r\n/etc/nutcracker/nutcracker.yml\r\n``` yaml\r\nbad_pool:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  server_retry_timeout: 30000\r\n  server_failure_limit: 3\r\n  servers:\r\n   - 10.10.10.33:11211:1 memcached-1\r\n   - 10.10.10.34:11211:1 memcached-2\r\n```\r\n\r\ntelnet 127.0.0.1 22122\r\n```\r\nTrying 127.0.0.1...\r\nConnected to 127.0.0.1.\r\nEscape character is '^]'.\r\nset testing 1 0 3\r\none\r\nSTORED\r\n```\r\n\r\nssh 10.10.10.33:\r\n```\r\nsudo systemctl stop memcached\r\n```\r\n\r\ntelnet console:\r\n```\r\nget testing\r\nSERVER_ERROR Connection refused\r\nConnection closed by foreign host.\r\n```\r\n\r\nnutcracker logs for sequence:\r\n```\r\n[2017-08-18 11:08:46.894] nc_core.c:43 max fds 1024 max client conns 989 max server conns 3\r\n[2017-08-18 11:08:46.894] nc_stats.c:851 m 4 listening on '0.0.0.0:22222'\r\n[2017-08-18 11:08:46.894] nc_proxy.c:217 p 6 listening on '127.0.0.1:22122' in memcache pool 0 'bad_pool' with 2 servers\r\n[2017-08-18 11:08:56.457] nc_proxy.c:377 accepted c 8 on p 6 from '127.0.0.1:41122'\r\n[2017-08-18 11:09:11.595] nc_request.c:96 req 1 done on c 8 req_time 1160.716 msec type REQ_MC_SET narg 2 req_len 24 rsp_len 8 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:00.115] nc_response.c:118 s 9 active 0 is done\r\n[2017-08-18 11:14:00.116] nc_core.c:237 close s 9 '10.50.20.35:11211' on event 00FF eof 1 done 1 rb 8 sb 24\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close s 9 '10.50.20.35:11211' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n[2017-08-18 11:14:06.887] nc_request.c:96 req 4 done on c 8 req_time 0.597 msec type REQ_MC_GET narg 2 req_len 13 rsp_len 33 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close c 8 '127.0.0.1:41122' on event FF00 eof 0 done 0 rb 37 sb 41: Operation not permitted\r\n```", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/534/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Vibe6": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/531", "title": "Change to grow yr Power", "body": "Hello twitter I what to give you advice please change \"follow\" text on yr button type. Add or only show a + income with profile it will help you believe me.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/531/reactions", "total_count": 3, "+1": 0, "-1": 3, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mishtika": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/530", "title": "Twemproxy \" Connection refused \" on failure of one redis instance", "body": "I have configured twem proxy with 2 redis servers.\r\nWhen one of these redis server fails the twem proxy gives a error saying \" Connection refused\"\r\n\r\nMy twem conf \r\nbeta:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  hash_tag: \"{}\"\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  redis: true\r\n  servers:\r\n   - 127.0.0.1:6381:1\r\n   - 127.0.0.1:6379:1\r\n\r\ni am trying to get a key from redis. If i kill one instance of redis and then try to run the get command then it gives the following error:\r\n[2017-07-26 16:25:24.548] nc_core.c:237 close s 15 '127.0.0.1:6381' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuetianle": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/528", "title": "is it  support the docker redis service ", "body": "when use the docker redis service ,it don't work well.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rposky": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/526", "title": "Add additional timeout option to apply to time between server request and response", "body": "The current \"timeout clock starts ticking the instant the message is enqueued into the server [in queue]\" and not at the time that the message is actually sent to the server. While this may be appropriate for some use cases, I suggest that it is problematic for others that wish to react to server response time irrespective of server demand.\r\n\r\nI have observed that in the presence of many requests using the same key, and thus mapped to the same server, a denial of service scenario is possible due to timeout of requests that may not have even left the server in-queue. Server ejection naturally follows, despite that the proxied service is still running and responding normally. It just could not respond to all queued requests within the configured timeout, again some of which it may not have even technically began servicing.\r\n\r\nIt would be beneficial were there an additional timeout option that could be evaluated specifically against server response times and used to influence server ejections in lieu of server_timeout errors. In the meantime, I have found that adjusting the existing timeout option to a much larger value can help to mitigate this error scenario, though it unfortunately means that the service will be slower to detect actual server issues.\r\n\r\nThanks ahead of time for any consideration and/or suggestions.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "danielkraaijbax": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/525", "title": "Redis PING command is sometimes slow.", "body": "Hi There,\r\n\r\nWe have a setup of Twemproxy and 3 Redis masters with 3 Redis Slaves  and i have found an issue with the PING command that we were using.\r\nFirst of, We use the PING command to check if a Redis server is correctly connected. We found some issues with this in the past where we would connect to Twemproxy, ask for a key and that the server did not respond. We fixed that and is totally not related to this ticket.\r\nIn our codebase (PHP) there was however still a line where we would issue a PING command after each connect. \r\nWe were sometimes experiencing slowness on our pages. Somehow our connection wrapper managed to wait 200ms before continuing.\r\n\r\nI created a couple of test scripts to try and resolve this issue.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->ping();\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times. \r\n\r\n> for i in {1..20}; do php redis-ping.php; sleep 1; done\r\n202.45825195312\r\n0.625\r\n1.968994140625\r\n0.60693359375\r\n1.89501953125\r\n202.28198242188\r\n0.452880859375\r\n1.438720703125\r\n201.91772460938\r\n2.072021484375\r\n201.50170898438\r\n202.705078125\r\n0.783935546875\r\n2.90966796875\r\n1.487060546875\r\n0.831787109375\r\n0.52587890625\r\n0.593017578125\r\n0.634033203125\r\n1.114990234375\r\n\r\nI made some adjustments to the script.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->get('FooBar');\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times.\r\n> for i in {1..20}; do php redis.php; sleep 1; done\r\n1.2978515625\r\n1.47607421875\r\n2.22216796875\r\n2.47509765625\r\n2.26708984375\r\n1.2548828125\r\n1.25\r\n0.85986328125\r\n4.005859375\r\n3.97802734375\r\n0.876953125\r\n1.739990234375\r\n0.784912109375\r\n0.760986328125\r\n1.220947265625\r\n3.2080078125\r\n2.267822265625\r\n0.810791015625\r\n2.806884765625\r\n0.744140625\r\n\r\nSo it seems that the PING command is sometimes slow.\r\nI assume this could be an issue? \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kigster": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/523", "title": "On newer SmartOS Twemproxy chooses epoll instead of event ports", "body": "Looks like SmartOS now [implements `epoll`](http://blog.shalman.org/exploring-epoll-on-smartos/), as well as their native event ports.\r\n\r\nBuild system prefers `epoll` when both are available, but it would be better if it chose native `event ports`. We are currently experiencing random sporadic lockups of twemproxy built with `epoll` support. Currently testing the event ports version.\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vishalsharma13": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/522", "title": "Redis rename command", "body": "hi,\r\n\r\nWhy twemproxy not used rename command? Is there any possibilty to use rename command through twemproxy.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516", "title": "Twemproxy Server Timedout", "body": "Hi,\r\n\r\nCan you please help to identify the reason of error with redis server.\r\n\r\nTwemproxy logs :- \r\n\r\n[2017-02-09 12:06:24.188] nc_request.c:96 req 219840 done on c 27903 req_time 1000.594 msec type REQ_REDIS_HMGET narg 7 req_len 117 rsp_len 27 key0 'USER:SESSION:8c5ab24a-c0b0-4ccd-a377-02c0b1d728ed' peer '10.247.74.50:58653' done 1 error 1\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1\r\n  timeout: 500\r\nbeta:\r\n  auto_eject_hosts: false\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 20000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 1000\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512", "title": "Twemproxy : Server Connections not closed properly", "body": "Hi,\r\n\r\nI am using Redis with 3 servers configuration i.e one master(to write data) and two slaves(to read data) and using Twemproxy for load balancing.\r\n\r\nAnd using Jedis with connection pooling.\r\n\r\nNow i doing load testing but i am facing below errors\r\n\r\n1) Twemproxy Server connections are not properly closed on time and increased upto double or triple of  request(Like for 10k requests server connections opened upto 30k connections)\r\n\r\n2) Getting exception like unable to get resource from pool however my poolsize limit is not exceeded and after one command connection returns back to pool.\r\n\r\nServer Configurations :- \r\n\r\n1) 64GB RAM\r\n2) 16 core processor\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1\r\n  timeout: 5000\r\nbeta:\r\n  auto_eject_hosts: true\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 50000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 5000\r\n\r\n\r\nJedis Pool Configuration :- \r\n\r\npublic class RedisReadManager {\r\n    private static final RedisReadManager instance = new RedisReadManager();\r\n    private static JedisPool pool;\r\n    private RedisReadManager() {}\r\n    public final static RedisReadManager getInstance() {\r\n        return instance;\r\n    }\r\n    public void connect(String host, int port) {\r\n        try {\r\n        \tif(pool==null || pool.isClosed()){\r\n\t\t\t\tJedisPoolConfig poolConfig = new JedisPoolConfig();\r\n\t\t\t\tpoolConfig.setMaxTotal(60000);\r\n\t\t\t\tpoolConfig.setTestOnBorrow(true);\r\n\t\t\t\tpoolConfig.setTestOnReturn(true);\r\n\t\t\t\tpoolConfig.setMaxIdle(3000);\r\n\t\t\t\tpoolConfig.setMinIdle(100);\r\n\t\t\t\tpoolConfig.setTestWhileIdle(true);\r\n\t\t\t\tpoolConfig.setNumTestsPerEvictionRun(10);\r\n\t\t\t\tpoolConfig.setTimeBetweenEvictionRunsMillis(10000);\r\n\t\t\t\tpool = new JedisPool(poolConfig, host, port);\r\n        \t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\t\t\t\r\n\t\t}\r\n    }\r\n    public void releasePool() {\r\n        try {\r\n\t\t\tif(pool!=null&&!pool.isClosed()){\r\n\t\t\t\tpool.destroy();\r\n\t\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n    public Jedis getJedis() {\r\n        return pool.getResource();\r\n    }\r\n\r\n\tpublic void returnJedis(Jedis jedis) {\r\n    \ttry {\r\n\t\t\tif(jedis!=null&jedis.isConnected()){\r\n\t\t\t\tjedis.disconnect();\r\n\t\t\t\tjedis.close();\r\n\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n}\r\n\r\nNutcracker-web :- \r\n\r\n![nutcracker-web](https://cloud.githubusercontent.com/assets/22707039/22018052/ebaf5d92-dcd2-11e6-91c6-463af69e960a.jpg)\r\n\r\nPlease help to resolve the issue.\r\n \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ArcticSnowman": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/521", "title": "Any recommendations For Using KeepaliveD with Twemproxy?", "body": "Anyone got recommendations For Using KeepaliveD with Twemproxy?\r\n\r\nUse a simple tcp check against the port?  Something more sophisticated with a check on the stats port?\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518", "title": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands", "body": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands.\r\n\r\n`redis-cli GET <key>`  does not work for twemproxy as redis-cli issues a `COMMAND` command to check that you have a valid command to send. \r\n\r\nThis works in interactive mode...\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mcanonic": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/519", "title": "Support for BRPOP and BLPOP", "body": "Hi,\r\nI saw the notes/redis.md file and I'm wondering if the unsupported commands are somethin that you are working on it or not. Just to know if the (near?) future these commads like BRPOP e BLPOP will be supported.\r\nThanks\r\nM", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hjhart": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/514", "title": "Twemproxy cluster not accepting clients", "body": "On a machine running twemproxy, it is no longer accepting client connections. We're not totally sure why, at this point. The logs look interesting enough, ending with an `eof`. I think I'm just not understanding 1) how twemproxy would ever \"go down\" and 2) choose not to come back up.\r\n\r\nThe redis machines are responsive, because we have five other twemproxy machines with exactly the same configuration and they are still up. There is a process that runs JUST before this problem occurs, and it is hitting redis incredibly hard. We're just not sure conceptually _why_ twemproxy goes down, and what configuration we can change to make it stop. \r\n\r\nUnderstanding, of course, that we are hitting redis incredibly hard from a single process and we could choose _not_ to do that, but we want to first understand why twemproxy goes down, and then why it _stays_ down.\r\n\r\nAnother thing to note, is that we've recently updated from twemproxy 0.3.0 to 0.4.1, and this didn't appear to happen on 0.3.0.\r\n\r\nThe end of the logs looks like this: \r\n\r\n## Logs\r\n\r\n```\r\n> tail /var/log/twemproxy-daily-sale.log\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782534 done on c 30 req_time 27.163 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20888845:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782535 done on c 30 req_time 27.181 msec type REQ_REDIS_DEL narg 2 req_len 34 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782536 done on c 30 req_time 27.200 msec type REQ_REDIS_RPUSH narg 12 req_len 177 rsp_len 5 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782539 done on c 30 req_time 27.209 msec type REQ_REDIS_EXPIRE narg 3 req_len 49 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782540 done on c 30 req_time 27.227 msec type REQ_REDIS_DEL narg 2 req_len 35 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782541 done on c 30 req_time 27.246 msec type REQ_REDIS_RPUSH narg 3 req_len 51 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782544 done on c 30 req_time 27.256 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:57.393] nc_connection.c:360 recv on sd 30 eof rb 830971428 sb 32948345\r\n[2017-01-27 05:44:57.393] nc_request.c:430 c 30 is done\r\n[2017-01-27 05:44:57.393] nc_core.c:237 close c 30 '10.100.17.85:56791' on event 00FF eof 1 done 1 rb 830971428 sb 32948345\r\n```\r\n\r\n## Configuration\r\n\r\nOur configuration on twemproxy looks like this:\r\n<details>\r\n  <summary>Click to expand configuration</summary>\r\n\r\n\r\n```\r\n> cat /etc/twemproxy/twemproxy-daily-sale.yml\r\ndaily-sale:\r\n  hash: fnv1_32\r\n  hash_tag: \"::\"\r\n  distribution: ketama\r\n  timeout: 1000\r\n  auto_eject_hosts: false\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 3\r\n  redis: true\r\n  listen: 10.100.18.248:22139\r\n  servers:\r\n  - 10.100.17.85:37001:1 daily-sale001\r\n  - 10.100.17.85:37002:1 daily-sale002\r\n  - 10.100.17.85:37003:1 daily-sale003\r\n  - 10.100.17.85:37004:1 daily-sale004\r\n  - 10.100.17.241:37005:1 daily-sale005\r\n  - 10.100.17.241:37006:1 daily-sale006\r\n  - 10.100.17.241:37007:1 daily-sale007\r\n  - 10.100.17.241:37008:1 daily-sale008\r\n  - 10.100.18.19:37009:1 daily-sale009\r\n  - 10.100.18.19:37010:1 daily-sale010\r\n  - 10.100.18.19:37011:1 daily-sale011\r\n  - 10.100.18.19:37012:1 daily-sale012\r\n  - 10.100.18.100:37013:1 daily-sale013\r\n  - 10.100.18.100:37014:1 daily-sale014\r\n  - 10.100.18.100:37015:1 daily-sale015\r\n  - 10.100.18.100:37016:1 daily-sale016\r\n```\r\n\r\n</details>\r\n\r\n## Connections\r\n\r\n`netstat -an` shows that there is a single `CLOSE_WAIT` connection stuck.\r\n\r\n```\r\n> netstat -an | grep 22139\r\n10.100.18.248.22139        *.*                0      0 1048576      0 LISTEN\r\n10.100.18.248.22139  10.100.104.187.23453 1049792      0 1049800      0 CLOSE_WAIT\r\n```\r\n\r\nAnd on the corresponding machine (10.100.104.187) there is a `SYN_SENT` connection stuck:\r\n\r\n```\r\n> netstat -an | grep 10.100.18.248\r\n10.100.104.187.28052 10.100.18.248.22136  1049792      0 1049800      0 ESTABLISHED\r\n10.100.104.187.38585 10.100.18.248.22139      0      0 1049740      0 SYN_SENT\r\n10.100.104.187.34939 10.100.18.248.22135  1049792      0 1049800      0 ESTABLISHED\r\n```\r\n\r\n## Statistics\r\n\r\nOur statistics do not indicate any servers are out of the hash ring...\r\n\r\n<details>\r\n  <summary>Click to expand statistics</summary>\r\n\r\n```\r\n> nc localhost 22227 | json\r\n{\r\n  \"service\": \"nutcracker\",\r\n  \"source\": \"twemproxy100.prod\",\r\n  \"version\": \"0.4.1\",\r\n  \"uptime\": 90618,\r\n  \"timestamp\": 1485566119,\r\n  \"total_connections\": 459796,\r\n  \"curr_connections\": 17,\r\n  \"daily-sale\": {\r\n    \"client_eof\": 3927,\r\n    \"client_err\": 455852,\r\n    \"client_connections\": 0,\r\n    \"server_ejects\": 0,\r\n    \"forward_error\": 0,\r\n    \"fragments\": 0,\r\n    \"daily-sale001\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 499925,\r\n      \"request_bytes\": 52043467,\r\n      \"responses\": 499925,\r\n      \"response_bytes\": 2109629,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale002\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 425855,\r\n      \"request_bytes\": 44208662,\r\n      \"responses\": 425855,\r\n      \"response_bytes\": 1791852,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale003\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 516402,\r\n      \"request_bytes\": 53579106,\r\n      \"responses\": 516402,\r\n      \"response_bytes\": 2170560,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale004\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 571640,\r\n      \"request_bytes\": 59475059,\r\n      \"responses\": 571640,\r\n      \"response_bytes\": 2405288,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale005\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 488641,\r\n      \"request_bytes\": 50937149,\r\n      \"responses\": 488641,\r\n      \"response_bytes\": 2060132,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale006\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 595062,\r\n      \"request_bytes\": 61596411,\r\n      \"responses\": 595062,\r\n      \"response_bytes\": 2514791,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale007\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 539622,\r\n      \"request_bytes\": 56245033,\r\n      \"responses\": 539622,\r\n      \"response_bytes\": 2274943,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale008\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 450732,\r\n      \"request_bytes\": 47025684,\r\n      \"responses\": 450732,\r\n      \"response_bytes\": 1898874,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale009\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 461595,\r\n      \"request_bytes\": 47897394,\r\n      \"responses\": 461595,\r\n      \"response_bytes\": 1938910,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale010\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 502702,\r\n      \"request_bytes\": 52229759,\r\n      \"responses\": 502702,\r\n      \"response_bytes\": 2124118,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale011\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 489467,\r\n      \"request_bytes\": 50950224,\r\n      \"responses\": 489467,\r\n      \"response_bytes\": 2060700,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale012\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 510860,\r\n      \"request_bytes\": 53141344,\r\n      \"responses\": 510860,\r\n      \"response_bytes\": 2145320,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale013\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 446362,\r\n      \"request_bytes\": 46440311,\r\n      \"responses\": 446362,\r\n      \"response_bytes\": 1872555,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale014\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 427993,\r\n      \"request_bytes\": 44477521,\r\n      \"responses\": 427993,\r\n      \"response_bytes\": 1803340,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale015\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 508857,\r\n      \"request_bytes\": 52884727,\r\n      \"responses\": 508857,\r\n      \"response_bytes\": 2141577,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale016\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 564176,\r\n      \"request_bytes\": 58592099,\r\n      \"responses\": 564176,\r\n      \"response_bytes\": 2376014,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n</details>\r\n\r\n## Connecting\r\n\r\nAnd similarly trying to establish a connection to twemproxy hangs indefinitely...\r\n\r\n```\r\n> telnet 10.100.18.248 22139\r\nTrying 10.100.18.248...\r\n```\r\n\r\nAny and all help is appreciated. Thank you!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/514/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jennyfountain": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/513", "title": "Questions about twemproxy and performance", "body": "We are seeing a few issues that I was hoping someone could help me resolve or point me in the right direction.\r\n\r\n1.  During high loads, we are seeing a lot of backup in the out_queue_bytes.  On normal traffic loads, this is 0. \r\n\r\nExample (sometimes goes into 2k/3k range as well):\r\n    \"out_queue_bytes\": 33\r\n    \"out_queue_bytes\": 91\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 174\r\n\r\nIn addition, it shows that our time spent in memcache goes up from 400 ms to 1000-2000 ms. This seriously affects our application. \r\n\r\n2.  auto eject also seems to not work as expected. Server goes down and our app freaks out - saying it cannot access a memcache server.\r\n\r\nhere is an example of a config:\r\n\r\nweb:\r\n  listen: /var/run/nutcracker/web.sock 0777\r\n  auto_eject_hosts: true\r\n  distribution: ketama\r\n  hash: one_at_a_time\r\n  backlog: 65536\r\n  server_connections: 16\r\n  server_failure_limit: 3\r\n  server_retry_timeout: 30000\r\n  servers:\r\n   - 1.2.3.4:11211:1\r\n   - 1.2.3.5:11211:1\r\n   - 1.2.3.6:11211:1\r\n  timeout: 2000\r\n\r\nsomaxconn = 128\r\n\r\nWhat we tried and didn't help.\r\n1.  mbuf to 512\r\n2.  server connection from 1 to 200\r\n\r\nThank you for any guidance on this problem.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "manjuraj": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/34eb60fb977da9aa8bcac784dee1e7fb04c79d47", "message": "Merge pull request #517 from takayamaki/fix_typo\n\nfix typo in README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0f9e1baf06db478065ba43402b975c57a567d00f", "message": "Merge pull request #492 from kalifg/patch-1\n\nFix typo in notes/memcache.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6fd922039bc696e07b7ef18946732df6b7cf2a5d", "message": "Merge pull request #493 from dennismartensson/patch-1\n\nUpdate README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/017d44503344ecdf2439747476556f4b9fee3e21", "message": "Merge pull request #494 from mortonfox/patch-1\n\nUpdate the sensu-metrics link"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e5739338ddf228b3b900aa116646cb9654fa5f65", "message": "Merge pull request #489 from rohitpaulk/update-redis-docs\n\nUpdate redis docs for PING and QUIT"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/74af2fb2d5d3e214d8c0741a4b0ebb7d93572fc8", "message": "Merge pull request #439 from Krinkle/patch-1\n\nreadme: Link to HTTPS for wikimedia.org"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9a2611e5992e65e6c15cd23bce06ffed3901f4f8", "message": "Merge pull request #425 from charsyam/feature/typos\n\nfix typos"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/619b55b6a1dcd21ffe16f312b52cd576c6a4caa0", "message": "Merge pull request #421 from charsyam/feature/upgrade-redis-lib\n\nupgrade redis-py version from 2.9.0 to 2.10.3"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/592c1b6b157721f66789bca6a3d463f32e6c59fc", "message": "Merge pull request #420 from esindril/master\n\nAdd script to build SRPM and fix spec file"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/defda674cdfb94fef31bdf8cc7a1dd766ea0a05a", "message": "Merge pull request #418 from twitter/revert-406-bugfix/redis-error-response-parser\n\nRevert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/65bb2ac5bbf0acce3fa52469752046c388e56ef5", "message": "Revert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/34f369ff91076b5d1b1229eca51bc3a2ed8f6d4a", "message": "Merge pull request #406 from tom-dalton-fanduel/bugfix/redis-error-response-parser\n\nFix parsing bug when error body contains no spaces"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954", "body": "I would be willing to take patches for kqueue support in twemproxy\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968", "body": "Thank you for the patches. I will merge this in the next version\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668", "body": "twemproxy does not do replication. If you are doing replication, you would have to do it on the client side\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106", "body": "> 1. Before you started work on twemproxy, did you consider using moxi: https://github.com/steveyen/moxi? If yes, why did you reject moxi?\n>    with nutcracker we have:  \n>    a) protocol pipelining which works really well for us, as we wanted to introduce minimal latency degration with a proxy sitting between a client and server.\n>    b) mbuf, which essentially enables zero copy when moving data from client to server (and vice versa)\n>    c) observability which was fairly important to us in our production environment\n> 2. Are you planning to support binary protocol in addition to ascii?\n>    no (see 'thoughts' section in notes/memcache.txt)\n> 3. Do you see any need for multi-threaded support? Moxi supports both single and multi-threaded configurations.\n>    no; if a run proxy of proxy (client --> proxy --> (proxy)+ --> server) you can actually make use of all cores and would probably be network bound in this scenario\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681", "body": "This is a good idea.\n\nThe start up time of twemproxy is really small, especially when preconnect is not set. So, if your clients had retries built into it, then you can trivially propagate configuration changes by doing a rolling restarts of twemprox'ies. This is a good enough solution, imo\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486", "body": "> > Would you be open to a patch for this?\n> > I am always open to accepting patches :) \n\nI just think that reloading of configuration file on-the-fly is tricky to get right.\n\n> >  The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n\nI am curious as how this is happening. I set SO_REUSEADDR address option on the twemcache's listening socket, which means that we would reuse ports even if they are busy (See: proxy_reuse() function in nc_proxy.c)\n\nCould you paste me the log file dump with the error that you seeing? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606", "body": "Thanks for the patch using libevent API. \n\nI am willing to accept patches if they don't use libevent but directly call bsd's kqueue API. All you might have to do is to implement abstraction in the event interfaces, which would be in nc_event.[ch]. Let if know if you have any questions. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377", "body": "you can always change the ulimit of the shell from which you launch nutcracker\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857", "body": "blake the patch looks great. One thing I realized is that memcache has some item header overhead and slab header overhead even for the largest item. So, even if you configure memcache with 1MB slab, the largest item that can be stored in the slab is < 1MB. Furthermore the item size not only includes the value length, but also key length. \n\nGiven this, do you think we should have two extra keys in yml configuration\nitem_max_kvlen: (maximum key + value length)\nitem_overhead:\n\nand we discard requests whose key + value length + overhead > item_max_kvlen\n\nthoughts?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944", "body": "the reason I don't use libevent api is because I wanted to build nutcracker without any dependency on 3rd party libraries. furthermore, I use ET semantics, which I believe is not available in 1.4 version of libevent.\n\ncould i get a pull request of your changes?\n\nI also looked at your changes. I think you might want to do few cleanups before submitting a pull request. Few suggestions:\na) abstract out the event interface and wire to the underlying event call using a function pointer\nb) maybe create event/ directory that contains event/nc_epoll.c and event/nc_kqueue.c files. \nc) follow style conventions outlined in: https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133", "body": "> SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nIs there a test case that would reproduce this scenario? I am unable to reproduce this scenario, even if I bombard twemproxy with a constant stream of traffic and restart it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361", "body": "blake, I wonder if this issue would go away if we set l_linger = 0 by calling nc_set_linger(sd, 0) on the listening sockets (see notes/socket.txt for details on it)? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080", "body": "Thanks for the test case; this is really useful\n\nI am still actively working on the redis branch. hopefully we should have a stable build out by end of this month\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159", "body": "should be straight forward if aws conforms to memcache ascii protocol\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813", "body": "is there a reason why you would prefer inline command over unified command besides the obvious ease of issuing such command from telenet;\n\njust supporting unified protocol makes parsing for req / rsp easy and simple in twemproxy.\n\nif you would like to contribute, you could look at nc_parse.[ch]\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651", "body": "twemproxy should just work with kestrel. I guess you might be well off using \"distribution: random\" instead of default of \"distribution: ketama\" for a pool of kestrel severs\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677", "body": "apologies @jsholmes ; I am still working on this\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/1694665", "body": "I am still actively testing this branch...I should have a stable build out in few weeks.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/1694665/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829605", "body": "hmm...this is a wrong commit from my end. ignore this. apologies\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829605/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829681", "body": "I chose the verbose way because the log message prints the line number which helps in deciphering the condition that was triggered\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536", "body": "No need for a py script. You can generate the strings using macro magic called strigificaion :) This way your code never gets outdate\n\nSee this for reference -- \n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L23-L49\n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L117-L122 \n- http://gcc.gnu.org/onlinedocs/cpp/Stringification.html\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553", "body": "s/char */uint_8 */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558", "body": "usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586", "body": "space between the closing paren and curly brace. So `log_loggable(LOG_NOTICE) {`\nHere are the coding guidelines -- https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt. Would really appreciate if you follow it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603", "body": "just call it req_log()\n\nalso format is `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612", "body": "formatting -- `/* a fragment */`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643", "body": "formatting - `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649", "body": "formatting - `if (rsp) {`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660", "body": "call this `int_64_t start_ts /\\* request start timestamp in usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410", "body": "Rename\ntotal_connections to ntotal_conn\ncurr_connections to nconn\ncurr_client_connections to nclient_conn\n\ncurr_connections should be int32_t\ncurr_client_connections should be int32_t\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414", "body": "how did we come up with RESERVED_FDS number as 32\nrename it as NC_NUM_RESERVED_FD\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416", "body": "I think we might still hit this scenario - One instance where I see this happening is that you close the connection() from the code (decrement the counters), but the sockets are still in TIME_WAIT state and hence are not available for accept() sys call.\n\nSo, instead of panic, we should log() and return NC_ERROR and not set p->recv_ready to 0\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121", "body": "what is this for?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123", "body": "why did we bump this up?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127", "body": "[formatting] `status: %d`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172", "body": "is get_mbuf() function required? It seems to be called only in msg_make_reply(). Prefer not having this function\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178", "body": "why are these commented out? Isn't it required to trigger the out event?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216", "body": "Should we move the lines 482 - 495 into server_pool_idx()? It seems that only place where the tag logic is not used is in server_pool_server(), which is surprising, because I think that tag logic should also apply there equally.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257", "body": "In this piece of code, you allocate message structs for every server in the pool - ncontinuum messages, even if incoming messages are destined to < ncontinuum servers.\n\nOnly at line 818-819, do you some of these allocated message structs. Can we refactor this differently - maybe two loops where the first loop aggregates all the keys destined per server and the second loop has the logic\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}]}, "mortonfox": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/fe68175e0200e3c2589139438ff3efa392042aa6", "message": "Update the sensu-metrics link"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dennismartensson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/80ef6a7444fd5ae97fcab9606c1abedc19f00824", "message": "Update README.md\n\nAdded Greta to the list of companys"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kalifg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b87ba1abfe6a814999279e69af7ce07ba0ff6c68", "message": "Fix typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohitpaulk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/eed195341a02fa688b0dfb784d120f337f15a454", "message": "Update redis docs for PING and QUIT\n\nSupport for these was introduced in\n@4175419288ef66d95e082cfa2124e77fe6d4fe6d."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "andyqzb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/330f43a430261aa48d4063771ed70fe191177154", "message": "Merge pull request #486 from deep011/deep011-patch-1\n\nfix a memory leak bug for mset command"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ced2044980e4b9dd0c91b25fc64ce127879a1491", "message": "Merge pull request #484 from postwait/patch-1\n\nFix typo circunous -> circonus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "deep011": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/558e0d40ad79f423c4784565648e6c83cf035777", "message": "fix a memory leak bug for mset command"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/462", "title": "Fix a bug for msg->mlen", "body": "Function msg_append() already added the dst->mlen.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10512396", "body": " I think it would be better to support \u201cdelete key 0\\r\\n\u201d command, because memcached support this command. What is your opinion?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10512396/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "postwait": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1e078e9e9d97560825ae4f1245177a0af29e3c82", "message": "Fix typo circunous -> circonus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Krinkle": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/91a68d3c42638eb8178001f4d67d2606dcd80f51", "message": "readme: Link to HTTPS for wikimedia.org"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "esindril": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/51c5228acdd8a324a43107330f6b936de028dc0c", "message": "Fix spec file to work for RHEL >= 6 and wrong changelog date"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/205323f87deb0f0004963717f2d7a80eed8e9c3b", "message": "Add script to build the SRPM package for RPM-based distributions"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "caniszczyk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/0e8708be4d365163a1061c2a89ecc344e21203d6", "message": "Add Uber to the list of Adopters\n\nhttp://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128", "body": "Any updates on this patch @manjuraj ?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280", "body": "Just to note that this is somewhat of a hack, but it works for now. I didn't find any tests to run via gtest or whatever harness.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "idning": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1f4e0dae62baa379209e4cdfe64f2224dadce632", "message": "Merge pull request #410 from vincentve/master\n\noptimize performance when single key del"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350", "body": "if current msg is a sub-msg, the `msg->frag_owner` may be freed, and reset by `_msg_get`, this is not correct, especially for `req_error`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378", "body": "macro create clean code but break cscope/ctags :( \n\nclean code is more important, I will have a try.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "invalid-email-address": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b6ac1bbc34aa0f470b55893090c7cf6a696ec184", "message": "optimize performance when single key del"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tom-dalton-fanduel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/6f32a928b4830d218fef67aa5d22bc0fd44000f6", "message": "Add testcase"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TysonAndre": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/545", "title": "Always initialize file permissions field for unix domain socket", "body": "It seems like field->perm might be uninitialized memory\r\ndepending on how it is allocated.\r\nThe intended behavior is to only change file permissions from the default if a permission was specified in the config:\r\nhttps://github.com/twitter/twemproxy/pull/311/files#diff-f74ea9da930e79a9573455a0cbe4785d\r\n\r\nI ran into an issue where different sockets had different file\r\npermissions, and some of those sockets weren't readable by the user\r\nwhich created it. (I specified *only* the path to the unix domain socket)\r\n\r\nThis behavior probably started in\r\nhttps://github.com/twitter/twemproxy/pull/311/files", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phamhongviet": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/544", "title": "Add systemd service file", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "essanpupil": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/541", "title": "fix list indentation in README", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "idirouhab": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/536", "title": "Add Foodora as company who uses it in prod", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pataquets": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/524", "title": "Add Docker support", "body": "Add Dockerfile to enable image building.\r\nUsing the official GCC image, latest tag. More info at https://hub.docker.com/_/gcc/\r\n\r\nJust adding files, setting working dir and running make instructions.\r\n\r\nBuild:\r\n\r\n```\r\n$ docker build -t twemproxy .\r\n```\r\n\r\nRun:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro twemproxy -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nFYI, there's a still quicker to test, already built image on my Docker Hub. Test it by running:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro pataquets/twemproxy  -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nUsing `--rm` instead of `-d` makes the container not go background and it to be deleted after stop. Should stop by `CTRL+C`'ing it.\r\nIn order for the Docker container to connect to external memcached or Redis instances, either them should be contactable as external IPs or hosts or be linked to other previously run Docker containers via Docker's ```--link``` option.\r\n\r\nHere it is an example Docker Compose file I'm using (I can submit it with the PR also if you find it useful):\r\n```\r\nredis1:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63791:6379\r\n\r\nredis2:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63792:6379\r\n\r\ntwemproxy:\r\n  image: pataquets/twemproxy\r\n  command: -c /etc/nutcracker.yml --verbose=6\r\n  links:\r\n    - redis1\r\n    - redis2\r\n  ports:\r\n    - 6379:6379\r\n    - 22222:22222\r\n  volumes:\r\n    - ./conf/nutcracker.redis.yml:/etc/nutcracker.yml:ro\r\n```\r\nNotice that it is tuned for two Redis instances (yml file not included, mount yours)\r\n\r\nOptional improvement to come (maybe in another issue):\r\n- Create an 'official', based on your repo, automated build at Docker Hub for the image: https://docs.docker.com/docker-hub/builds/ . Just requires a free Docker Hub account and a following a quick 'Create automated build' process. I'll be happy to help on it, if needed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "galusben": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/520", "title": "JFrog is using twemproxy on production", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "santoshsahoo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/515", "title": "Added CircleHD to companies using Twemproxy", "body": "Updated README.md, we are using of twemproxy on aws.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "voetsjoeba": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/511", "title": "Fix build failure with --disable-stats", "body": "A build with ./configure --disable-stats currently fails with the following error (gcc 4.8.5 on CentOS 6):\r\n\r\n```\r\nnc_server.c: In function \u2018server_failure\u2019:\r\nnc_server.c:291:5: warning: implicit declaration of function \u2018stats_server_set_ts\u2019 [-Wimplicit-function-declaration]\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n     ^\r\nnc_server.c:291:38: error: \u2018server_ejected_at\u2019 undeclared (first use in this function)\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n                                      ^\r\nnc_server.c:291:38: note: each undeclared identifier is reported only once for each function it appears in\r\n```\r\n\r\nThe reason appears to be that some functions are not being nopped out in `nc_stats.h` when `NC_STATS` is 0. This change adds the missing entries. Technically only `stats_server_set_ts` is needed to fix the build error, but from the intent of the code it's clear that `stats_pool_set_ts` should also be nopped out, despite never being called directly.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/510", "title": "Allow negative exptime values in memcached storage commands", "body": "Memcached allows negative exptime values in storage commands to immediately expire the stored values, but this wasn't very well documented until recently (here's the commit that adds the documentation from May 2016: https://github.com/memcached/memcached/commit/e7d4521cd8b27f7ebc6e4c1b9aee9eb3544f6af5).\r\n\r\nThis patch allows the memcached parser to support negative exptime values in storage commands. The server still responds with the usual STORED if the exptime is negative, so no additional response handling is needed.\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ugurengin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/502", "title": "update memcache populate script", "body": "update memcache populate script to improve test cause of memcached for get and set operations", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mahdi-hdi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/497", "title": "Added Geo Functionality", "body": "New Redis functionality for geo date type now supported:\n\n```\nGEOADD\nGEODIST\nGEOHASH\nGEOPOS\nGEORADIUS\nGEORADIUSBYMEMBER\n```\n\n**Mutli bulk array** was implemented in Redis response parser to support response of **GEORADIUS**. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "willfitch": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/480", "title": "Implement ERROR protocol", "body": "Rather than severing connections upon an invalid event, this patch adds the ERROR portion of the Memcached protocol.  This does not, however, add CLIENT_ERROR or SERVER_ERROR.  That can be discussed.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "VishalRocks": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/476", "title": "Update README.md", "body": "Added Codechef in Companies using Twemproxy in Production\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "artursitarski": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/474", "title": "Wikia as twemproxy user", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flygoast": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/463", "title": "Implemented client connections limiting of server pool.", "body": "Implemented client connections limiting of server pool.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "huachaohuang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/461", "title": "Fix memory leak for redis mset.", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charsyam": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/458", "title": "Fix Bug: redis_auth don't work with redis_db", "body": "currently, twemproxy ignore \"redis_db\" when \"redis_auth\" is set.\nselect command is set as noforward because of auth.\n\nthis patch fix this.\nand move add_auth to post_connected.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/428", "title": "rebase heartbeat branch ", "body": "It had a bug when redis is loading.\nbut after patch of https://github.com/twitter/twemproxy/commit/ef453130e321974b332dd99d585ae7285eee4b5d\n(handle loading state as error)\nI think it fixed.\n\nIn my tests. this works fine :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/423", "title": "Fix parsing bug when error body contains no spaces", "body": "This is only copy for https://github.com/twitter/twemproxy/pull/406 (by @tom-dalton-fanduel )\nfor testing. I just PR again.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/395", "title": "support nested multibulk reply", "body": "@manjuraj This is part of https://github.com/twitter/twemproxy/pull/393\n\nfirst. I divided it that is only supporting nested multi bulk reply.\n\nafter merging this :)\n\nI will PR second of them :)\n\nThanks for your reivew :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/393", "title": "support geo commands and nested multibulk reply.", "body": "1. support GEO Commands\n   -> geoadd, geohash, geodist, georadius, georadiusbymember\n2. support nested multibulk reply\n   -> geo commands return nested multibulk reply\n\ncurrently we only support this type of multibulk\n\n```\n                 * - mulit-bulk\n                 *    - cursor\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n```\n\nbut georadius and georadiusbymember returns maximum 3 depth multibulk. so this patch supports this kind of multibulk also.\n\n```\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n```\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031155", "body": "@BrandonBrowning Could you give me a script to test this? Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10032839", "body": "@BrandonBrowning Thank you. I found the reason. and I will fix it soon. Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10032839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10033615", "body": "@BrandonBrowning could you try this version?\nhttps://github.com/charsyam/twemproxy/tree/feature/issue-323\nThank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10033615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572", "body": "How was just initializing with CONF_DEFAULT_SELECT?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575", "body": "I just think that add new conf file for this patch like this: nutcracker.select.yml\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579", "body": "in twemproxy, it is better to change the variable name to is_select_msg\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597", "body": "and conn->sd is nonblocking socket. so write can be failed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "ofirule": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/435", "title": "Added support for listening to same port from multiple twemproxy processes", "body": "My team (from IBM Security Trusteer) has ran into some bottlenecks when using twemproxy and developed this patch.\n\nThis patch allows to use socket option SO_REUSEPORT on twemproxy socket. This option is available on Linux kernel version 3.9+.\n\nAfter stress testing our redis-server we found that handling many clients consumes too much processing from redis but not redis code itself rather its networking code (epoll/etc.).\n\nIt seems that the single thread nature of redis maxes out the networking part with one core in extreme conditions (like many connections scenario, no pipelining, etc.).\n\nUsing this patch we can connect many twemproxy instances (which runs on the same computer) to a redis server using the same socket, we were able to get a much greater throughput and still have one logical redis server. \n\nThese twemproxy instances can all be configured with the same configuration file, and only needs to have a different stats port. (we don't enable SO_REUSEPORT for that socket)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rosmo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/431", "title": "Allow ignoring SELECT from client", "body": "Even if you are just using database 0, some developers want to call Redis' SELECT command always up front. This add a new redis_ignore_select option to pools to simply return OK for any SELECT command.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "umegaya": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/424", "title": "add support of script load command by broadcast", "body": "refs #68 \nthis pull request aims to add support for SCRIPT LOAD command, by following fix.\n- define msg type for SCRIPT LOAD and parse correctly (8158d2e)\n- able to give 'broadcast' attribute to certain kind of msg type, and give broadcast attribute to SCRIPT command (8158d2e)\n- do broadcast correctly (cb50953)\n\nalso original python test seems to be broken, add new test by shell script (dfba9f8), you can run new test like following:\n\n```\ncd tests/test_redis_sh\n./run ./test_script_load.sh\n```\n\nit is only passed limited test case, and I'm very new to twemproxy, so not enough confidence about correctness. can you review and if it looks good, merge to master? thanks in advance.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dec5e": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/412", "title": "Removing duplicate Hootsuite mention in list of companies using Twemproxy", "body": "Hootsuite was added twice: in #291 and #387.\n\nPS. Maybe it's good to sort companies list alphabetically to prevent such duplication in future. I can do it if you agree.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ideal": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/411", "title": "add rbtree_entry to fetch a struct pointer from a rbtree node pointer", "body": "So the code is more generic.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "susman": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/400", "title": "add rhel7 compatibility", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ton31337": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/396", "title": "Add SO_REUSEPORT support to socket", "body": "Add SO_REUSEPORT support to socket. Introduce new feature for Twemproxy running on newer kernels. With this feature you are able to do upgrades, restarts without any downtime. Kernel does load balancing between processes with the same host:port pairs. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nanzhushan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/391", "title": "add", "body": "\u4e2d\u6587\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "travisbot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1743714) (merged bd16a4cc into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1744434) (merged 12b9c896 into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003216) (merged 9a17a01d into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003218) (merged f34559a3 into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "bmatheny": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865", "body": "Would you be open to a patch for this? The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063", "body": "SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nI agree the on the fly reload is tricky, I'll put some thought into it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782", "body": "One additional comment. Based on feedback from Manju I moved the old vlen values to vlen_rem, and made vlen be an immutable value representing the total size of the value. This allows a much cleaner calculation in the req_filter of whether the object value exceeds the configured item_size_max or not.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Ayutthaya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117", "body": "As a school project, I've made a patch to support kqueue, epoll and event ports, using the libevent API. There are also a few additional changes to make it work on my mac os x. (the patch is at github.com/ayutthaya/twemproxy ). I'm ready to improve my work if necessary, so don't hesitate to give feedback. Regards.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583", "body": "I changed the patch. Now it uses kqueue API directly (but doesn't support event ports anymore). It needed changes mainly to nc_event.[ch] and nc_stats.[ch], since stats aggregation was also based on epoll. The patch is still at github.com/ayutthaya/twemproxy . one question: what is the reason for not using libevent API ? Thanks. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593", "body": "I've tried to comply with the suggestions above as much as possible. In particular, I've abstracted out the event interface and used a function pointer to make all #ifdef HAVE_EPOLL/KQUEUE disappear in all files except nc_event.h nc_epoll.c and nc_kqueue.c. Don't hesitate if you think it needs further changes before doing a pull request. \nI also have a question about twemproxy: what is the number of sockets from which point one should use twemproxy / the performance of the cache server starts degrading seriously due to per-connection overhead? 65k? 200k? If you could simply give me a lead, it would help me a lot for my project. Thanks !\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jsholmes": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243", "body": "Any update on that stable build?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "mezzatto": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/1693973", "body": "This is awesome! Any known issues?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/1693973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nitper": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/3668996", "body": "@manjuraj just wanted to let you know that we are fully using twemproxy with redis at bright.com and are loving it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/3668996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "1125449708": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/4628152", "body": "Fantastic.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/4628152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dominis": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/5512362", "body": "lol\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/5512362/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "GOPALYADAV": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924236", "body": "# hash_tag\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924245", "body": "+language:6\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "BrandonBrowning": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10019622", "body": "I seem to have gotten a similar error with the latest code?\n\n```\n[2015-03-03 19:01:55.777] nc_redis.c:2076 parsed bad rsp 2 res 1 type 133 state 11\n00000000  2a 32 0d 0a 24 32 0d 0a  72 30 0d 0a 2b 4f 4b 0d   |*2..$2..r0..+OK.|\n00000010  0a                                                 |.|\n[2015-03-03 19:01:55.777] nc_core.c:198 recv on s 8 failed: Invalid argument\n```\n\nExpected\n\n```\n1) \"r0\"\n2) OK\n```\n\nCaused by running\n\n```\nevalsha 370ca495ee43ef26fae6c1d4dfa16baac375026d 1 a set 5\n```\n\n(it's a wrapper around normal command execution)\n(reference #108)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10019622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031992", "body": "Give twemproxy a single redis instance (only for simplicity; works with multiple obviously)\nBash variables represent ports\n\n```\n$ redis-cli -p $nutcracker set _name r0\n$ redis-cli -p $nutcracker eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a get\n1) \"r0\"\n2) (nil)\n\n# looking good\n\n$ redis-cli -p $nutcracker eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a set 5\n(error) ERR Invalid argument\n\n# oh no\n\n$ redis-cli -p $redis eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a set 5\n1) \"r0\"\n2) OK\n\n# works directly on client\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10071652", "body": "Repro case is fixed!  Thanks :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10071652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "vlm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829590", "body": "Does it make sense to add CONN_KIND_AS_STRING(conn) instead of every `\"s` though?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829594", "body": "What's the reason for this commit?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11831080", "body": "Ack.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11831080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Niteesh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739", "body": "if cp->select  is initlized by CONF_DEFAULT_SELECT, the function conf_set_num will throw error that select is duplicate,\nthus to avoid writing very similar function  and  let conf_set_num work fine, this is done, later if cp->select remains unset default value is  assigned in conf_validate_pool\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748", "body": "i made sure that when i call this funtion, connection is connected.\neven than i should  have kept it in loop to try more than once if EAGAIN error is encountered\n\nor Ill appritiate  your comments if there is a better way to do it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055", "body": "```\nvoid  setSelectDb(struct conn *conn,struct server *server){\n     ASSERT(!conn->client && conn->connected )\n     int n, i;\n     char selectCommand[25];\n     sprintf(selectCommand,\"*2\\r\\n$6\\r\\nSELECT\\r\\n$1\\r\\n%d\\r\\n\",server->owner->select);\n     n = write(conn->sd,selectCommand,strlen(selectCommand));\n if (n < 0) {\n     log_error(\"ERROR selecting db on  socket for socket %d-> error %d \", conn->sd, strerror(errno) );\n     conn->err = errno;\n }\n```\n\nI am really not sure about this  so i want to ask you if this will do the needful        \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "jdi-tagged": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780", "body": "This does the opposite of what the comment above says; with this change having unique server names doesn't help if the pnames match.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "allenlz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133", "body": "All fragements will be freed at the same time if some error happens (`rsp_make_error`). If there is no error, they will also be freed at the same time.\nIf I can't believe `frag_owner` is valid, which you think it may be a wild pointer, that will be a bug of twemproxy upstream.\nWill it happen?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}, "3": {"unisqu": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/546", "title": "can implement xxhash as hash?", "body": "can implement xxhash as hash?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stanisavs": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/543", "title": "Stats is empty", "body": "Hello!\r\nHow can I see stats?\r\n``src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222``\r\nThen I use this connection on production for 1 minute (5k qps).\r\nLogs:\r\n```...\r\n...\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 10 eof rb 567 sb 124\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 10 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 10 '127.0.0.1:20964' on event 00FF eof 1 done 1 rb 567 sb 124  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76105 done on c 8 req_time 0.226 msec type REQ_REDIS_ZREVRANGEBYSCORE narg 7 req_len 86 rsp_len 97 key0 'DB11' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76107 done on c 11 req_time 0.181 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '456:51:d8437695e7a446210fc356b730d5e909:tmp' peer '127.0.0.1:20968' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76111 done on c 8 req_time 0.140 msec type REQ_REDIS_HGET narg 3 req_len 100 rsp_len 13 key0 'tables:domain' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 11 eof rb 456 sb 159\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 11 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 11 '127.0.0.1:20968' on event 00FF eof 1 done 1 rb 456 sb 159  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76114 done on c 8 req_time 0.156 msec type REQ_REDIS_HINCRBY narg 4 req_len 90 rsp_len 7 key0 'imps:420155' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76116 done on c 8 req_time 0.151 msec type REQ_REDIS_HINCRBY narg 4 req_len 81 rsp_len 8 key0 'campaigns:ads:stats_day:420144' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76118 done on c 8 req_time 0.139 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '459:51:489c90dd70cc6bf3292a1722ac0f46ee:tmp' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_connection.c:360 recv on sd 8 eof rb 436 sb 130\r\n[2017-12-06 11:05:52.735] nc_request.c:430 c 8 is done\r\n....\r\n```\r\n\r\n\r\nStats (I checked it every 5 second):\r\n```\r\nsrc/nutcracker --describe-stats\r\nThis is nutcracker-0.4.1\r\n\r\npool stats:\r\n  client_eof          \"# eof on client connections\"\r\n  client_err          \"# errors on client connections\"\r\n  client_connections  \"# active client connections\"\r\n  server_ejects       \"# times backend server was ejected\"\r\n  forward_error       \"# times we encountered a forwarding error\"\r\n  fragments           \"# fragments created from a multi-vector request\"\r\n\r\nserver stats:\r\n  server_eof          \"# eof on server connections\"\r\n  server_err          \"# errors on server connections\"\r\n  server_timedout     \"# timeouts on server connections\"\r\n  server_connections  \"# active server connections\"\r\n  server_ejected_at   \"timestamp when server was ejected in usec since epoch\"\r\n  requests            \"# requests\"\r\n  request_bytes       \"total request bytes\"\r\n  responses           \"# responses\"\r\n  response_bytes      \"total response bytes\"\r\n  in_queue            \"# requests in incoming queue\"\r\n  in_queue_bytes      \"current request bytes in incoming queue\"\r\n  out_queue           \"# requests in outgoing queue\"\r\n  out_queue_bytes     \"current request bytes in outgoing queue\"\r\n```\r\n\r\n```\r\nps aux | grep nut\r\nroot      5985  1.0  0.0  18584  2660 ?        Sl   11:04   0:44 src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222\r\n```\r\n\r\n```\r\nnetstat -tulpn | grep 22222\r\ntcp        0      0 0.0.0.0:22222           0.0.0.0:*               LISTEN      5985/nutcracker \r\n```\r\n\r\nBTW connection with redis took 0.15ms, connection with twemproxy took 0.03ms, but 4 pipes with redis was faster (1.0-1.15ms) than the same operations with twemproxy (1.0-1.5ms). I was hoping for more acceleration.\r\n\r\nAnd also how to understand the logs?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "filippog": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/540", "title": "Prometheus stats support?", "body": "Hi,\r\nwould you be interested in having native Prometheus stats support? I'm not sure what would be the best way implementation-wise (a separate port?).", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532", "title": "Double quotes in server name result in invalid json stats", "body": "Hi,\r\nwe're using nutcracker/twemproxy with server names with double quotes, though this results in invalid json because the double quotes are not escaped but should (`src/nc_stats.c` at `stats_begin_nesting` if I'm reading the code correctly)", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "praveenbalaji-blippar": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/539", "title": "Twemproxy (Redis) for large batch/small # of connections", "body": "I use Redis in our offline processing (large batch/small # of connections). I want to use twemproxy to partition data. I see two issues which I want to address:\r\n\r\n- I notice that the performance drops ~2x when I use twemproxy.\r\n- Of greater concern to me is that twemproxy seems to drop the connection intermittently. I tried various `--mbuf-size` ranging from the default (16K) to 16M.\r\n\r\nI looked through several Issues on github, but most use cases seem to be large # of connection/online use cases. I want to figure out the configuration to handle large batch/small # of connections instead. Suggestions are much appreciated.\r\n\r\n\r\nOn an 8-core machine, `top` shows that available memory is not a problem. However, twemproxy appears to consume >90% CPU. The Redis instances consume ~10% CPU. When I execute the same commands in one of the Redis instances, the Redis instance consumes >90% CPU.\r\n\r\nThe configuration is as follows:\r\n```\r\ntest:\r\n  listen: 0.0.0.0:6378\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  redis: true\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 1\r\n  servers:\r\n   - 127.0.0.1:7378:1\r\n   - 127.0.0.1:7379:1\r\n```\r\n\r\nRequest payloads have one of the following characteristics:\r\n\r\n- pipelined request with `del`. Each pipelined request has 200,000 - 2,000,000 `del` commands. Each key is ~10 bytes.\r\n- `mget` batches of size 100,000 - 1,000,000. Each key and value are ~10 bytes.\r\n- pipelined request with `set` and `sadd`. Each pipelined request has 200,000 - 2,000,000 commands (set + sadd). Each key and value is ~10 bytes.\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "inter169": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/538", "title": "OOM on the mbuf freelist (100+GB)", "body": "the nutcracker process consumed 100+GB phisycal memory on my production box after a data migration from another redis to this one (nutcracker).\r\nand the gdb console showed below:\r\n```\r\n(gdb) p nfree_mbufq\r\n$1 = 6365614\r\n(gdb) p mbuf_chunk_size\r\n$1 = 16384\r\n```\r\n\r\nthe memory consumption was nfree_mbufq * mbuf_chunk_size = 101GB approx. \r\nI have read some code fixes (pr)s about the similar phenomenon, like:\r\nhttps://github.com/twitter/twemproxy/pull/461\r\nhttps://github.com/twitter/twemproxy/issues/203\r\n\r\nbut such fixes didn't set the limitation of the mbuf chunks, so the OOM was still here, \r\nI coded a fix, and the nutcracker can pass a command param ('-n ',  in my fix) to set the max number of mbuf chunks, once exceeded the limitation it can free the mbuf after processing one req immediately.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jhwillett": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/537", "title": "Twemproxy hangs parsing nested Array responses from Redis", "body": "Twemproxy hangs when processing a Redis Lua script via EVALwhich returns nested sub-arrays. \r\nThis is easy to reproduce.\r\n\r\nSetup:\r\n* On Mac OS 10.12.6.\r\n* Running redis-server 2.8.24 at localhost:7379 (installed via Homebrew).\r\n* Running nutcracker-0.4.1 at localhost:221221 which is configured with:\r\n```\r\ndevelopment:\r\n  redis:            true\r\n  auto_eject_hosts: false\r\n  listen:           127.0.0.1:22121\r\n  servers:\r\n    - 127.0.0.1:7379:1\r\n```\r\n\r\nDemo of direct connection to Redis being happy:\r\n```\r\n$ redis-cli -p 7379\r\n127.0.0.1:7379> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379> EVAL 'return {1,{2},3}' 1 x\r\n1) (integer) 1\r\n2) 1) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379>\r\n```\r\n\r\nDemo of connection through Twemproxy being unhappy:\r\n```\r\n$ redis-cli -p 221221\r\n127.0.0.1:221221> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:221221> EVAL 'return {1,{2},3}' 1 x\r\n^C\r\n```\r\nThe last command hangs as long as I was willing to wait (several minutes), until I cancel it.\r\n\r\nI see no messages in the twemproxy logs during this time.\r\n\r\nI notice that src/proto/nc_redis.c has special cases for nested multi bulk reply element from sscan/hscan/zscan.  Perhaps eval and evalsha are simply missing the special case treatment?\r\n\r\nI apologize for phrasing this as a bug report instead of as a pull request with a fix, but I am afraid it may be a long time before I can dig deeper.\r\n\r\nThank you for a wonderful tool.  It is a critical piece of our infrastructure at ProsperWorks and has made our lives much easier.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/537/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BlueCatFlord": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/535", "title": " Whether the project is in maintenance", "body": "Whether the project is in maintenance", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslusher": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/534", "title": "twemproxy only sees one of the memcached servers in the pool", "body": "It's my understanding that when a server in the twemproxy pool gets ejected, the other server in the pool should still be available for caching. It seems that when I take out *memcached-1 only*, the proxy itself becomes unavailable. If I take out memcached-2 from the pool, everything operates normally, except that there doesn't seem to be any indication in the logs that the server leaves or returns to the pool. \r\n\r\nI have tested that both memcached servers are available directly. If I put one or the other memcached sever by itself in the pool configuration, they're available using the proxy, but only memcached-1 is available if I have them both in the pool. I've tried ordering them differently and it doesn't seem to make a difference. A tcpdump only ever shows traffic to memcached-1 when they are both in the pool. When nutcracker is restarted, I only see arp traffic going to one of the two servers, but never both.\r\n\r\nTo reproduce:\r\n(nutcracker version 0.4.1 on centos 7)\r\n/etc/nutcracker/nutcracker.yml\r\n``` yaml\r\nbad_pool:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  server_retry_timeout: 30000\r\n  server_failure_limit: 3\r\n  servers:\r\n   - 10.10.10.33:11211:1 memcached-1\r\n   - 10.10.10.34:11211:1 memcached-2\r\n```\r\n\r\ntelnet 127.0.0.1 22122\r\n```\r\nTrying 127.0.0.1...\r\nConnected to 127.0.0.1.\r\nEscape character is '^]'.\r\nset testing 1 0 3\r\none\r\nSTORED\r\n```\r\n\r\nssh 10.10.10.33:\r\n```\r\nsudo systemctl stop memcached\r\n```\r\n\r\ntelnet console:\r\n```\r\nget testing\r\nSERVER_ERROR Connection refused\r\nConnection closed by foreign host.\r\n```\r\n\r\nnutcracker logs for sequence:\r\n```\r\n[2017-08-18 11:08:46.894] nc_core.c:43 max fds 1024 max client conns 989 max server conns 3\r\n[2017-08-18 11:08:46.894] nc_stats.c:851 m 4 listening on '0.0.0.0:22222'\r\n[2017-08-18 11:08:46.894] nc_proxy.c:217 p 6 listening on '127.0.0.1:22122' in memcache pool 0 'bad_pool' with 2 servers\r\n[2017-08-18 11:08:56.457] nc_proxy.c:377 accepted c 8 on p 6 from '127.0.0.1:41122'\r\n[2017-08-18 11:09:11.595] nc_request.c:96 req 1 done on c 8 req_time 1160.716 msec type REQ_MC_SET narg 2 req_len 24 rsp_len 8 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:00.115] nc_response.c:118 s 9 active 0 is done\r\n[2017-08-18 11:14:00.116] nc_core.c:237 close s 9 '10.50.20.35:11211' on event 00FF eof 1 done 1 rb 8 sb 24\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close s 9 '10.50.20.35:11211' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n[2017-08-18 11:14:06.887] nc_request.c:96 req 4 done on c 8 req_time 0.597 msec type REQ_MC_GET narg 2 req_len 13 rsp_len 33 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close c 8 '127.0.0.1:41122' on event FF00 eof 0 done 0 rb 37 sb 41: Operation not permitted\r\n```", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/534/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Vibe6": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/531", "title": "Change to grow yr Power", "body": "Hello twitter I what to give you advice please change \"follow\" text on yr button type. Add or only show a + income with profile it will help you believe me.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/531/reactions", "total_count": 3, "+1": 0, "-1": 3, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mishtika": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/530", "title": "Twemproxy \" Connection refused \" on failure of one redis instance", "body": "I have configured twem proxy with 2 redis servers.\r\nWhen one of these redis server fails the twem proxy gives a error saying \" Connection refused\"\r\n\r\nMy twem conf \r\nbeta:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  hash_tag: \"{}\"\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  redis: true\r\n  servers:\r\n   - 127.0.0.1:6381:1\r\n   - 127.0.0.1:6379:1\r\n\r\ni am trying to get a key from redis. If i kill one instance of redis and then try to run the get command then it gives the following error:\r\n[2017-07-26 16:25:24.548] nc_core.c:237 close s 15 '127.0.0.1:6381' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuetianle": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/528", "title": "is it  support the docker redis service ", "body": "when use the docker redis service ,it don't work well.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rposky": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/526", "title": "Add additional timeout option to apply to time between server request and response", "body": "The current \"timeout clock starts ticking the instant the message is enqueued into the server [in queue]\" and not at the time that the message is actually sent to the server. While this may be appropriate for some use cases, I suggest that it is problematic for others that wish to react to server response time irrespective of server demand.\r\n\r\nI have observed that in the presence of many requests using the same key, and thus mapped to the same server, a denial of service scenario is possible due to timeout of requests that may not have even left the server in-queue. Server ejection naturally follows, despite that the proxied service is still running and responding normally. It just could not respond to all queued requests within the configured timeout, again some of which it may not have even technically began servicing.\r\n\r\nIt would be beneficial were there an additional timeout option that could be evaluated specifically against server response times and used to influence server ejections in lieu of server_timeout errors. In the meantime, I have found that adjusting the existing timeout option to a much larger value can help to mitigate this error scenario, though it unfortunately means that the service will be slower to detect actual server issues.\r\n\r\nThanks ahead of time for any consideration and/or suggestions.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "danielkraaijbax": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/525", "title": "Redis PING command is sometimes slow.", "body": "Hi There,\r\n\r\nWe have a setup of Twemproxy and 3 Redis masters with 3 Redis Slaves  and i have found an issue with the PING command that we were using.\r\nFirst of, We use the PING command to check if a Redis server is correctly connected. We found some issues with this in the past where we would connect to Twemproxy, ask for a key and that the server did not respond. We fixed that and is totally not related to this ticket.\r\nIn our codebase (PHP) there was however still a line where we would issue a PING command after each connect. \r\nWe were sometimes experiencing slowness on our pages. Somehow our connection wrapper managed to wait 200ms before continuing.\r\n\r\nI created a couple of test scripts to try and resolve this issue.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->ping();\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times. \r\n\r\n> for i in {1..20}; do php redis-ping.php; sleep 1; done\r\n202.45825195312\r\n0.625\r\n1.968994140625\r\n0.60693359375\r\n1.89501953125\r\n202.28198242188\r\n0.452880859375\r\n1.438720703125\r\n201.91772460938\r\n2.072021484375\r\n201.50170898438\r\n202.705078125\r\n0.783935546875\r\n2.90966796875\r\n1.487060546875\r\n0.831787109375\r\n0.52587890625\r\n0.593017578125\r\n0.634033203125\r\n1.114990234375\r\n\r\nI made some adjustments to the script.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->get('FooBar');\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times.\r\n> for i in {1..20}; do php redis.php; sleep 1; done\r\n1.2978515625\r\n1.47607421875\r\n2.22216796875\r\n2.47509765625\r\n2.26708984375\r\n1.2548828125\r\n1.25\r\n0.85986328125\r\n4.005859375\r\n3.97802734375\r\n0.876953125\r\n1.739990234375\r\n0.784912109375\r\n0.760986328125\r\n1.220947265625\r\n3.2080078125\r\n2.267822265625\r\n0.810791015625\r\n2.806884765625\r\n0.744140625\r\n\r\nSo it seems that the PING command is sometimes slow.\r\nI assume this could be an issue? \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kigster": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/523", "title": "On newer SmartOS Twemproxy chooses epoll instead of event ports", "body": "Looks like SmartOS now [implements `epoll`](http://blog.shalman.org/exploring-epoll-on-smartos/), as well as their native event ports.\r\n\r\nBuild system prefers `epoll` when both are available, but it would be better if it chose native `event ports`. We are currently experiencing random sporadic lockups of twemproxy built with `epoll` support. Currently testing the event ports version.\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vishalsharma13": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/522", "title": "Redis rename command", "body": "hi,\r\n\r\nWhy twemproxy not used rename command? Is there any possibilty to use rename command through twemproxy.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516", "title": "Twemproxy Server Timedout", "body": "Hi,\r\n\r\nCan you please help to identify the reason of error with redis server.\r\n\r\nTwemproxy logs :- \r\n\r\n[2017-02-09 12:06:24.188] nc_request.c:96 req 219840 done on c 27903 req_time 1000.594 msec type REQ_REDIS_HMGET narg 7 req_len 117 rsp_len 27 key0 'USER:SESSION:8c5ab24a-c0b0-4ccd-a377-02c0b1d728ed' peer '10.247.74.50:58653' done 1 error 1\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1\r\n  timeout: 500\r\nbeta:\r\n  auto_eject_hosts: false\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 20000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 1000\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512", "title": "Twemproxy : Server Connections not closed properly", "body": "Hi,\r\n\r\nI am using Redis with 3 servers configuration i.e one master(to write data) and two slaves(to read data) and using Twemproxy for load balancing.\r\n\r\nAnd using Jedis with connection pooling.\r\n\r\nNow i doing load testing but i am facing below errors\r\n\r\n1) Twemproxy Server connections are not properly closed on time and increased upto double or triple of  request(Like for 10k requests server connections opened upto 30k connections)\r\n\r\n2) Getting exception like unable to get resource from pool however my poolsize limit is not exceeded and after one command connection returns back to pool.\r\n\r\nServer Configurations :- \r\n\r\n1) 64GB RAM\r\n2) 16 core processor\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1\r\n  timeout: 5000\r\nbeta:\r\n  auto_eject_hosts: true\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 50000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 5000\r\n\r\n\r\nJedis Pool Configuration :- \r\n\r\npublic class RedisReadManager {\r\n    private static final RedisReadManager instance = new RedisReadManager();\r\n    private static JedisPool pool;\r\n    private RedisReadManager() {}\r\n    public final static RedisReadManager getInstance() {\r\n        return instance;\r\n    }\r\n    public void connect(String host, int port) {\r\n        try {\r\n        \tif(pool==null || pool.isClosed()){\r\n\t\t\t\tJedisPoolConfig poolConfig = new JedisPoolConfig();\r\n\t\t\t\tpoolConfig.setMaxTotal(60000);\r\n\t\t\t\tpoolConfig.setTestOnBorrow(true);\r\n\t\t\t\tpoolConfig.setTestOnReturn(true);\r\n\t\t\t\tpoolConfig.setMaxIdle(3000);\r\n\t\t\t\tpoolConfig.setMinIdle(100);\r\n\t\t\t\tpoolConfig.setTestWhileIdle(true);\r\n\t\t\t\tpoolConfig.setNumTestsPerEvictionRun(10);\r\n\t\t\t\tpoolConfig.setTimeBetweenEvictionRunsMillis(10000);\r\n\t\t\t\tpool = new JedisPool(poolConfig, host, port);\r\n        \t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\t\t\t\r\n\t\t}\r\n    }\r\n    public void releasePool() {\r\n        try {\r\n\t\t\tif(pool!=null&&!pool.isClosed()){\r\n\t\t\t\tpool.destroy();\r\n\t\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n    public Jedis getJedis() {\r\n        return pool.getResource();\r\n    }\r\n\r\n\tpublic void returnJedis(Jedis jedis) {\r\n    \ttry {\r\n\t\t\tif(jedis!=null&jedis.isConnected()){\r\n\t\t\t\tjedis.disconnect();\r\n\t\t\t\tjedis.close();\r\n\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n}\r\n\r\nNutcracker-web :- \r\n\r\n![nutcracker-web](https://cloud.githubusercontent.com/assets/22707039/22018052/ebaf5d92-dcd2-11e6-91c6-463af69e960a.jpg)\r\n\r\nPlease help to resolve the issue.\r\n \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ArcticSnowman": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/521", "title": "Any recommendations For Using KeepaliveD with Twemproxy?", "body": "Anyone got recommendations For Using KeepaliveD with Twemproxy?\r\n\r\nUse a simple tcp check against the port?  Something more sophisticated with a check on the stats port?\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518", "title": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands", "body": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands.\r\n\r\n`redis-cli GET <key>`  does not work for twemproxy as redis-cli issues a `COMMAND` command to check that you have a valid command to send. \r\n\r\nThis works in interactive mode...\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mcanonic": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/519", "title": "Support for BRPOP and BLPOP", "body": "Hi,\r\nI saw the notes/redis.md file and I'm wondering if the unsupported commands are somethin that you are working on it or not. Just to know if the (near?) future these commads like BRPOP e BLPOP will be supported.\r\nThanks\r\nM", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hjhart": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/514", "title": "Twemproxy cluster not accepting clients", "body": "On a machine running twemproxy, it is no longer accepting client connections. We're not totally sure why, at this point. The logs look interesting enough, ending with an `eof`. I think I'm just not understanding 1) how twemproxy would ever \"go down\" and 2) choose not to come back up.\r\n\r\nThe redis machines are responsive, because we have five other twemproxy machines with exactly the same configuration and they are still up. There is a process that runs JUST before this problem occurs, and it is hitting redis incredibly hard. We're just not sure conceptually _why_ twemproxy goes down, and what configuration we can change to make it stop. \r\n\r\nUnderstanding, of course, that we are hitting redis incredibly hard from a single process and we could choose _not_ to do that, but we want to first understand why twemproxy goes down, and then why it _stays_ down.\r\n\r\nAnother thing to note, is that we've recently updated from twemproxy 0.3.0 to 0.4.1, and this didn't appear to happen on 0.3.0.\r\n\r\nThe end of the logs looks like this: \r\n\r\n## Logs\r\n\r\n```\r\n> tail /var/log/twemproxy-daily-sale.log\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782534 done on c 30 req_time 27.163 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20888845:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782535 done on c 30 req_time 27.181 msec type REQ_REDIS_DEL narg 2 req_len 34 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782536 done on c 30 req_time 27.200 msec type REQ_REDIS_RPUSH narg 12 req_len 177 rsp_len 5 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782539 done on c 30 req_time 27.209 msec type REQ_REDIS_EXPIRE narg 3 req_len 49 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782540 done on c 30 req_time 27.227 msec type REQ_REDIS_DEL narg 2 req_len 35 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782541 done on c 30 req_time 27.246 msec type REQ_REDIS_RPUSH narg 3 req_len 51 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782544 done on c 30 req_time 27.256 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:57.393] nc_connection.c:360 recv on sd 30 eof rb 830971428 sb 32948345\r\n[2017-01-27 05:44:57.393] nc_request.c:430 c 30 is done\r\n[2017-01-27 05:44:57.393] nc_core.c:237 close c 30 '10.100.17.85:56791' on event 00FF eof 1 done 1 rb 830971428 sb 32948345\r\n```\r\n\r\n## Configuration\r\n\r\nOur configuration on twemproxy looks like this:\r\n<details>\r\n  <summary>Click to expand configuration</summary>\r\n\r\n\r\n```\r\n> cat /etc/twemproxy/twemproxy-daily-sale.yml\r\ndaily-sale:\r\n  hash: fnv1_32\r\n  hash_tag: \"::\"\r\n  distribution: ketama\r\n  timeout: 1000\r\n  auto_eject_hosts: false\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 3\r\n  redis: true\r\n  listen: 10.100.18.248:22139\r\n  servers:\r\n  - 10.100.17.85:37001:1 daily-sale001\r\n  - 10.100.17.85:37002:1 daily-sale002\r\n  - 10.100.17.85:37003:1 daily-sale003\r\n  - 10.100.17.85:37004:1 daily-sale004\r\n  - 10.100.17.241:37005:1 daily-sale005\r\n  - 10.100.17.241:37006:1 daily-sale006\r\n  - 10.100.17.241:37007:1 daily-sale007\r\n  - 10.100.17.241:37008:1 daily-sale008\r\n  - 10.100.18.19:37009:1 daily-sale009\r\n  - 10.100.18.19:37010:1 daily-sale010\r\n  - 10.100.18.19:37011:1 daily-sale011\r\n  - 10.100.18.19:37012:1 daily-sale012\r\n  - 10.100.18.100:37013:1 daily-sale013\r\n  - 10.100.18.100:37014:1 daily-sale014\r\n  - 10.100.18.100:37015:1 daily-sale015\r\n  - 10.100.18.100:37016:1 daily-sale016\r\n```\r\n\r\n</details>\r\n\r\n## Connections\r\n\r\n`netstat -an` shows that there is a single `CLOSE_WAIT` connection stuck.\r\n\r\n```\r\n> netstat -an | grep 22139\r\n10.100.18.248.22139        *.*                0      0 1048576      0 LISTEN\r\n10.100.18.248.22139  10.100.104.187.23453 1049792      0 1049800      0 CLOSE_WAIT\r\n```\r\n\r\nAnd on the corresponding machine (10.100.104.187) there is a `SYN_SENT` connection stuck:\r\n\r\n```\r\n> netstat -an | grep 10.100.18.248\r\n10.100.104.187.28052 10.100.18.248.22136  1049792      0 1049800      0 ESTABLISHED\r\n10.100.104.187.38585 10.100.18.248.22139      0      0 1049740      0 SYN_SENT\r\n10.100.104.187.34939 10.100.18.248.22135  1049792      0 1049800      0 ESTABLISHED\r\n```\r\n\r\n## Statistics\r\n\r\nOur statistics do not indicate any servers are out of the hash ring...\r\n\r\n<details>\r\n  <summary>Click to expand statistics</summary>\r\n\r\n```\r\n> nc localhost 22227 | json\r\n{\r\n  \"service\": \"nutcracker\",\r\n  \"source\": \"twemproxy100.prod\",\r\n  \"version\": \"0.4.1\",\r\n  \"uptime\": 90618,\r\n  \"timestamp\": 1485566119,\r\n  \"total_connections\": 459796,\r\n  \"curr_connections\": 17,\r\n  \"daily-sale\": {\r\n    \"client_eof\": 3927,\r\n    \"client_err\": 455852,\r\n    \"client_connections\": 0,\r\n    \"server_ejects\": 0,\r\n    \"forward_error\": 0,\r\n    \"fragments\": 0,\r\n    \"daily-sale001\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 499925,\r\n      \"request_bytes\": 52043467,\r\n      \"responses\": 499925,\r\n      \"response_bytes\": 2109629,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale002\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 425855,\r\n      \"request_bytes\": 44208662,\r\n      \"responses\": 425855,\r\n      \"response_bytes\": 1791852,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale003\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 516402,\r\n      \"request_bytes\": 53579106,\r\n      \"responses\": 516402,\r\n      \"response_bytes\": 2170560,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale004\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 571640,\r\n      \"request_bytes\": 59475059,\r\n      \"responses\": 571640,\r\n      \"response_bytes\": 2405288,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale005\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 488641,\r\n      \"request_bytes\": 50937149,\r\n      \"responses\": 488641,\r\n      \"response_bytes\": 2060132,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale006\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 595062,\r\n      \"request_bytes\": 61596411,\r\n      \"responses\": 595062,\r\n      \"response_bytes\": 2514791,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale007\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 539622,\r\n      \"request_bytes\": 56245033,\r\n      \"responses\": 539622,\r\n      \"response_bytes\": 2274943,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale008\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 450732,\r\n      \"request_bytes\": 47025684,\r\n      \"responses\": 450732,\r\n      \"response_bytes\": 1898874,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale009\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 461595,\r\n      \"request_bytes\": 47897394,\r\n      \"responses\": 461595,\r\n      \"response_bytes\": 1938910,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale010\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 502702,\r\n      \"request_bytes\": 52229759,\r\n      \"responses\": 502702,\r\n      \"response_bytes\": 2124118,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale011\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 489467,\r\n      \"request_bytes\": 50950224,\r\n      \"responses\": 489467,\r\n      \"response_bytes\": 2060700,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale012\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 510860,\r\n      \"request_bytes\": 53141344,\r\n      \"responses\": 510860,\r\n      \"response_bytes\": 2145320,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale013\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 446362,\r\n      \"request_bytes\": 46440311,\r\n      \"responses\": 446362,\r\n      \"response_bytes\": 1872555,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale014\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 427993,\r\n      \"request_bytes\": 44477521,\r\n      \"responses\": 427993,\r\n      \"response_bytes\": 1803340,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale015\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 508857,\r\n      \"request_bytes\": 52884727,\r\n      \"responses\": 508857,\r\n      \"response_bytes\": 2141577,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale016\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 564176,\r\n      \"request_bytes\": 58592099,\r\n      \"responses\": 564176,\r\n      \"response_bytes\": 2376014,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n</details>\r\n\r\n## Connecting\r\n\r\nAnd similarly trying to establish a connection to twemproxy hangs indefinitely...\r\n\r\n```\r\n> telnet 10.100.18.248 22139\r\nTrying 10.100.18.248...\r\n```\r\n\r\nAny and all help is appreciated. Thank you!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/514/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jennyfountain": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/513", "title": "Questions about twemproxy and performance", "body": "We are seeing a few issues that I was hoping someone could help me resolve or point me in the right direction.\r\n\r\n1.  During high loads, we are seeing a lot of backup in the out_queue_bytes.  On normal traffic loads, this is 0. \r\n\r\nExample (sometimes goes into 2k/3k range as well):\r\n    \"out_queue_bytes\": 33\r\n    \"out_queue_bytes\": 91\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 174\r\n\r\nIn addition, it shows that our time spent in memcache goes up from 400 ms to 1000-2000 ms. This seriously affects our application. \r\n\r\n2.  auto eject also seems to not work as expected. Server goes down and our app freaks out - saying it cannot access a memcache server.\r\n\r\nhere is an example of a config:\r\n\r\nweb:\r\n  listen: /var/run/nutcracker/web.sock 0777\r\n  auto_eject_hosts: true\r\n  distribution: ketama\r\n  hash: one_at_a_time\r\n  backlog: 65536\r\n  server_connections: 16\r\n  server_failure_limit: 3\r\n  server_retry_timeout: 30000\r\n  servers:\r\n   - 1.2.3.4:11211:1\r\n   - 1.2.3.5:11211:1\r\n   - 1.2.3.6:11211:1\r\n  timeout: 2000\r\n\r\nsomaxconn = 128\r\n\r\nWhat we tried and didn't help.\r\n1.  mbuf to 512\r\n2.  server connection from 1 to 200\r\n\r\nThank you for any guidance on this problem.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "manjuraj": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/34eb60fb977da9aa8bcac784dee1e7fb04c79d47", "message": "Merge pull request #517 from takayamaki/fix_typo\n\nfix typo in README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0f9e1baf06db478065ba43402b975c57a567d00f", "message": "Merge pull request #492 from kalifg/patch-1\n\nFix typo in notes/memcache.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6fd922039bc696e07b7ef18946732df6b7cf2a5d", "message": "Merge pull request #493 from dennismartensson/patch-1\n\nUpdate README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/017d44503344ecdf2439747476556f4b9fee3e21", "message": "Merge pull request #494 from mortonfox/patch-1\n\nUpdate the sensu-metrics link"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e5739338ddf228b3b900aa116646cb9654fa5f65", "message": "Merge pull request #489 from rohitpaulk/update-redis-docs\n\nUpdate redis docs for PING and QUIT"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/74af2fb2d5d3e214d8c0741a4b0ebb7d93572fc8", "message": "Merge pull request #439 from Krinkle/patch-1\n\nreadme: Link to HTTPS for wikimedia.org"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9a2611e5992e65e6c15cd23bce06ffed3901f4f8", "message": "Merge pull request #425 from charsyam/feature/typos\n\nfix typos"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/619b55b6a1dcd21ffe16f312b52cd576c6a4caa0", "message": "Merge pull request #421 from charsyam/feature/upgrade-redis-lib\n\nupgrade redis-py version from 2.9.0 to 2.10.3"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/592c1b6b157721f66789bca6a3d463f32e6c59fc", "message": "Merge pull request #420 from esindril/master\n\nAdd script to build SRPM and fix spec file"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/defda674cdfb94fef31bdf8cc7a1dd766ea0a05a", "message": "Merge pull request #418 from twitter/revert-406-bugfix/redis-error-response-parser\n\nRevert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/65bb2ac5bbf0acce3fa52469752046c388e56ef5", "message": "Revert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/34f369ff91076b5d1b1229eca51bc3a2ed8f6d4a", "message": "Merge pull request #406 from tom-dalton-fanduel/bugfix/redis-error-response-parser\n\nFix parsing bug when error body contains no spaces"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954", "body": "I would be willing to take patches for kqueue support in twemproxy\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968", "body": "Thank you for the patches. I will merge this in the next version\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668", "body": "twemproxy does not do replication. If you are doing replication, you would have to do it on the client side\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106", "body": "> 1. Before you started work on twemproxy, did you consider using moxi: https://github.com/steveyen/moxi? If yes, why did you reject moxi?\n>    with nutcracker we have:  \n>    a) protocol pipelining which works really well for us, as we wanted to introduce minimal latency degration with a proxy sitting between a client and server.\n>    b) mbuf, which essentially enables zero copy when moving data from client to server (and vice versa)\n>    c) observability which was fairly important to us in our production environment\n> 2. Are you planning to support binary protocol in addition to ascii?\n>    no (see 'thoughts' section in notes/memcache.txt)\n> 3. Do you see any need for multi-threaded support? Moxi supports both single and multi-threaded configurations.\n>    no; if a run proxy of proxy (client --> proxy --> (proxy)+ --> server) you can actually make use of all cores and would probably be network bound in this scenario\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681", "body": "This is a good idea.\n\nThe start up time of twemproxy is really small, especially when preconnect is not set. So, if your clients had retries built into it, then you can trivially propagate configuration changes by doing a rolling restarts of twemprox'ies. This is a good enough solution, imo\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486", "body": "> > Would you be open to a patch for this?\n> > I am always open to accepting patches :) \n\nI just think that reloading of configuration file on-the-fly is tricky to get right.\n\n> >  The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n\nI am curious as how this is happening. I set SO_REUSEADDR address option on the twemcache's listening socket, which means that we would reuse ports even if they are busy (See: proxy_reuse() function in nc_proxy.c)\n\nCould you paste me the log file dump with the error that you seeing? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606", "body": "Thanks for the patch using libevent API. \n\nI am willing to accept patches if they don't use libevent but directly call bsd's kqueue API. All you might have to do is to implement abstraction in the event interfaces, which would be in nc_event.[ch]. Let if know if you have any questions. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377", "body": "you can always change the ulimit of the shell from which you launch nutcracker\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857", "body": "blake the patch looks great. One thing I realized is that memcache has some item header overhead and slab header overhead even for the largest item. So, even if you configure memcache with 1MB slab, the largest item that can be stored in the slab is < 1MB. Furthermore the item size not only includes the value length, but also key length. \n\nGiven this, do you think we should have two extra keys in yml configuration\nitem_max_kvlen: (maximum key + value length)\nitem_overhead:\n\nand we discard requests whose key + value length + overhead > item_max_kvlen\n\nthoughts?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944", "body": "the reason I don't use libevent api is because I wanted to build nutcracker without any dependency on 3rd party libraries. furthermore, I use ET semantics, which I believe is not available in 1.4 version of libevent.\n\ncould i get a pull request of your changes?\n\nI also looked at your changes. I think you might want to do few cleanups before submitting a pull request. Few suggestions:\na) abstract out the event interface and wire to the underlying event call using a function pointer\nb) maybe create event/ directory that contains event/nc_epoll.c and event/nc_kqueue.c files. \nc) follow style conventions outlined in: https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133", "body": "> SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nIs there a test case that would reproduce this scenario? I am unable to reproduce this scenario, even if I bombard twemproxy with a constant stream of traffic and restart it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361", "body": "blake, I wonder if this issue would go away if we set l_linger = 0 by calling nc_set_linger(sd, 0) on the listening sockets (see notes/socket.txt for details on it)? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080", "body": "Thanks for the test case; this is really useful\n\nI am still actively working on the redis branch. hopefully we should have a stable build out by end of this month\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159", "body": "should be straight forward if aws conforms to memcache ascii protocol\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813", "body": "is there a reason why you would prefer inline command over unified command besides the obvious ease of issuing such command from telenet;\n\njust supporting unified protocol makes parsing for req / rsp easy and simple in twemproxy.\n\nif you would like to contribute, you could look at nc_parse.[ch]\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651", "body": "twemproxy should just work with kestrel. I guess you might be well off using \"distribution: random\" instead of default of \"distribution: ketama\" for a pool of kestrel severs\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677", "body": "apologies @jsholmes ; I am still working on this\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536", "body": "No need for a py script. You can generate the strings using macro magic called strigificaion :) This way your code never gets outdate\n\nSee this for reference -- \n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L23-L49\n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L117-L122 \n- http://gcc.gnu.org/onlinedocs/cpp/Stringification.html\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553", "body": "s/char */uint_8 */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558", "body": "usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586", "body": "space between the closing paren and curly brace. So `log_loggable(LOG_NOTICE) {`\nHere are the coding guidelines -- https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt. Would really appreciate if you follow it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603", "body": "just call it req_log()\n\nalso format is `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612", "body": "formatting -- `/* a fragment */`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643", "body": "formatting - `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649", "body": "formatting - `if (rsp) {`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660", "body": "call this `int_64_t start_ts /\\* request start timestamp in usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410", "body": "Rename\ntotal_connections to ntotal_conn\ncurr_connections to nconn\ncurr_client_connections to nclient_conn\n\ncurr_connections should be int32_t\ncurr_client_connections should be int32_t\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414", "body": "how did we come up with RESERVED_FDS number as 32\nrename it as NC_NUM_RESERVED_FD\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416", "body": "I think we might still hit this scenario - One instance where I see this happening is that you close the connection() from the code (decrement the counters), but the sockets are still in TIME_WAIT state and hence are not available for accept() sys call.\n\nSo, instead of panic, we should log() and return NC_ERROR and not set p->recv_ready to 0\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121", "body": "what is this for?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123", "body": "why did we bump this up?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127", "body": "[formatting] `status: %d`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172", "body": "is get_mbuf() function required? It seems to be called only in msg_make_reply(). Prefer not having this function\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178", "body": "why are these commented out? Isn't it required to trigger the out event?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216", "body": "Should we move the lines 482 - 495 into server_pool_idx()? It seems that only place where the tag logic is not used is in server_pool_server(), which is surprising, because I think that tag logic should also apply there equally.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257", "body": "In this piece of code, you allocate message structs for every server in the pool - ncontinuum messages, even if incoming messages are destined to < ncontinuum servers.\n\nOnly at line 818-819, do you some of these allocated message structs. Can we refactor this differently - maybe two loops where the first loop aggregates all the keys destined per server and the second loop has the logic\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}]}, "mortonfox": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/fe68175e0200e3c2589139438ff3efa392042aa6", "message": "Update the sensu-metrics link"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dennismartensson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/80ef6a7444fd5ae97fcab9606c1abedc19f00824", "message": "Update README.md\n\nAdded Greta to the list of companys"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kalifg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b87ba1abfe6a814999279e69af7ce07ba0ff6c68", "message": "Fix typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohitpaulk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/eed195341a02fa688b0dfb784d120f337f15a454", "message": "Update redis docs for PING and QUIT\n\nSupport for these was introduced in\n@4175419288ef66d95e082cfa2124e77fe6d4fe6d."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "andyqzb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/330f43a430261aa48d4063771ed70fe191177154", "message": "Merge pull request #486 from deep011/deep011-patch-1\n\nfix a memory leak bug for mset command"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ced2044980e4b9dd0c91b25fc64ce127879a1491", "message": "Merge pull request #484 from postwait/patch-1\n\nFix typo circunous -> circonus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "deep011": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/558e0d40ad79f423c4784565648e6c83cf035777", "message": "fix a memory leak bug for mset command"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/462", "title": "Fix a bug for msg->mlen", "body": "Function msg_append() already added the dst->mlen.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "postwait": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1e078e9e9d97560825ae4f1245177a0af29e3c82", "message": "Fix typo circunous -> circonus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Krinkle": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/91a68d3c42638eb8178001f4d67d2606dcd80f51", "message": "readme: Link to HTTPS for wikimedia.org"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "esindril": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/51c5228acdd8a324a43107330f6b936de028dc0c", "message": "Fix spec file to work for RHEL >= 6 and wrong changelog date"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/205323f87deb0f0004963717f2d7a80eed8e9c3b", "message": "Add script to build the SRPM package for RPM-based distributions"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "caniszczyk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/0e8708be4d365163a1061c2a89ecc344e21203d6", "message": "Add Uber to the list of Adopters\n\nhttp://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128", "body": "Any updates on this patch @manjuraj ?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280", "body": "Just to note that this is somewhat of a hack, but it works for now. I didn't find any tests to run via gtest or whatever harness.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "idning": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1f4e0dae62baa379209e4cdfe64f2224dadce632", "message": "Merge pull request #410 from vincentve/master\n\noptimize performance when single key del"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350", "body": "if current msg is a sub-msg, the `msg->frag_owner` may be freed, and reset by `_msg_get`, this is not correct, especially for `req_error`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378", "body": "macro create clean code but break cscope/ctags :( \n\nclean code is more important, I will have a try.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "invalid-email-address": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b6ac1bbc34aa0f470b55893090c7cf6a696ec184", "message": "optimize performance when single key del"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tom-dalton-fanduel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/6f32a928b4830d218fef67aa5d22bc0fd44000f6", "message": "Add testcase"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TysonAndre": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/545", "title": "Always initialize file permissions field for unix domain socket", "body": "It seems like field->perm might be uninitialized memory\r\ndepending on how it is allocated.\r\nThe intended behavior is to only change file permissions from the default if a permission was specified in the config:\r\nhttps://github.com/twitter/twemproxy/pull/311/files#diff-f74ea9da930e79a9573455a0cbe4785d\r\n\r\nI ran into an issue where different sockets had different file\r\npermissions, and some of those sockets weren't readable by the user\r\nwhich created it. (I specified *only* the path to the unix domain socket)\r\n\r\nThis behavior probably started in\r\nhttps://github.com/twitter/twemproxy/pull/311/files", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phamhongviet": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/544", "title": "Add systemd service file", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "essanpupil": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/541", "title": "fix list indentation in README", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "idirouhab": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/536", "title": "Add Foodora as company who uses it in prod", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pataquets": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/524", "title": "Add Docker support", "body": "Add Dockerfile to enable image building.\r\nUsing the official GCC image, latest tag. More info at https://hub.docker.com/_/gcc/\r\n\r\nJust adding files, setting working dir and running make instructions.\r\n\r\nBuild:\r\n\r\n```\r\n$ docker build -t twemproxy .\r\n```\r\n\r\nRun:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro twemproxy -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nFYI, there's a still quicker to test, already built image on my Docker Hub. Test it by running:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro pataquets/twemproxy  -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nUsing `--rm` instead of `-d` makes the container not go background and it to be deleted after stop. Should stop by `CTRL+C`'ing it.\r\nIn order for the Docker container to connect to external memcached or Redis instances, either them should be contactable as external IPs or hosts or be linked to other previously run Docker containers via Docker's ```--link``` option.\r\n\r\nHere it is an example Docker Compose file I'm using (I can submit it with the PR also if you find it useful):\r\n```\r\nredis1:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63791:6379\r\n\r\nredis2:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63792:6379\r\n\r\ntwemproxy:\r\n  image: pataquets/twemproxy\r\n  command: -c /etc/nutcracker.yml --verbose=6\r\n  links:\r\n    - redis1\r\n    - redis2\r\n  ports:\r\n    - 6379:6379\r\n    - 22222:22222\r\n  volumes:\r\n    - ./conf/nutcracker.redis.yml:/etc/nutcracker.yml:ro\r\n```\r\nNotice that it is tuned for two Redis instances (yml file not included, mount yours)\r\n\r\nOptional improvement to come (maybe in another issue):\r\n- Create an 'official', based on your repo, automated build at Docker Hub for the image: https://docs.docker.com/docker-hub/builds/ . Just requires a free Docker Hub account and a following a quick 'Create automated build' process. I'll be happy to help on it, if needed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "galusben": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/520", "title": "JFrog is using twemproxy on production", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "santoshsahoo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/515", "title": "Added CircleHD to companies using Twemproxy", "body": "Updated README.md, we are using of twemproxy on aws.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "voetsjoeba": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/511", "title": "Fix build failure with --disable-stats", "body": "A build with ./configure --disable-stats currently fails with the following error (gcc 4.8.5 on CentOS 6):\r\n\r\n```\r\nnc_server.c: In function \u2018server_failure\u2019:\r\nnc_server.c:291:5: warning: implicit declaration of function \u2018stats_server_set_ts\u2019 [-Wimplicit-function-declaration]\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n     ^\r\nnc_server.c:291:38: error: \u2018server_ejected_at\u2019 undeclared (first use in this function)\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n                                      ^\r\nnc_server.c:291:38: note: each undeclared identifier is reported only once for each function it appears in\r\n```\r\n\r\nThe reason appears to be that some functions are not being nopped out in `nc_stats.h` when `NC_STATS` is 0. This change adds the missing entries. Technically only `stats_server_set_ts` is needed to fix the build error, but from the intent of the code it's clear that `stats_pool_set_ts` should also be nopped out, despite never being called directly.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/510", "title": "Allow negative exptime values in memcached storage commands", "body": "Memcached allows negative exptime values in storage commands to immediately expire the stored values, but this wasn't very well documented until recently (here's the commit that adds the documentation from May 2016: https://github.com/memcached/memcached/commit/e7d4521cd8b27f7ebc6e4c1b9aee9eb3544f6af5).\r\n\r\nThis patch allows the memcached parser to support negative exptime values in storage commands. The server still responds with the usual STORED if the exptime is negative, so no additional response handling is needed.\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ugurengin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/502", "title": "update memcache populate script", "body": "update memcache populate script to improve test cause of memcached for get and set operations", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mahdi-hdi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/497", "title": "Added Geo Functionality", "body": "New Redis functionality for geo date type now supported:\n\n```\nGEOADD\nGEODIST\nGEOHASH\nGEOPOS\nGEORADIUS\nGEORADIUSBYMEMBER\n```\n\n**Mutli bulk array** was implemented in Redis response parser to support response of **GEORADIUS**. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "willfitch": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/480", "title": "Implement ERROR protocol", "body": "Rather than severing connections upon an invalid event, this patch adds the ERROR portion of the Memcached protocol.  This does not, however, add CLIENT_ERROR or SERVER_ERROR.  That can be discussed.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "VishalRocks": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/476", "title": "Update README.md", "body": "Added Codechef in Companies using Twemproxy in Production\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "artursitarski": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/474", "title": "Wikia as twemproxy user", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flygoast": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/463", "title": "Implemented client connections limiting of server pool.", "body": "Implemented client connections limiting of server pool.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "huachaohuang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/461", "title": "Fix memory leak for redis mset.", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charsyam": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/458", "title": "Fix Bug: redis_auth don't work with redis_db", "body": "currently, twemproxy ignore \"redis_db\" when \"redis_auth\" is set.\nselect command is set as noforward because of auth.\n\nthis patch fix this.\nand move add_auth to post_connected.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/428", "title": "rebase heartbeat branch ", "body": "It had a bug when redis is loading.\nbut after patch of https://github.com/twitter/twemproxy/commit/ef453130e321974b332dd99d585ae7285eee4b5d\n(handle loading state as error)\nI think it fixed.\n\nIn my tests. this works fine :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/423", "title": "Fix parsing bug when error body contains no spaces", "body": "This is only copy for https://github.com/twitter/twemproxy/pull/406 (by @tom-dalton-fanduel )\nfor testing. I just PR again.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/395", "title": "support nested multibulk reply", "body": "@manjuraj This is part of https://github.com/twitter/twemproxy/pull/393\n\nfirst. I divided it that is only supporting nested multi bulk reply.\n\nafter merging this :)\n\nI will PR second of them :)\n\nThanks for your reivew :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/393", "title": "support geo commands and nested multibulk reply.", "body": "1. support GEO Commands\n   -> geoadd, geohash, geodist, georadius, georadiusbymember\n2. support nested multibulk reply\n   -> geo commands return nested multibulk reply\n\ncurrently we only support this type of multibulk\n\n```\n                 * - mulit-bulk\n                 *    - cursor\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n```\n\nbut georadius and georadiusbymember returns maximum 3 depth multibulk. so this patch supports this kind of multibulk also.\n\n```\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n```\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572", "body": "How was just initializing with CONF_DEFAULT_SELECT?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575", "body": "I just think that add new conf file for this patch like this: nutcracker.select.yml\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579", "body": "in twemproxy, it is better to change the variable name to is_select_msg\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597", "body": "and conn->sd is nonblocking socket. so write can be failed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "ofirule": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/435", "title": "Added support for listening to same port from multiple twemproxy processes", "body": "My team (from IBM Security Trusteer) has ran into some bottlenecks when using twemproxy and developed this patch.\n\nThis patch allows to use socket option SO_REUSEPORT on twemproxy socket. This option is available on Linux kernel version 3.9+.\n\nAfter stress testing our redis-server we found that handling many clients consumes too much processing from redis but not redis code itself rather its networking code (epoll/etc.).\n\nIt seems that the single thread nature of redis maxes out the networking part with one core in extreme conditions (like many connections scenario, no pipelining, etc.).\n\nUsing this patch we can connect many twemproxy instances (which runs on the same computer) to a redis server using the same socket, we were able to get a much greater throughput and still have one logical redis server. \n\nThese twemproxy instances can all be configured with the same configuration file, and only needs to have a different stats port. (we don't enable SO_REUSEPORT for that socket)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rosmo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/431", "title": "Allow ignoring SELECT from client", "body": "Even if you are just using database 0, some developers want to call Redis' SELECT command always up front. This add a new redis_ignore_select option to pools to simply return OK for any SELECT command.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "umegaya": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/424", "title": "add support of script load command by broadcast", "body": "refs #68 \nthis pull request aims to add support for SCRIPT LOAD command, by following fix.\n- define msg type for SCRIPT LOAD and parse correctly (8158d2e)\n- able to give 'broadcast' attribute to certain kind of msg type, and give broadcast attribute to SCRIPT command (8158d2e)\n- do broadcast correctly (cb50953)\n\nalso original python test seems to be broken, add new test by shell script (dfba9f8), you can run new test like following:\n\n```\ncd tests/test_redis_sh\n./run ./test_script_load.sh\n```\n\nit is only passed limited test case, and I'm very new to twemproxy, so not enough confidence about correctness. can you review and if it looks good, merge to master? thanks in advance.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dec5e": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/412", "title": "Removing duplicate Hootsuite mention in list of companies using Twemproxy", "body": "Hootsuite was added twice: in #291 and #387.\n\nPS. Maybe it's good to sort companies list alphabetically to prevent such duplication in future. I can do it if you agree.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ideal": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/411", "title": "add rbtree_entry to fetch a struct pointer from a rbtree node pointer", "body": "So the code is more generic.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "susman": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/400", "title": "add rhel7 compatibility", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ton31337": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/396", "title": "Add SO_REUSEPORT support to socket", "body": "Add SO_REUSEPORT support to socket. Introduce new feature for Twemproxy running on newer kernels. With this feature you are able to do upgrades, restarts without any downtime. Kernel does load balancing between processes with the same host:port pairs. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nanzhushan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/391", "title": "add", "body": "\u4e2d\u6587\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "travisbot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1743714) (merged bd16a4cc into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1744434) (merged 12b9c896 into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003216) (merged 9a17a01d into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003218) (merged f34559a3 into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "bmatheny": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865", "body": "Would you be open to a patch for this? The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063", "body": "SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nI agree the on the fly reload is tricky, I'll put some thought into it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782", "body": "One additional comment. Based on feedback from Manju I moved the old vlen values to vlen_rem, and made vlen be an immutable value representing the total size of the value. This allows a much cleaner calculation in the req_filter of whether the object value exceeds the configured item_size_max or not.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Ayutthaya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117", "body": "As a school project, I've made a patch to support kqueue, epoll and event ports, using the libevent API. There are also a few additional changes to make it work on my mac os x. (the patch is at github.com/ayutthaya/twemproxy ). I'm ready to improve my work if necessary, so don't hesitate to give feedback. Regards.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583", "body": "I changed the patch. Now it uses kqueue API directly (but doesn't support event ports anymore). It needed changes mainly to nc_event.[ch] and nc_stats.[ch], since stats aggregation was also based on epoll. The patch is still at github.com/ayutthaya/twemproxy . one question: what is the reason for not using libevent API ? Thanks. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593", "body": "I've tried to comply with the suggestions above as much as possible. In particular, I've abstracted out the event interface and used a function pointer to make all #ifdef HAVE_EPOLL/KQUEUE disappear in all files except nc_event.h nc_epoll.c and nc_kqueue.c. Don't hesitate if you think it needs further changes before doing a pull request. \nI also have a question about twemproxy: what is the number of sockets from which point one should use twemproxy / the performance of the cache server starts degrading seriously due to per-connection overhead? 65k? 200k? If you could simply give me a lead, it would help me a lot for my project. Thanks !\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jsholmes": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243", "body": "Any update on that stable build?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Niteesh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739", "body": "if cp->select  is initlized by CONF_DEFAULT_SELECT, the function conf_set_num will throw error that select is duplicate,\nthus to avoid writing very similar function  and  let conf_set_num work fine, this is done, later if cp->select remains unset default value is  assigned in conf_validate_pool\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748", "body": "i made sure that when i call this funtion, connection is connected.\neven than i should  have kept it in loop to try more than once if EAGAIN error is encountered\n\nor Ill appritiate  your comments if there is a better way to do it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055", "body": "```\nvoid  setSelectDb(struct conn *conn,struct server *server){\n     ASSERT(!conn->client && conn->connected )\n     int n, i;\n     char selectCommand[25];\n     sprintf(selectCommand,\"*2\\r\\n$6\\r\\nSELECT\\r\\n$1\\r\\n%d\\r\\n\",server->owner->select);\n     n = write(conn->sd,selectCommand,strlen(selectCommand));\n if (n < 0) {\n     log_error(\"ERROR selecting db on  socket for socket %d-> error %d \", conn->sd, strerror(errno) );\n     conn->err = errno;\n }\n```\n\nI am really not sure about this  so i want to ask you if this will do the needful        \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "jdi-tagged": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780", "body": "This does the opposite of what the comment above says; with this change having unique server names doesn't help if the pnames match.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "allenlz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133", "body": "All fragements will be freed at the same time if some error happens (`rsp_make_error`). If there is no error, they will also be freed at the same time.\nIf I can't believe `frag_owner` is valid, which you think it may be a wild pointer, that will be a bug of twemproxy upstream.\nWill it happen?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}, "4": {"unisqu": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/546", "title": "can implement xxhash as hash?", "body": "can implement xxhash as hash?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "stanisavs": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/543", "title": "Stats is empty", "body": "Hello!\r\nHow can I see stats?\r\n``src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222``\r\nThen I use this connection on production for 1 minute (5k qps).\r\nLogs:\r\n```...\r\n...\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 10 eof rb 567 sb 124\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 10 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 10 '127.0.0.1:20964' on event 00FF eof 1 done 1 rb 567 sb 124  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76105 done on c 8 req_time 0.226 msec type REQ_REDIS_ZREVRANGEBYSCORE narg 7 req_len 86 rsp_len 97 key0 'DB11' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76107 done on c 11 req_time 0.181 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '456:51:d8437695e7a446210fc356b730d5e909:tmp' peer '127.0.0.1:20968' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76111 done on c 8 req_time 0.140 msec type REQ_REDIS_HGET narg 3 req_len 100 rsp_len 13 key0 'tables:domain' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.734] nc_connection.c:360 recv on sd 11 eof rb 456 sb 159\r\n[2017-12-06 11:05:52.734] nc_request.c:430 c 11 is done\r\n[2017-12-06 11:05:52.734] nc_core.c:237 close c 11 '127.0.0.1:20968' on event 00FF eof 1 done 1 rb 456 sb 159  \r\n[2017-12-06 11:05:52.734] nc_request.c:96 req 76114 done on c 8 req_time 0.156 msec type REQ_REDIS_HINCRBY narg 4 req_len 90 rsp_len 7 key0 'imps:420155' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76116 done on c 8 req_time 0.151 msec type REQ_REDIS_HINCRBY narg 4 req_len 81 rsp_len 8 key0 'campaigns:ads:stats_day:420144' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_request.c:96 req 76118 done on c 8 req_time 0.139 msec type REQ_REDIS_SETEX narg 4 req_len 79 rsp_len 5 key0 '459:51:489c90dd70cc6bf3292a1722ac0f46ee:tmp' peer '127.0.0.1:20970' done 1 error 0\r\n[2017-12-06 11:05:52.735] nc_connection.c:360 recv on sd 8 eof rb 436 sb 130\r\n[2017-12-06 11:05:52.735] nc_request.c:430 c 8 is done\r\n....\r\n```\r\n\r\n\r\nStats (I checked it every 5 second):\r\n```\r\nsrc/nutcracker --describe-stats\r\nThis is nutcracker-0.4.1\r\n\r\npool stats:\r\n  client_eof          \"# eof on client connections\"\r\n  client_err          \"# errors on client connections\"\r\n  client_connections  \"# active client connections\"\r\n  server_ejects       \"# times backend server was ejected\"\r\n  forward_error       \"# times we encountered a forwarding error\"\r\n  fragments           \"# fragments created from a multi-vector request\"\r\n\r\nserver stats:\r\n  server_eof          \"# eof on server connections\"\r\n  server_err          \"# errors on server connections\"\r\n  server_timedout     \"# timeouts on server connections\"\r\n  server_connections  \"# active server connections\"\r\n  server_ejected_at   \"timestamp when server was ejected in usec since epoch\"\r\n  requests            \"# requests\"\r\n  request_bytes       \"total request bytes\"\r\n  responses           \"# responses\"\r\n  response_bytes      \"total response bytes\"\r\n  in_queue            \"# requests in incoming queue\"\r\n  in_queue_bytes      \"current request bytes in incoming queue\"\r\n  out_queue           \"# requests in outgoing queue\"\r\n  out_queue_bytes     \"current request bytes in outgoing queue\"\r\n```\r\n\r\n```\r\nps aux | grep nut\r\nroot      5985  1.0  0.0  18584  2660 ?        Sl   11:04   0:44 src/nutcracker -d --mbuf-size=1024 --stats-interval=30000 --verbose=6 --output=/var/log/nginx/proxy.log --stats-port=22222\r\n```\r\n\r\n```\r\nnetstat -tulpn | grep 22222\r\ntcp        0      0 0.0.0.0:22222           0.0.0.0:*               LISTEN      5985/nutcracker \r\n```\r\n\r\nBTW connection with redis took 0.15ms, connection with twemproxy took 0.03ms, but 4 pipes with redis was faster (1.0-1.15ms) than the same operations with twemproxy (1.0-1.5ms). I was hoping for more acceleration.\r\n\r\nAnd also how to understand the logs?", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/543/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "filippog": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/540", "title": "Prometheus stats support?", "body": "Hi,\r\nwould you be interested in having native Prometheus stats support? I'm not sure what would be the best way implementation-wise (a separate port?).", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/540/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532", "title": "Double quotes in server name result in invalid json stats", "body": "Hi,\r\nwe're using nutcracker/twemproxy with server names with double quotes, though this results in invalid json because the double quotes are not escaped but should (`src/nc_stats.c` at `stats_begin_nesting` if I'm reading the code correctly)", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/532/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "praveenbalaji-blippar": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/539", "title": "Twemproxy (Redis) for large batch/small # of connections", "body": "I use Redis in our offline processing (large batch/small # of connections). I want to use twemproxy to partition data. I see two issues which I want to address:\r\n\r\n- I notice that the performance drops ~2x when I use twemproxy.\r\n- Of greater concern to me is that twemproxy seems to drop the connection intermittently. I tried various `--mbuf-size` ranging from the default (16K) to 16M.\r\n\r\nI looked through several Issues on github, but most use cases seem to be large # of connection/online use cases. I want to figure out the configuration to handle large batch/small # of connections instead. Suggestions are much appreciated.\r\n\r\n\r\nOn an 8-core machine, `top` shows that available memory is not a problem. However, twemproxy appears to consume >90% CPU. The Redis instances consume ~10% CPU. When I execute the same commands in one of the Redis instances, the Redis instance consumes >90% CPU.\r\n\r\nThe configuration is as follows:\r\n```\r\ntest:\r\n  listen: 0.0.0.0:6378\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  redis: true\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 1\r\n  servers:\r\n   - 127.0.0.1:7378:1\r\n   - 127.0.0.1:7379:1\r\n```\r\n\r\nRequest payloads have one of the following characteristics:\r\n\r\n- pipelined request with `del`. Each pipelined request has 200,000 - 2,000,000 `del` commands. Each key is ~10 bytes.\r\n- `mget` batches of size 100,000 - 1,000,000. Each key and value are ~10 bytes.\r\n- pipelined request with `set` and `sadd`. Each pipelined request has 200,000 - 2,000,000 commands (set + sadd). Each key and value is ~10 bytes.\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/539/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "inter169": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/538", "title": "OOM on the mbuf freelist (100+GB)", "body": "the nutcracker process consumed 100+GB phisycal memory on my production box after a data migration from another redis to this one (nutcracker).\r\nand the gdb console showed below:\r\n```\r\n(gdb) p nfree_mbufq\r\n$1 = 6365614\r\n(gdb) p mbuf_chunk_size\r\n$1 = 16384\r\n```\r\n\r\nthe memory consumption was nfree_mbufq * mbuf_chunk_size = 101GB approx. \r\nI have read some code fixes (pr)s about the similar phenomenon, like:\r\nhttps://github.com/twitter/twemproxy/pull/461\r\nhttps://github.com/twitter/twemproxy/issues/203\r\n\r\nbut such fixes didn't set the limitation of the mbuf chunks, so the OOM was still here, \r\nI coded a fix, and the nutcracker can pass a command param ('-n ',  in my fix) to set the max number of mbuf chunks, once exceeded the limitation it can free the mbuf after processing one req immediately.\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/538/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jhwillett": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/537", "title": "Twemproxy hangs parsing nested Array responses from Redis", "body": "Twemproxy hangs when processing a Redis Lua script via EVALwhich returns nested sub-arrays. \r\nThis is easy to reproduce.\r\n\r\nSetup:\r\n* On Mac OS 10.12.6.\r\n* Running redis-server 2.8.24 at localhost:7379 (installed via Homebrew).\r\n* Running nutcracker-0.4.1 at localhost:221221 which is configured with:\r\n```\r\ndevelopment:\r\n  redis:            true\r\n  auto_eject_hosts: false\r\n  listen:           127.0.0.1:22121\r\n  servers:\r\n    - 127.0.0.1:7379:1\r\n```\r\n\r\nDemo of direct connection to Redis being happy:\r\n```\r\n$ redis-cli -p 7379\r\n127.0.0.1:7379> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379> EVAL 'return {1,{2},3}' 1 x\r\n1) (integer) 1\r\n2) 1) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:7379>\r\n```\r\n\r\nDemo of connection through Twemproxy being unhappy:\r\n```\r\n$ redis-cli -p 221221\r\n127.0.0.1:221221> EVAL 'return {1,2,3}' 1 x\r\n1) (integer) 1\r\n2) (integer) 2\r\n3) (integer) 3\r\n127.0.0.1:221221> EVAL 'return {1,{2},3}' 1 x\r\n^C\r\n```\r\nThe last command hangs as long as I was willing to wait (several minutes), until I cancel it.\r\n\r\nI see no messages in the twemproxy logs during this time.\r\n\r\nI notice that src/proto/nc_redis.c has special cases for nested multi bulk reply element from sscan/hscan/zscan.  Perhaps eval and evalsha are simply missing the special case treatment?\r\n\r\nI apologize for phrasing this as a bug report instead of as a pull request with a fix, but I am afraid it may be a long time before I can dig deeper.\r\n\r\nThank you for a wonderful tool.  It is a critical piece of our infrastructure at ProsperWorks and has made our lives much easier.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/537/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "BlueCatFlord": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/535", "title": " Whether the project is in maintenance", "body": "Whether the project is in maintenance", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/535/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jslusher": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/534", "title": "twemproxy only sees one of the memcached servers in the pool", "body": "It's my understanding that when a server in the twemproxy pool gets ejected, the other server in the pool should still be available for caching. It seems that when I take out *memcached-1 only*, the proxy itself becomes unavailable. If I take out memcached-2 from the pool, everything operates normally, except that there doesn't seem to be any indication in the logs that the server leaves or returns to the pool. \r\n\r\nI have tested that both memcached servers are available directly. If I put one or the other memcached sever by itself in the pool configuration, they're available using the proxy, but only memcached-1 is available if I have them both in the pool. I've tried ordering them differently and it doesn't seem to make a difference. A tcpdump only ever shows traffic to memcached-1 when they are both in the pool. When nutcracker is restarted, I only see arp traffic going to one of the two servers, but never both.\r\n\r\nTo reproduce:\r\n(nutcracker version 0.4.1 on centos 7)\r\n/etc/nutcracker/nutcracker.yml\r\n``` yaml\r\nbad_pool:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  server_retry_timeout: 30000\r\n  server_failure_limit: 3\r\n  servers:\r\n   - 10.10.10.33:11211:1 memcached-1\r\n   - 10.10.10.34:11211:1 memcached-2\r\n```\r\n\r\ntelnet 127.0.0.1 22122\r\n```\r\nTrying 127.0.0.1...\r\nConnected to 127.0.0.1.\r\nEscape character is '^]'.\r\nset testing 1 0 3\r\none\r\nSTORED\r\n```\r\n\r\nssh 10.10.10.33:\r\n```\r\nsudo systemctl stop memcached\r\n```\r\n\r\ntelnet console:\r\n```\r\nget testing\r\nSERVER_ERROR Connection refused\r\nConnection closed by foreign host.\r\n```\r\n\r\nnutcracker logs for sequence:\r\n```\r\n[2017-08-18 11:08:46.894] nc_core.c:43 max fds 1024 max client conns 989 max server conns 3\r\n[2017-08-18 11:08:46.894] nc_stats.c:851 m 4 listening on '0.0.0.0:22222'\r\n[2017-08-18 11:08:46.894] nc_proxy.c:217 p 6 listening on '127.0.0.1:22122' in memcache pool 0 'bad_pool' with 2 servers\r\n[2017-08-18 11:08:56.457] nc_proxy.c:377 accepted c 8 on p 6 from '127.0.0.1:41122'\r\n[2017-08-18 11:09:11.595] nc_request.c:96 req 1 done on c 8 req_time 1160.716 msec type REQ_MC_SET narg 2 req_len 24 rsp_len 8 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:00.115] nc_response.c:118 s 9 active 0 is done\r\n[2017-08-18 11:14:00.116] nc_core.c:237 close s 9 '10.50.20.35:11211' on event 00FF eof 1 done 1 rb 8 sb 24\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close s 9 '10.50.20.35:11211' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n[2017-08-18 11:14:06.887] nc_request.c:96 req 4 done on c 8 req_time 0.597 msec type REQ_MC_GET narg 2 req_len 13 rsp_len 33 key0 'testing' peer '127.0.0.1:41122' done 1 error 0\r\n[2017-08-18 11:14:06.887] nc_core.c:237 close c 8 '127.0.0.1:41122' on event FF00 eof 0 done 0 rb 37 sb 41: Operation not permitted\r\n```", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/534/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Vibe6": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/531", "title": "Change to grow yr Power", "body": "Hello twitter I what to give you advice please change \"follow\" text on yr button type. Add or only show a + income with profile it will help you believe me.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/531/reactions", "total_count": 3, "+1": 0, "-1": 3, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mishtika": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/530", "title": "Twemproxy \" Connection refused \" on failure of one redis instance", "body": "I have configured twem proxy with 2 redis servers.\r\nWhen one of these redis server fails the twem proxy gives a error saying \" Connection refused\"\r\n\r\nMy twem conf \r\nbeta:\r\n  listen: 127.0.0.1:22122\r\n  hash: fnv1a_64\r\n  hash_tag: \"{}\"\r\n  distribution: ketama\r\n  auto_eject_hosts: true\r\n  timeout: 400\r\n  redis: true\r\n  servers:\r\n   - 127.0.0.1:6381:1\r\n   - 127.0.0.1:6379:1\r\n\r\ni am trying to get a key from redis. If i kill one instance of redis and then try to run the get command then it gives the following error:\r\n[2017-07-26 16:25:24.548] nc_core.c:237 close s 15 '127.0.0.1:6381' on event FFFFFF eof 0 done 0 rb 0 sb 0: Connection refused\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/530/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yuetianle": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/528", "title": "is it  support the docker redis service ", "body": "when use the docker redis service ,it don't work well.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/528/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rposky": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/526", "title": "Add additional timeout option to apply to time between server request and response", "body": "The current \"timeout clock starts ticking the instant the message is enqueued into the server [in queue]\" and not at the time that the message is actually sent to the server. While this may be appropriate for some use cases, I suggest that it is problematic for others that wish to react to server response time irrespective of server demand.\r\n\r\nI have observed that in the presence of many requests using the same key, and thus mapped to the same server, a denial of service scenario is possible due to timeout of requests that may not have even left the server in-queue. Server ejection naturally follows, despite that the proxied service is still running and responding normally. It just could not respond to all queued requests within the configured timeout, again some of which it may not have even technically began servicing.\r\n\r\nIt would be beneficial were there an additional timeout option that could be evaluated specifically against server response times and used to influence server ejections in lieu of server_timeout errors. In the meantime, I have found that adjusting the existing timeout option to a much larger value can help to mitigate this error scenario, though it unfortunately means that the service will be slower to detect actual server issues.\r\n\r\nThanks ahead of time for any consideration and/or suggestions.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "danielkraaijbax": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/525", "title": "Redis PING command is sometimes slow.", "body": "Hi There,\r\n\r\nWe have a setup of Twemproxy and 3 Redis masters with 3 Redis Slaves  and i have found an issue with the PING command that we were using.\r\nFirst of, We use the PING command to check if a Redis server is correctly connected. We found some issues with this in the past where we would connect to Twemproxy, ask for a key and that the server did not respond. We fixed that and is totally not related to this ticket.\r\nIn our codebase (PHP) there was however still a line where we would issue a PING command after each connect. \r\nWe were sometimes experiencing slowness on our pages. Somehow our connection wrapper managed to wait 200ms before continuing.\r\n\r\nI created a couple of test scripts to try and resolve this issue.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->ping();\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times. \r\n\r\n> for i in {1..20}; do php redis-ping.php; sleep 1; done\r\n202.45825195312\r\n0.625\r\n1.968994140625\r\n0.60693359375\r\n1.89501953125\r\n202.28198242188\r\n0.452880859375\r\n1.438720703125\r\n201.91772460938\r\n2.072021484375\r\n201.50170898438\r\n202.705078125\r\n0.783935546875\r\n2.90966796875\r\n1.487060546875\r\n0.831787109375\r\n0.52587890625\r\n0.593017578125\r\n0.634033203125\r\n1.114990234375\r\n\r\nI made some adjustments to the script.\r\n```php\r\n<?php\r\n\r\n$start = (microtime(true) * 1000);\r\n$redis = new Redis();\r\n$redis->connect('<twemproxyIP>', 6379, 1);\r\n$redis->get('FooBar');\r\n$end = (microtime(true) * 1000);\r\n\r\necho $end-$start.PHP_EOL;\r\n```\r\nThis resulted in the following times.\r\n> for i in {1..20}; do php redis.php; sleep 1; done\r\n1.2978515625\r\n1.47607421875\r\n2.22216796875\r\n2.47509765625\r\n2.26708984375\r\n1.2548828125\r\n1.25\r\n0.85986328125\r\n4.005859375\r\n3.97802734375\r\n0.876953125\r\n1.739990234375\r\n0.784912109375\r\n0.760986328125\r\n1.220947265625\r\n3.2080078125\r\n2.267822265625\r\n0.810791015625\r\n2.806884765625\r\n0.744140625\r\n\r\nSo it seems that the PING command is sometimes slow.\r\nI assume this could be an issue? \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/525/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kigster": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/523", "title": "On newer SmartOS Twemproxy chooses epoll instead of event ports", "body": "Looks like SmartOS now [implements `epoll`](http://blog.shalman.org/exploring-epoll-on-smartos/), as well as their native event ports.\r\n\r\nBuild system prefers `epoll` when both are available, but it would be better if it chose native `event ports`. We are currently experiencing random sporadic lockups of twemproxy built with `epoll` support. Currently testing the event ports version.\r\n\r\nThanks!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/523/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "vishalsharma13": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/522", "title": "Redis rename command", "body": "hi,\r\n\r\nWhy twemproxy not used rename command? Is there any possibilty to use rename command through twemproxy.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/522/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516", "title": "Twemproxy Server Timedout", "body": "Hi,\r\n\r\nCan you please help to identify the reason of error with redis server.\r\n\r\nTwemproxy logs :- \r\n\r\n[2017-02-09 12:06:24.188] nc_request.c:96 req 219840 done on c 27903 req_time 1000.594 msec type REQ_REDIS_HMGET narg 7 req_len 117 rsp_len 27 key0 'USER:SESSION:8c5ab24a-c0b0-4ccd-a377-02c0b1d728ed' peer '10.247.74.50:58653' done 1 error 1\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1\r\n  timeout: 500\r\nbeta:\r\n  auto_eject_hosts: false\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: false\r\n  redis: true\r\n  server_connections: 20000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 1000\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/516/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512", "title": "Twemproxy : Server Connections not closed properly", "body": "Hi,\r\n\r\nI am using Redis with 3 servers configuration i.e one master(to write data) and two slaves(to read data) and using Twemproxy for load balancing.\r\n\r\nAnd using Jedis with connection pooling.\r\n\r\nNow i doing load testing but i am facing below errors\r\n\r\n1) Twemproxy Server connections are not properly closed on time and increased upto double or triple of  request(Like for 10k requests server connections opened upto 30k connections)\r\n\r\n2) Getting exception like unable to get resource from pool however my poolsize limit is not exceeded and after one command connection returns back to pool.\r\n\r\nServer Configurations :- \r\n\r\n1) 64GB RAM\r\n2) 16 core processor\r\n\r\nTwemproxy Configurations :- \r\n\r\nalpha:\r\n  auto_eject_hosts: false\r\n  client_connections: 40000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8511\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 10000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6380:1\r\n  timeout: 5000\r\nbeta:\r\n  auto_eject_hosts: true\r\n  client_connections: 75000\r\n  distribution: modula\r\n  hash: crc32a\r\n  hash_tag: '{}'\r\n  listen: 0.0.0.0:8510\r\n  preconnect: true\r\n  redis: true\r\n  server_connections: 50000\r\n  server_failure_limit: 2\r\n  server_retry_timeout: 30000\r\n  servers:\r\n  - 0.0.0.0:6382:1 slave1\r\n  - 0.0.0.0:6381:1 slave2\r\n  timeout: 5000\r\n\r\n\r\nJedis Pool Configuration :- \r\n\r\npublic class RedisReadManager {\r\n    private static final RedisReadManager instance = new RedisReadManager();\r\n    private static JedisPool pool;\r\n    private RedisReadManager() {}\r\n    public final static RedisReadManager getInstance() {\r\n        return instance;\r\n    }\r\n    public void connect(String host, int port) {\r\n        try {\r\n        \tif(pool==null || pool.isClosed()){\r\n\t\t\t\tJedisPoolConfig poolConfig = new JedisPoolConfig();\r\n\t\t\t\tpoolConfig.setMaxTotal(60000);\r\n\t\t\t\tpoolConfig.setTestOnBorrow(true);\r\n\t\t\t\tpoolConfig.setTestOnReturn(true);\r\n\t\t\t\tpoolConfig.setMaxIdle(3000);\r\n\t\t\t\tpoolConfig.setMinIdle(100);\r\n\t\t\t\tpoolConfig.setTestWhileIdle(true);\r\n\t\t\t\tpoolConfig.setNumTestsPerEvictionRun(10);\r\n\t\t\t\tpoolConfig.setTimeBetweenEvictionRunsMillis(10000);\r\n\t\t\t\tpool = new JedisPool(poolConfig, host, port);\r\n        \t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\t\t\t\r\n\t\t}\r\n    }\r\n    public void releasePool() {\r\n        try {\r\n\t\t\tif(pool!=null&&!pool.isClosed()){\r\n\t\t\t\tpool.destroy();\r\n\t\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n    public Jedis getJedis() {\r\n        return pool.getResource();\r\n    }\r\n\r\n\tpublic void returnJedis(Jedis jedis) {\r\n    \ttry {\r\n\t\t\tif(jedis!=null&jedis.isConnected()){\r\n\t\t\t\tjedis.disconnect();\r\n\t\t\t\tjedis.close();\r\n\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\te.printStackTrace();\r\n\t\t\tthrow e;\r\n\t\t}\r\n    }\r\n}\r\n\r\nNutcracker-web :- \r\n\r\n![nutcracker-web](https://cloud.githubusercontent.com/assets/22707039/22018052/ebaf5d92-dcd2-11e6-91c6-463af69e960a.jpg)\r\n\r\nPlease help to resolve the issue.\r\n \r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ArcticSnowman": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/521", "title": "Any recommendations For Using KeepaliveD with Twemproxy?", "body": "Anyone got recommendations For Using KeepaliveD with Twemproxy?\r\n\r\nUse a simple tcp check against the port?  Something more sophisticated with a check on the stats port?\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/521/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518", "title": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands", "body": "Please support the server command 'COMMAND' as 'redis-cli' uses it for non-interactive commands.\r\n\r\n`redis-cli GET <key>`  does not work for twemproxy as redis-cli issues a `COMMAND` command to check that you have a valid command to send. \r\n\r\nThis works in interactive mode...\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/518/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mcanonic": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/519", "title": "Support for BRPOP and BLPOP", "body": "Hi,\r\nI saw the notes/redis.md file and I'm wondering if the unsupported commands are somethin that you are working on it or not. Just to know if the (near?) future these commads like BRPOP e BLPOP will be supported.\r\nThanks\r\nM", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/519/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hjhart": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/514", "title": "Twemproxy cluster not accepting clients", "body": "On a machine running twemproxy, it is no longer accepting client connections. We're not totally sure why, at this point. The logs look interesting enough, ending with an `eof`. I think I'm just not understanding 1) how twemproxy would ever \"go down\" and 2) choose not to come back up.\r\n\r\nThe redis machines are responsive, because we have five other twemproxy machines with exactly the same configuration and they are still up. There is a process that runs JUST before this problem occurs, and it is hitting redis incredibly hard. We're just not sure conceptually _why_ twemproxy goes down, and what configuration we can change to make it stop. \r\n\r\nUnderstanding, of course, that we are hitting redis incredibly hard from a single process and we could choose _not_ to do that, but we want to first understand why twemproxy goes down, and then why it _stays_ down.\r\n\r\nAnother thing to note, is that we've recently updated from twemproxy 0.3.0 to 0.4.1, and this didn't appear to happen on 0.3.0.\r\n\r\nThe end of the logs looks like this: \r\n\r\n## Logs\r\n\r\n```\r\n> tail /var/log/twemproxy-daily-sale.log\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782534 done on c 30 req_time 27.163 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20888845:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782535 done on c 30 req_time 27.181 msec type REQ_REDIS_DEL narg 2 req_len 34 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782536 done on c 30 req_time 27.200 msec type REQ_REDIS_RPUSH narg 12 req_len 177 rsp_len 5 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782539 done on c 30 req_time 27.209 msec type REQ_REDIS_EXPIRE narg 3 req_len 49 rsp_len 4 key0 'ds:20889139:mp' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782540 done on c 30 req_time 27.227 msec type REQ_REDIS_DEL narg 2 req_len 35 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782541 done on c 30 req_time 27.246 msec type REQ_REDIS_RPUSH narg 3 req_len 51 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:55.946] nc_request.c:96 req 21782544 done on c 30 req_time 27.256 msec type REQ_REDIS_EXPIRE narg 3 req_len 50 rsp_len 4 key0 'ds:20889503:fbw' peer '10.100.17.85:56791' done 1 error 0\r\n[2017-01-27 05:44:57.393] nc_connection.c:360 recv on sd 30 eof rb 830971428 sb 32948345\r\n[2017-01-27 05:44:57.393] nc_request.c:430 c 30 is done\r\n[2017-01-27 05:44:57.393] nc_core.c:237 close c 30 '10.100.17.85:56791' on event 00FF eof 1 done 1 rb 830971428 sb 32948345\r\n```\r\n\r\n## Configuration\r\n\r\nOur configuration on twemproxy looks like this:\r\n<details>\r\n  <summary>Click to expand configuration</summary>\r\n\r\n\r\n```\r\n> cat /etc/twemproxy/twemproxy-daily-sale.yml\r\ndaily-sale:\r\n  hash: fnv1_32\r\n  hash_tag: \"::\"\r\n  distribution: ketama\r\n  timeout: 1000\r\n  auto_eject_hosts: false\r\n  server_retry_timeout: 2000\r\n  server_failure_limit: 3\r\n  redis: true\r\n  listen: 10.100.18.248:22139\r\n  servers:\r\n  - 10.100.17.85:37001:1 daily-sale001\r\n  - 10.100.17.85:37002:1 daily-sale002\r\n  - 10.100.17.85:37003:1 daily-sale003\r\n  - 10.100.17.85:37004:1 daily-sale004\r\n  - 10.100.17.241:37005:1 daily-sale005\r\n  - 10.100.17.241:37006:1 daily-sale006\r\n  - 10.100.17.241:37007:1 daily-sale007\r\n  - 10.100.17.241:37008:1 daily-sale008\r\n  - 10.100.18.19:37009:1 daily-sale009\r\n  - 10.100.18.19:37010:1 daily-sale010\r\n  - 10.100.18.19:37011:1 daily-sale011\r\n  - 10.100.18.19:37012:1 daily-sale012\r\n  - 10.100.18.100:37013:1 daily-sale013\r\n  - 10.100.18.100:37014:1 daily-sale014\r\n  - 10.100.18.100:37015:1 daily-sale015\r\n  - 10.100.18.100:37016:1 daily-sale016\r\n```\r\n\r\n</details>\r\n\r\n## Connections\r\n\r\n`netstat -an` shows that there is a single `CLOSE_WAIT` connection stuck.\r\n\r\n```\r\n> netstat -an | grep 22139\r\n10.100.18.248.22139        *.*                0      0 1048576      0 LISTEN\r\n10.100.18.248.22139  10.100.104.187.23453 1049792      0 1049800      0 CLOSE_WAIT\r\n```\r\n\r\nAnd on the corresponding machine (10.100.104.187) there is a `SYN_SENT` connection stuck:\r\n\r\n```\r\n> netstat -an | grep 10.100.18.248\r\n10.100.104.187.28052 10.100.18.248.22136  1049792      0 1049800      0 ESTABLISHED\r\n10.100.104.187.38585 10.100.18.248.22139      0      0 1049740      0 SYN_SENT\r\n10.100.104.187.34939 10.100.18.248.22135  1049792      0 1049800      0 ESTABLISHED\r\n```\r\n\r\n## Statistics\r\n\r\nOur statistics do not indicate any servers are out of the hash ring...\r\n\r\n<details>\r\n  <summary>Click to expand statistics</summary>\r\n\r\n```\r\n> nc localhost 22227 | json\r\n{\r\n  \"service\": \"nutcracker\",\r\n  \"source\": \"twemproxy100.prod\",\r\n  \"version\": \"0.4.1\",\r\n  \"uptime\": 90618,\r\n  \"timestamp\": 1485566119,\r\n  \"total_connections\": 459796,\r\n  \"curr_connections\": 17,\r\n  \"daily-sale\": {\r\n    \"client_eof\": 3927,\r\n    \"client_err\": 455852,\r\n    \"client_connections\": 0,\r\n    \"server_ejects\": 0,\r\n    \"forward_error\": 0,\r\n    \"fragments\": 0,\r\n    \"daily-sale001\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 499925,\r\n      \"request_bytes\": 52043467,\r\n      \"responses\": 499925,\r\n      \"response_bytes\": 2109629,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale002\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 425855,\r\n      \"request_bytes\": 44208662,\r\n      \"responses\": 425855,\r\n      \"response_bytes\": 1791852,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale003\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 516402,\r\n      \"request_bytes\": 53579106,\r\n      \"responses\": 516402,\r\n      \"response_bytes\": 2170560,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale004\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 571640,\r\n      \"request_bytes\": 59475059,\r\n      \"responses\": 571640,\r\n      \"response_bytes\": 2405288,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale005\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 488641,\r\n      \"request_bytes\": 50937149,\r\n      \"responses\": 488641,\r\n      \"response_bytes\": 2060132,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale006\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 595062,\r\n      \"request_bytes\": 61596411,\r\n      \"responses\": 595062,\r\n      \"response_bytes\": 2514791,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale007\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 539622,\r\n      \"request_bytes\": 56245033,\r\n      \"responses\": 539622,\r\n      \"response_bytes\": 2274943,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale008\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 450732,\r\n      \"request_bytes\": 47025684,\r\n      \"responses\": 450732,\r\n      \"response_bytes\": 1898874,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale009\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 461595,\r\n      \"request_bytes\": 47897394,\r\n      \"responses\": 461595,\r\n      \"response_bytes\": 1938910,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale010\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 502702,\r\n      \"request_bytes\": 52229759,\r\n      \"responses\": 502702,\r\n      \"response_bytes\": 2124118,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale011\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 489467,\r\n      \"request_bytes\": 50950224,\r\n      \"responses\": 489467,\r\n      \"response_bytes\": 2060700,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale012\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 510860,\r\n      \"request_bytes\": 53141344,\r\n      \"responses\": 510860,\r\n      \"response_bytes\": 2145320,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale013\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 446362,\r\n      \"request_bytes\": 46440311,\r\n      \"responses\": 446362,\r\n      \"response_bytes\": 1872555,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale014\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 427993,\r\n      \"request_bytes\": 44477521,\r\n      \"responses\": 427993,\r\n      \"response_bytes\": 1803340,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale015\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 508857,\r\n      \"request_bytes\": 52884727,\r\n      \"responses\": 508857,\r\n      \"response_bytes\": 2141577,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    },\r\n    \"daily-sale016\": {\r\n      \"server_eof\": 0,\r\n      \"server_err\": 0,\r\n      \"server_timedout\": 0,\r\n      \"server_connections\": 1,\r\n      \"server_ejected_at\": 0,\r\n      \"requests\": 564176,\r\n      \"request_bytes\": 58592099,\r\n      \"responses\": 564176,\r\n      \"response_bytes\": 2376014,\r\n      \"in_queue\": 0,\r\n      \"in_queue_bytes\": 0,\r\n      \"out_queue\": 0,\r\n      \"out_queue_bytes\": 0\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n</details>\r\n\r\n## Connecting\r\n\r\nAnd similarly trying to establish a connection to twemproxy hangs indefinitely...\r\n\r\n```\r\n> telnet 10.100.18.248 22139\r\nTrying 10.100.18.248...\r\n```\r\n\r\nAny and all help is appreciated. Thank you!", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/514/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jennyfountain": {"issues": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/513", "title": "Questions about twemproxy and performance", "body": "We are seeing a few issues that I was hoping someone could help me resolve or point me in the right direction.\r\n\r\n1.  During high loads, we are seeing a lot of backup in the out_queue_bytes.  On normal traffic loads, this is 0. \r\n\r\nExample (sometimes goes into 2k/3k range as well):\r\n    \"out_queue_bytes\": 33\r\n    \"out_queue_bytes\": 91\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 29\r\n    \"out_queue_bytes\": 174\r\n\r\nIn addition, it shows that our time spent in memcache goes up from 400 ms to 1000-2000 ms. This seriously affects our application. \r\n\r\n2.  auto eject also seems to not work as expected. Server goes down and our app freaks out - saying it cannot access a memcache server.\r\n\r\nhere is an example of a config:\r\n\r\nweb:\r\n  listen: /var/run/nutcracker/web.sock 0777\r\n  auto_eject_hosts: true\r\n  distribution: ketama\r\n  hash: one_at_a_time\r\n  backlog: 65536\r\n  server_connections: 16\r\n  server_failure_limit: 3\r\n  server_retry_timeout: 30000\r\n  servers:\r\n   - 1.2.3.4:11211:1\r\n   - 1.2.3.5:11211:1\r\n   - 1.2.3.6:11211:1\r\n  timeout: 2000\r\n\r\nsomaxconn = 128\r\n\r\nWhat we tried and didn't help.\r\n1.  mbuf to 512\r\n2.  server connection from 1 to 200\r\n\r\nThank you for any guidance on this problem.", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/513/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "manjuraj": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/34eb60fb977da9aa8bcac784dee1e7fb04c79d47", "message": "Merge pull request #517 from takayamaki/fix_typo\n\nfix typo in README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/0f9e1baf06db478065ba43402b975c57a567d00f", "message": "Merge pull request #492 from kalifg/patch-1\n\nFix typo in notes/memcache.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/6fd922039bc696e07b7ef18946732df6b7cf2a5d", "message": "Merge pull request #493 from dennismartensson/patch-1\n\nUpdate README.md"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/017d44503344ecdf2439747476556f4b9fee3e21", "message": "Merge pull request #494 from mortonfox/patch-1\n\nUpdate the sensu-metrics link"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/e5739338ddf228b3b900aa116646cb9654fa5f65", "message": "Merge pull request #489 from rohitpaulk/update-redis-docs\n\nUpdate redis docs for PING and QUIT"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/74af2fb2d5d3e214d8c0741a4b0ebb7d93572fc8", "message": "Merge pull request #439 from Krinkle/patch-1\n\nreadme: Link to HTTPS for wikimedia.org"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/9a2611e5992e65e6c15cd23bce06ffed3901f4f8", "message": "Merge pull request #425 from charsyam/feature/typos\n\nfix typos"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/619b55b6a1dcd21ffe16f312b52cd576c6a4caa0", "message": "Merge pull request #421 from charsyam/feature/upgrade-redis-lib\n\nupgrade redis-py version from 2.9.0 to 2.10.3"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/592c1b6b157721f66789bca6a3d463f32e6c59fc", "message": "Merge pull request #420 from esindril/master\n\nAdd script to build SRPM and fix spec file"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/defda674cdfb94fef31bdf8cc7a1dd766ea0a05a", "message": "Merge pull request #418 from twitter/revert-406-bugfix/redis-error-response-parser\n\nRevert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/65bb2ac5bbf0acce3fa52469752046c388e56ef5", "message": "Revert \"Fix parsing bug when error body contains no spaces\""}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/34f369ff91076b5d1b1229eca51bc3a2ed8f6d4a", "message": "Merge pull request #406 from tom-dalton-fanduel/bugfix/redis-error-response-parser\n\nFix parsing bug when error body contains no spaces"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954", "body": "I would be willing to take patches for kqueue support in twemproxy\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887954/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968", "body": "Thank you for the patches. I will merge this in the next version\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/5887968/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668", "body": "twemproxy does not do replication. If you are doing replication, you would have to do it on the client side\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6456668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106", "body": "> 1. Before you started work on twemproxy, did you consider using moxi: https://github.com/steveyen/moxi? If yes, why did you reject moxi?\n>    with nutcracker we have:  \n>    a) protocol pipelining which works really well for us, as we wanted to introduce minimal latency degration with a proxy sitting between a client and server.\n>    b) mbuf, which essentially enables zero copy when moving data from client to server (and vice versa)\n>    c) observability which was fairly important to us in our production environment\n> 2. Are you planning to support binary protocol in addition to ascii?\n>    no (see 'thoughts' section in notes/memcache.txt)\n> 3. Do you see any need for multi-threaded support? Moxi supports both single and multi-threaded configurations.\n>    no; if a run proxy of proxy (client --> proxy --> (proxy)+ --> server) you can actually make use of all cores and would probably be network bound in this scenario\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6798106/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681", "body": "This is a good idea.\n\nThe start up time of twemproxy is really small, especially when preconnect is not set. So, if your clients had retries built into it, then you can trivially propagate configuration changes by doing a rolling restarts of twemprox'ies. This is a good enough solution, imo\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6864681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486", "body": "> > Would you be open to a patch for this?\n> > I am always open to accepting patches :) \n\nI just think that reloading of configuration file on-the-fly is tricky to get right.\n\n> >  The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n\nI am curious as how this is happening. I set SO_REUSEADDR address option on the twemcache's listening socket, which means that we would reuse ports even if they are busy (See: proxy_reuse() function in nc_proxy.c)\n\nCould you paste me the log file dump with the error that you seeing? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606", "body": "Thanks for the patch using libevent API. \n\nI am willing to accept patches if they don't use libevent but directly call bsd's kqueue API. All you might have to do is to implement abstraction in the event interfaces, which would be in nc_event.[ch]. Let if know if you have any questions. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7373606/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377", "body": "you can always change the ulimit of the shell from which you launch nutcracker\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7387377/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857", "body": "blake the patch looks great. One thing I realized is that memcache has some item header overhead and slab header overhead even for the largest item. So, even if you configure memcache with 1MB slab, the largest item that can be stored in the slab is < 1MB. Furthermore the item size not only includes the value length, but also key length. \n\nGiven this, do you think we should have two extra keys in yml configuration\nitem_max_kvlen: (maximum key + value length)\nitem_overhead:\n\nand we discard requests whose key + value length + overhead > item_max_kvlen\n\nthoughts?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448857/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944", "body": "the reason I don't use libevent api is because I wanted to build nutcracker without any dependency on 3rd party libraries. furthermore, I use ET semantics, which I believe is not available in 1.4 version of libevent.\n\ncould i get a pull request of your changes?\n\nI also looked at your changes. I think you might want to do few cleanups before submitting a pull request. Few suggestions:\na) abstract out the event interface and wire to the underlying event call using a function pointer\nb) maybe create event/ directory that contains event/nc_epoll.c and event/nc_kqueue.c files. \nc) follow style conventions outlined in: https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7448944/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133", "body": "> SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nIs there a test case that would reproduce this scenario? I am unable to reproduce this scenario, even if I bombard twemproxy with a constant stream of traffic and restart it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361", "body": "blake, I wonder if this issue would go away if we set l_linger = 0 by calling nc_set_linger(sd, 0) on the listening sockets (see notes/socket.txt for details on it)? \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7456361/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080", "body": "Thanks for the test case; this is really useful\n\nI am still actively working on the redis branch. hopefully we should have a stable build out by end of this month\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7733080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159", "body": "should be straight forward if aws conforms to memcache ascii protocol\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7792159/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813", "body": "is there a reason why you would prefer inline command over unified command besides the obvious ease of issuing such command from telenet;\n\njust supporting unified protocol makes parsing for req / rsp easy and simple in twemproxy.\n\nif you would like to contribute, you could look at nc_parse.[ch]\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/8287813/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651", "body": "twemproxy should just work with kestrel. I guess you might be well off using \"distribution: random\" instead of default of \"distribution: ketama\" for a pool of kestrel severs\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315651/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677", "body": "apologies @jsholmes ; I am still working on this\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9315677/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/1694665", "body": "I am still actively testing this branch...I should have a stable build out in few weeks.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/1694665/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829605", "body": "hmm...this is a wrong commit from my end. ignore this. apologies\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829605/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829681", "body": "I chose the verbose way because the log message prints the line number which helps in deciphering the condition that was triggered\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829681/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536", "body": "No need for a py script. You can generate the strings using macro magic called strigificaion :) This way your code never gets outdate\n\nSee this for reference -- \n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L23-L49\n- https://github.com/twitter/twemproxy/blob/master/src%2Fnc_stats.h#L117-L122 \n- http://gcc.gnu.org/onlinedocs/cpp/Stringification.html\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899536/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553", "body": "s/char */uint_8 */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558", "body": "usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899558/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586", "body": "space between the closing paren and curly brace. So `log_loggable(LOG_NOTICE) {`\nHere are the coding guidelines -- https://github.com/twitter/twemproxy/blob/master/notes/c-styleguide.txt. Would really appreciate if you follow it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899586/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603", "body": "just call it req_log()\n\nalso format is `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899603/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612", "body": "formatting -- `/* a fragment */`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899612/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643", "body": "formatting - `struct msg *req`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899643/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649", "body": "formatting - `if (rsp) {`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899649/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660", "body": "call this `int_64_t start_ts /\\* request start timestamp in usec */\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13899660/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410", "body": "Rename\ntotal_connections to ntotal_conn\ncurr_connections to nconn\ncurr_client_connections to nclient_conn\n\ncurr_connections should be int32_t\ncurr_client_connections should be int32_t\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436410/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414", "body": "how did we come up with RESERVED_FDS number as 32\nrename it as NC_NUM_RESERVED_FD\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436414/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416", "body": "I think we might still hit this scenario - One instance where I see this happening is that you close the connection() from the code (decrement the counters), but the sockets are still in TIME_WAIT state and hence are not available for accept() sys call.\n\nSo, instead of panic, we should log() and return NC_ERROR and not set p->recv_ready to 0\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/15436416/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121", "body": "what is this for?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326121/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123", "body": "why did we bump this up?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326123/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127", "body": "[formatting] `status: %d`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326127/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172", "body": "is get_mbuf() function required? It seems to be called only in msg_make_reply(). Prefer not having this function\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326172/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178", "body": "why are these commented out? Isn't it required to trigger the out event?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326178/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216", "body": "Should we move the lines 482 - 495 into server_pool_idx()? It seems that only place where the tag logic is not used is in server_pool_server(), which is surprising, because I think that tag logic should also apply there equally.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326216/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257", "body": "In this piece of code, you allocate message structs for every server in the pool - ncontinuum messages, even if incoming messages are destined to < ncontinuum servers.\n\nOnly at line 818-819, do you some of these allocated message structs. Can we refactor this differently - maybe two loops where the first loop aggregates all the keys destined per server and the second loop has the logic\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14326257/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "COLLABORATOR"}]}, "mortonfox": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/fe68175e0200e3c2589139438ff3efa392042aa6", "message": "Update the sensu-metrics link"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dennismartensson": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/80ef6a7444fd5ae97fcab9606c1abedc19f00824", "message": "Update README.md\n\nAdded Greta to the list of companys"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kalifg": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b87ba1abfe6a814999279e69af7ce07ba0ff6c68", "message": "Fix typo"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rohitpaulk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/eed195341a02fa688b0dfb784d120f337f15a454", "message": "Update redis docs for PING and QUIT\n\nSupport for these was introduced in\n@4175419288ef66d95e082cfa2124e77fe6d4fe6d."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "andyqzb": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/330f43a430261aa48d4063771ed70fe191177154", "message": "Merge pull request #486 from deep011/deep011-patch-1\n\nfix a memory leak bug for mset command"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/ced2044980e4b9dd0c91b25fc64ce127879a1491", "message": "Merge pull request #484 from postwait/patch-1\n\nFix typo circunous -> circonus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "deep011": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/558e0d40ad79f423c4784565648e6c83cf035777", "message": "fix a memory leak bug for mset command"}], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/462", "title": "Fix a bug for msg->mlen", "body": "Function msg_append() already added the dst->mlen.\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10512396", "body": " I think it would be better to support \u201cdelete key 0\\r\\n\u201d command, because memcached support this command. What is your opinion?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10512396/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "postwait": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1e078e9e9d97560825ae4f1245177a0af29e3c82", "message": "Fix typo circunous -> circonus"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Krinkle": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/91a68d3c42638eb8178001f4d67d2606dcd80f51", "message": "readme: Link to HTTPS for wikimedia.org"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "esindril": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/51c5228acdd8a324a43107330f6b936de028dc0c", "message": "Fix spec file to work for RHEL >= 6 and wrong changelog date"}, {"url": "https://api.github.com/repos/twitter/twemproxy/commits/205323f87deb0f0004963717f2d7a80eed8e9c3b", "message": "Add script to build the SRPM package for RPM-based distributions"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "caniszczyk": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/0e8708be4d365163a1061c2a89ecc344e21203d6", "message": "Add Uber to the list of Adopters\n\nhttp://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html"}], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128", "body": "Any updates on this patch @manjuraj ?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426128/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280", "body": "Just to note that this is somewhat of a hack, but it works for now. I didn't find any tests to run via gtest or whatever harness.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6426280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "idning": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/1f4e0dae62baa379209e4cdfe64f2224dadce632", "message": "Merge pull request #410 from vincentve/master\n\noptimize performance when single key del"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350", "body": "if current msg is a sub-msg, the `msg->frag_owner` may be freed, and reset by `_msg_get`, this is not correct, especially for `req_error`\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13062350/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378", "body": "macro create clean code but break cscope/ctags :( \n\nclean code is more important, I will have a try.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/14004378/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "invalid-email-address": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/b6ac1bbc34aa0f470b55893090c7cf6a696ec184", "message": "optimize performance when single key del"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "tom-dalton-fanduel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/twitter/twemproxy/commits/6f32a928b4830d218fef67aa5d22bc0fd44000f6", "message": "Add testcase"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "TysonAndre": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/545", "title": "Always initialize file permissions field for unix domain socket", "body": "It seems like field->perm might be uninitialized memory\r\ndepending on how it is allocated.\r\nThe intended behavior is to only change file permissions from the default if a permission was specified in the config:\r\nhttps://github.com/twitter/twemproxy/pull/311/files#diff-f74ea9da930e79a9573455a0cbe4785d\r\n\r\nI ran into an issue where different sockets had different file\r\npermissions, and some of those sockets weren't readable by the user\r\nwhich created it. (I specified *only* the path to the unix domain socket)\r\n\r\nThis behavior probably started in\r\nhttps://github.com/twitter/twemproxy/pull/311/files", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "phamhongviet": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/544", "title": "Add systemd service file", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "essanpupil": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/541", "title": "fix list indentation in README", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "idirouhab": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/536", "title": "Add Foodora as company who uses it in prod", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pataquets": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/524", "title": "Add Docker support", "body": "Add Dockerfile to enable image building.\r\nUsing the official GCC image, latest tag. More info at https://hub.docker.com/_/gcc/\r\n\r\nJust adding files, setting working dir and running make instructions.\r\n\r\nBuild:\r\n\r\n```\r\n$ docker build -t twemproxy .\r\n```\r\n\r\nRun:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro twemproxy -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nFYI, there's a still quicker to test, already built image on my Docker Hub. Test it by running:\r\n\r\n```\r\n$ docker run --rm -it -p [ext_port1:port1] -p [ext_port2:port2] -p [...] -v [your_host_config_yml]:/etc/nutcracker.yml:ro pataquets/twemproxy  -c /etc/nutcracker.yml --verbose=6 [other options...]\r\n```\r\n\r\nUsing `--rm` instead of `-d` makes the container not go background and it to be deleted after stop. Should stop by `CTRL+C`'ing it.\r\nIn order for the Docker container to connect to external memcached or Redis instances, either them should be contactable as external IPs or hosts or be linked to other previously run Docker containers via Docker's ```--link``` option.\r\n\r\nHere it is an example Docker Compose file I'm using (I can submit it with the PR also if you find it useful):\r\n```\r\nredis1:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63791:6379\r\n\r\nredis2:\r\n  image: redis:3.2\r\n  command: --save \"\" --maxmemory 128mb\r\n  ports:\r\n    - 63792:6379\r\n\r\ntwemproxy:\r\n  image: pataquets/twemproxy\r\n  command: -c /etc/nutcracker.yml --verbose=6\r\n  links:\r\n    - redis1\r\n    - redis2\r\n  ports:\r\n    - 6379:6379\r\n    - 22222:22222\r\n  volumes:\r\n    - ./conf/nutcracker.redis.yml:/etc/nutcracker.yml:ro\r\n```\r\nNotice that it is tuned for two Redis instances (yml file not included, mount yours)\r\n\r\nOptional improvement to come (maybe in another issue):\r\n- Create an 'official', based on your repo, automated build at Docker Hub for the image: https://docs.docker.com/docker-hub/builds/ . Just requires a free Docker Hub account and a following a quick 'Create automated build' process. I'll be happy to help on it, if needed.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "galusben": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/520", "title": "JFrog is using twemproxy on production", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "santoshsahoo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/515", "title": "Added CircleHD to companies using Twemproxy", "body": "Updated README.md, we are using of twemproxy on aws.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "voetsjoeba": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/511", "title": "Fix build failure with --disable-stats", "body": "A build with ./configure --disable-stats currently fails with the following error (gcc 4.8.5 on CentOS 6):\r\n\r\n```\r\nnc_server.c: In function \u2018server_failure\u2019:\r\nnc_server.c:291:5: warning: implicit declaration of function \u2018stats_server_set_ts\u2019 [-Wimplicit-function-declaration]\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n     ^\r\nnc_server.c:291:38: error: \u2018server_ejected_at\u2019 undeclared (first use in this function)\r\n     stats_server_set_ts(ctx, server, server_ejected_at, now);\r\n                                      ^\r\nnc_server.c:291:38: note: each undeclared identifier is reported only once for each function it appears in\r\n```\r\n\r\nThe reason appears to be that some functions are not being nopped out in `nc_stats.h` when `NC_STATS` is 0. This change adds the missing entries. Technically only `stats_server_set_ts` is needed to fix the build error, but from the intent of the code it's clear that `stats_pool_set_ts` should also be nopped out, despite never being called directly.", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/510", "title": "Allow negative exptime values in memcached storage commands", "body": "Memcached allows negative exptime values in storage commands to immediately expire the stored values, but this wasn't very well documented until recently (here's the commit that adds the documentation from May 2016: https://github.com/memcached/memcached/commit/e7d4521cd8b27f7ebc6e4c1b9aee9eb3544f6af5).\r\n\r\nThis patch allows the memcached parser to support negative exptime values in storage commands. The server still responds with the usual STORED if the exptime is negative, so no additional response handling is needed.\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ugurengin": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/502", "title": "update memcache populate script", "body": "update memcache populate script to improve test cause of memcached for get and set operations", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "mahdi-hdi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/497", "title": "Added Geo Functionality", "body": "New Redis functionality for geo date type now supported:\n\n```\nGEOADD\nGEODIST\nGEOHASH\nGEOPOS\nGEORADIUS\nGEORADIUSBYMEMBER\n```\n\n**Mutli bulk array** was implemented in Redis response parser to support response of **GEORADIUS**. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "willfitch": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/480", "title": "Implement ERROR protocol", "body": "Rather than severing connections upon an invalid event, this patch adds the ERROR portion of the Memcached protocol.  This does not, however, add CLIENT_ERROR or SERVER_ERROR.  That can be discussed.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "VishalRocks": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/476", "title": "Update README.md", "body": "Added Codechef in Companies using Twemproxy in Production\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "artursitarski": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/474", "title": "Wikia as twemproxy user", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "flygoast": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/463", "title": "Implemented client connections limiting of server pool.", "body": "Implemented client connections limiting of server pool.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "huachaohuang": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/461", "title": "Fix memory leak for redis mset.", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charsyam": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/458", "title": "Fix Bug: redis_auth don't work with redis_db", "body": "currently, twemproxy ignore \"redis_db\" when \"redis_auth\" is set.\nselect command is set as noforward because of auth.\n\nthis patch fix this.\nand move add_auth to post_connected.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/428", "title": "rebase heartbeat branch ", "body": "It had a bug when redis is loading.\nbut after patch of https://github.com/twitter/twemproxy/commit/ef453130e321974b332dd99d585ae7285eee4b5d\n(handle loading state as error)\nI think it fixed.\n\nIn my tests. this works fine :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/423", "title": "Fix parsing bug when error body contains no spaces", "body": "This is only copy for https://github.com/twitter/twemproxy/pull/406 (by @tom-dalton-fanduel )\nfor testing. I just PR again.\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/395", "title": "support nested multibulk reply", "body": "@manjuraj This is part of https://github.com/twitter/twemproxy/pull/393\n\nfirst. I divided it that is only supporting nested multi bulk reply.\n\nafter merging this :)\n\nI will PR second of them :)\n\nThanks for your reivew :)\n", "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/393", "title": "support geo commands and nested multibulk reply.", "body": "1. support GEO Commands\n   -> geoadd, geohash, geodist, georadius, georadiusbymember\n2. support nested multibulk reply\n   -> geo commands return nested multibulk reply\n\ncurrently we only support this type of multibulk\n\n```\n                 * - mulit-bulk\n                 *    - cursor\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n```\n\nbut georadius and georadiusbymember returns maximum 3 depth multibulk. so this patch supports this kind of multibulk also.\n\n```\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - val2\n                 *       - val3\n                 *\n                 * - mulit-bulk\n                 *    - multi-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n                 *    - mulit-bulk\n                 *       - val1\n                 *       - multi-bulk\n                 *          - val2\n                 *          - val3\n```\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031155", "body": "@BrandonBrowning Could you give me a script to test this? Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031155/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10032839", "body": "@BrandonBrowning Thank you. I found the reason. and I will fix it soon. Thank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10032839/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10033615", "body": "@BrandonBrowning could you try this version?\nhttps://github.com/charsyam/twemproxy/tree/feature/issue-323\nThank you.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10033615/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572", "body": "How was just initializing with CONF_DEFAULT_SELECT?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991572/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575", "body": "I just think that add new conf file for this patch like this: nutcracker.select.yml\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991575/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579", "body": "in twemproxy, it is better to change the variable name to is_select_msg\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991579/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597", "body": "and conn->sd is nonblocking socket. so write can be failed.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991597/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "ofirule": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/435", "title": "Added support for listening to same port from multiple twemproxy processes", "body": "My team (from IBM Security Trusteer) has ran into some bottlenecks when using twemproxy and developed this patch.\n\nThis patch allows to use socket option SO_REUSEPORT on twemproxy socket. This option is available on Linux kernel version 3.9+.\n\nAfter stress testing our redis-server we found that handling many clients consumes too much processing from redis but not redis code itself rather its networking code (epoll/etc.).\n\nIt seems that the single thread nature of redis maxes out the networking part with one core in extreme conditions (like many connections scenario, no pipelining, etc.).\n\nUsing this patch we can connect many twemproxy instances (which runs on the same computer) to a redis server using the same socket, we were able to get a much greater throughput and still have one logical redis server. \n\nThese twemproxy instances can all be configured with the same configuration file, and only needs to have a different stats port. (we don't enable SO_REUSEPORT for that socket)\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rosmo": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/431", "title": "Allow ignoring SELECT from client", "body": "Even if you are just using database 0, some developers want to call Redis' SELECT command always up front. This add a new redis_ignore_select option to pools to simply return OK for any SELECT command.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "umegaya": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/424", "title": "add support of script load command by broadcast", "body": "refs #68 \nthis pull request aims to add support for SCRIPT LOAD command, by following fix.\n- define msg type for SCRIPT LOAD and parse correctly (8158d2e)\n- able to give 'broadcast' attribute to certain kind of msg type, and give broadcast attribute to SCRIPT command (8158d2e)\n- do broadcast correctly (cb50953)\n\nalso original python test seems to be broken, add new test by shell script (dfba9f8), you can run new test like following:\n\n```\ncd tests/test_redis_sh\n./run ./test_script_load.sh\n```\n\nit is only passed limited test case, and I'm very new to twemproxy, so not enough confidence about correctness. can you review and if it looks good, merge to master? thanks in advance.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dec5e": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/412", "title": "Removing duplicate Hootsuite mention in list of companies using Twemproxy", "body": "Hootsuite was added twice: in #291 and #387.\n\nPS. Maybe it's good to sort companies list alphabetically to prevent such duplication in future. I can do it if you agree.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ideal": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/411", "title": "add rbtree_entry to fetch a struct pointer from a rbtree node pointer", "body": "So the code is more generic.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "susman": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/400", "title": "add rhel7 compatibility", "body": "", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ton31337": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/396", "title": "Add SO_REUSEPORT support to socket", "body": "Add SO_REUSEPORT support to socket. Introduce new feature for Twemproxy running on newer kernels. With this feature you are able to do upgrades, restarts without any downtime. Kernel does load balancing between processes with the same host:port pairs. \n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nanzhushan": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/391", "title": "add", "body": "\u4e2d\u6587\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "travisbot": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1743714) (merged bd16a4cc into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6685567/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/1744434) (merged 12b9c896 into 6a426fd1).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/6686512/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003216) (merged 9a17a01d into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407742/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765", "body": "This pull request [passes](http://travis-ci.org/twitter/twemproxy/builds/2003218) (merged f34559a3 into b85ac82c).\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407765/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "bmatheny": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865", "body": "Would you be open to a patch for this? The problem we have is that if the server is busy enough the ports for twem may become used in the short restart window, causing twem to not start. In that case we have to shutdown apache, restart twem, start apache.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7306865/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063", "body": "SO_REUSEADDR will only reuse a TIME_WAIT socket, not an ESTABLISHED socket. If twem is running on a busy web server (as a local proxy), and the port gets used, reuseaddr is no help.\n\nI agree the on the fly reload is tricky, I'll put some thought into it.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7375063/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782", "body": "One additional comment. Based on feedback from Manju I moved the old vlen values to vlen_rem, and made vlen be an immutable value representing the total size of the value. This allows a much cleaner calculation in the req_filter of whether the object value exceeds the configured item_size_max or not.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7407782/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "Ayutthaya": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117", "body": "As a school project, I've made a patch to support kqueue, epoll and event ports, using the libevent API. There are also a few additional changes to make it work on my mac os x. (the patch is at github.com/ayutthaya/twemproxy ). I'm ready to improve my work if necessary, so don't hesitate to give feedback. Regards.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7364117/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583", "body": "I changed the patch. Now it uses kqueue API directly (but doesn't support event ports anymore). It needed changes mainly to nc_event.[ch] and nc_stats.[ch], since stats aggregation was also based on epoll. The patch is still at github.com/ayutthaya/twemproxy . one question: what is the reason for not using libevent API ? Thanks. \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7409583/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593", "body": "I've tried to comply with the suggestions above as much as possible. In particular, I've abstracted out the event interface and used a function pointer to make all #ifdef HAVE_EPOLL/KQUEUE disappear in all files except nc_event.h nc_epoll.c and nc_kqueue.c. Don't hesitate if you think it needs further changes before doing a pull request. \nI also have a question about twemproxy: what is the number of sockets from which point one should use twemproxy / the performance of the cache server starts degrading seriously due to per-connection overhead? 65k? 200k? If you could simply give me a lead, it would help me a lot for my project. Thanks !\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/7638593/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "jsholmes": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243", "body": "Any update on that stable build?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/issues/comments/9115243/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "mezzatto": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/1693973", "body": "This is awesome! Any known issues?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/1693973/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nitper": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/3668996", "body": "@manjuraj just wanted to let you know that we are fully using twemproxy with redis at bright.com and are loving it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/3668996/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "1125449708": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/4628152", "body": "Fantastic.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/4628152/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dominis": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/5512362", "body": "lol\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/5512362/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "GOPALYADAV": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924236", "body": "# hash_tag\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924236/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924245", "body": "+language:6\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/6924245/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "BrandonBrowning": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/10019622", "body": "I seem to have gotten a similar error with the latest code?\n\n```\n[2015-03-03 19:01:55.777] nc_redis.c:2076 parsed bad rsp 2 res 1 type 133 state 11\n00000000  2a 32 0d 0a 24 32 0d 0a  72 30 0d 0a 2b 4f 4b 0d   |*2..$2..r0..+OK.|\n00000010  0a                                                 |.|\n[2015-03-03 19:01:55.777] nc_core.c:198 recv on s 8 failed: Invalid argument\n```\n\nExpected\n\n```\n1) \"r0\"\n2) OK\n```\n\nCaused by running\n\n```\nevalsha 370ca495ee43ef26fae6c1d4dfa16baac375026d 1 a set 5\n```\n\n(it's a wrapper around normal command execution)\n(reference #108)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10019622/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031992", "body": "Give twemproxy a single redis instance (only for simplicity; works with multiple obviously)\nBash variables represent ports\n\n```\n$ redis-cli -p $nutcracker set _name r0\n$ redis-cli -p $nutcracker eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a get\n1) \"r0\"\n2) (nil)\n\n# looking good\n\n$ redis-cli -p $nutcracker eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a set 5\n(error) ERR Invalid argument\n\n# oh no\n\n$ redis-cli -p $redis eval \"local command = table.remove(ARGV, 1); table.insert(ARGV, 1, KEYS[1]); return { redis.call('get', '_name'), redis.call(command, unpack(ARGV)) }\" 1 a set 5\n1) \"r0\"\n2) OK\n\n# works directly on client\n```\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10031992/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10071652", "body": "Repro case is fixed!  Thanks :)\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/10071652/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "vlm": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829590", "body": "Does it make sense to add CONN_KIND_AS_STRING(conn) instead of every `\"s` though?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829590/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829594", "body": "What's the reason for this commit?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11829594/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11831080", "body": "Ack.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/comments/11831080/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Niteesh": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739", "body": "if cp->select  is initlized by CONF_DEFAULT_SELECT, the function conf_set_num will throw error that select is duplicate,\nthus to avoid writing very similar function  and  let conf_set_num work fine, this is done, later if cp->select remains unset default value is  assigned in conf_validate_pool\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991739/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748", "body": "i made sure that when i call this funtion, connection is connected.\neven than i should  have kept it in loop to try more than once if EAGAIN error is encountered\n\nor Ill appritiate  your comments if there is a better way to do it\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3991748/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055", "body": "```\nvoid  setSelectDb(struct conn *conn,struct server *server){\n     ASSERT(!conn->client && conn->connected )\n     int n, i;\n     char selectCommand[25];\n     sprintf(selectCommand,\"*2\\r\\n$6\\r\\nSELECT\\r\\n$1\\r\\n%d\\r\\n\",server->owner->select);\n     n = write(conn->sd,selectCommand,strlen(selectCommand));\n if (n < 0) {\n     log_error(\"ERROR selecting db on  socket for socket %d-> error %d \", conn->sd, strerror(errno) );\n     conn->err = errno;\n }\n```\n\nI am really not sure about this  so i want to ask you if this will do the needful        \n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/3992055/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "jdi-tagged": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780", "body": "This does the opposite of what the comment above says; with this change having unique server names doesn't help if the pnames match.\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/6936780/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "allenlz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133", "body": "All fragements will be freed at the same time if some error happens (`rsp_make_error`). If there is no error, they will also be freed at the same time.\nIf I can't believe `frag_owner` is valid, which you think it may be a wild pointer, that will be a bug of twemproxy upstream.\nWill it happen?\n", "reactions": {"url": "https://api.github.com/repos/twitter/twemproxy/pulls/comments/13274133/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}}}}