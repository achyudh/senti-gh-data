{"_default": {"1": {"xunto": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4491", "title": "[BUG] ready() always returns False for groups", "body": "## Steps to reproduce\r\n\r\n```python\r\nsent = group(\r\n    celery_app.signature('task1', args=arg),\r\n    celery_app.signature('task2', args=arg),\r\n    celery_app.signature('task3', args=arg)\r\n).apply_async()\r\n\r\nwhile not sent.ready():\r\n    pass\r\n\r\nprint(\"test\")\r\n```\r\n\r\n## Expected behavior\r\nI expect ```ready()``` to return ```True``` when all tasks are finished.\r\n\r\n## Actual behavior\r\nLooks like this code will never finish execution as ```ready()``` always return ```False``` even if all tasks are finished. Tested on celery master branch.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nitinmeharia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4489", "title": "django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread", "body": "### Application details:\r\n```\r\n    software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.4\r\n            billiard:3.5.0.3 redis:2.10.6\r\n    platform -> system:Darwin arch:64bit imp:CPython\r\n    loader   -> celery.loaders.app.AppLoader\r\n    settings -> transport:redis results:redis://localhost:6379/2\r\n```\r\n\r\n### Error log\r\n```\r\n    Signal handler <bound method DjangoWorkerFixup.on_worker_process_init of <celery.fixups.django.DjangoWorkerFixup object at 0x108e46fd0>> raised: DatabaseError(\"DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\",)\r\n    Traceback (most recent call last):\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/utils/dispatch/signal.py\", line 227, in send\r\n        response = receiver(signal=self, sender=sender, **named)\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 154, in on_worker_process_init\r\n        self._close_database()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 186, in _close_database\r\n        conn.close()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 283, in close\r\n        self.validate_thread_sharing()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 542, in validate_thread_sharing\r\n        % (self.alias, self._thread_ident, thread.get_ident())\r\n    django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\r\n```\r\n\r\n## Steps to reproduce\r\n```\r\ncelery multi restart w1 -A proj -l info\r\n```\r\nIts a django 1.11 application where we are getting this error.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kurara": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4488", "title": "launching worker from python: error with configuration from object.", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nVersion celery: 4.1.0 (latentcall)\r\n\r\nI'm trying to launch a worker from python code. When I use the class CeleryCommand with 'worker' option, it works. But if I add the option '--detach' or 'multi' the broker configuration is wrong. The code is:\r\n\r\n```\r\napp.config_from_object(config_module)\r\ncelerycmd = CeleryCommand(app)\r\ncelerycmd.execute_from_commandline(argv=[prog_name, 'worker', 'app_name', pidfile, logfile, loglevel])\r\n```\r\nor (which is not working)\r\n\r\n`celerycmd.execute_from_commandline(argv=[prog_name, 'multi', 'start', 'app_name', pidfile, logfile, loglevel])`\r\n\r\n# Expected behavior\r\nGet the same broker as in the configuration, not the default one\r\n\r\n## Actual behavior\r\n\r\nlog_file:\r\n\r\n```\r\n[2018-01-15 15:08:58,471: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2018-01-15 15:08:58,506: INFO/MainProcess] mingle: searching for neighbors\r\n[2018-01-15 15:08:59,647: INFO/MainProcess] mingle: all alone\r\n[2018-01-15 15:08:59,672: INFO/MainProcess] app_name@hostname ready.\r\n```\r\n\r\nFYI: I'm using lower case configuration. \r\n\r\nI could provide you what the object has in debug mode. Just ask me and I post it. When I debugged, I think the 'app' had the configuration of the file when it was inside the function _execute_from_commandline_, I can't understand when it loses it.\r\n\r\nI tought that maybe the problem is that I don't provide the configuration at the begining of the file, where app is declared, but in a function. So I tested to add the broker directly when I create the app: \r\n`app = Celery('app_name', broker_url='broker@...')`\r\nbut it didn't work either.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MShekow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4486", "title": "Memory hogging in client when using RPC (with RabbitMQ)", "body": "## Description\r\nIn my scenario I have a client program that puts thousands of tasks on the queue (`generate_data.delay()`). The workers produce a result that is of considerable size (suppose each result uses 1 MB of memory). The result is pickled back, and the client processes the results in some way, whenever results are available. In other words, once `AsyncResult.ready() == True`, I `get()` the result and do something with it.\r\nUsing `objgraph` I found out that celery never releases the result data.\r\n\r\n## My configuration\r\n```\r\nSoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.2\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Windows arch:32bit, WindowsPE imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:rpc:///\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\nresult_backend: 'rpc:///'\r\nresult_serializer: 'pickle'\r\ntask_serializer: 'pickle'\r\naccept_content: ['pickle']\r\n```\r\n\r\n\r\n## Steps to reproduce\r\nWorker program simply returns large objects, e.g.:\r\n```\r\nclass Data:\r\n    def __init__(self):\r\n        self.data = b'1' * 1024 * 1024\r\n\r\n@app.task\r\ndef generate_data() -> Data:\r\n    time.sleep(random.uniform(0, 0.2))\r\n    d = Data()\r\n    return d\r\n```\r\nThe client program retrieves results whenever they are available:\r\n```\r\nresults = {}\r\n\r\nfor i in range(2000):\r\n    async_result = generate_data.delay()\r\n    results[async_result] = True\r\n\r\nlogger.info(\"Put {} jobs on process queue!\".format(len(results)))\r\n\r\nresults_collected = 0\r\nwhile True:\r\n    # get results that are ready now\r\n    ready_results = [async_result for async_result, _ in results.items() if async_result.ready()]\r\n    if not ready_results:\r\n        time.sleep(10)\r\n        continue\r\n\r\n    results_collected += len(ready_results)\r\n\r\n    logger.info(\"Processing {} results. Got {} results so far\".format(len(ready_results), results_collected))\r\n    for ready_result in ready_results:\r\n        # we don't actually use the data - a real program would process the data somehow\r\n        result_data = ready_result.get()  # type: Data\r\n        del results[ready_result]\r\n\r\n    gc.collect()\r\n\r\n    # Exit loop once all results were processed:\r\n    if not results:\r\n        break\r\nlogger.info(\"Finished collecting all results\")\r\n```\r\n\r\n## Expected behavior\r\nWhen my client no longer has a reference to neither the actual data returned by `AsyncResult.get()`, nor the `AsyncResult` object itself, the memory of the data and the `AsyncResult` should be freed by celery.\r\n\r\n## Actual behavior\r\nMemory is being hogged so quickly that my client process (32-bit) dies soon. The reason is the huge size of `MESSAGE_BUFFER_MAX` (8192) and the oddly-hardcoded `bufmaxsize` (1000) in `BufferMap`. Are there any reasons for these huge buffers?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thedrow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4483", "title": "Add test coverage for #4356", "body": "#4356 is a critical fix for a bug that occurs when messages migrate between Celery 3 and Celery 4 clusters.\r\nDue to it's severity It was merged without proper test coverage.\r\nWe need to ensure this code is covered by the appropriate unit tests.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4434", "title": "Run the integration tests in a different build stage", "body": "We need to find a way to run the integration tests in a different build stage so that the unit tests will before them.\r\nThe unit tests are much quicker to execute and thus should run first to free up build resources for other contributors.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4423", "title": "Document that tasks are now documented automatically by celery", "body": "See https://github.com/celery/celery/pull/4422", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2547666a1ea13b27bc13ef296ae43a163ecd4ab3", "message": "Don't cover this branch as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/3af6a635cf90a4452db4e87c2326579ebad750c2", "message": "Merge branch 'master' into master"}, {"url": "https://api.github.com/repos/celery/celery/commits/bd0ed23c81b20fd75c0e2188fcf78e4d74898953", "message": "Use editable installs to measure code coverage correctly."}, {"url": "https://api.github.com/repos/celery/celery/commits/0a0fc0fbf698d30e9b6be29661e0d447548cc47c", "message": "Report coverage to terminal as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/4b4bf2a4eee65871d3e9c0e96d94b23aa2ee602c", "message": "Report coverage correctly (#4445)\n\n* Report coverage correctly.\r\n\r\nAs it turns out this repository does not report coverage to codecov at all since the path to the executables has changed at some point.\r\nThis should fix the problem.\r\n\r\n* Readd code coverage badge."}, {"url": "https://api.github.com/repos/celery/celery/commits/dbd59d9fc988ad0f04ae710c32066c5ff62374ef", "message": "Added bandit to the build matrix."}, {"url": "https://api.github.com/repos/celery/celery/commits/2ae00362179897968cfbd1fc8d61b648356769a7", "message": "Added bandit to lint for security issues."}, {"url": "https://api.github.com/repos/celery/celery/commits/56b94c327244fad4933706fbddc02eeea508d21a", "message": "Prettify test output."}, {"url": "https://api.github.com/repos/celery/celery/commits/ebd98fa4d36bb8003c2f46dbd16e9888af13720f", "message": "Parallel doc lints (#4435)\n\n* Bump sphinx.\r\n\r\n* Update copyright year. Mark the celerydocs & the celery.contrib.sphinx extensions as read_parallel_safe.\r\n\r\n* Install from git for now :(\r\n\r\n* Fix flake8 errors."}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4268", "title": "Add Var.CI integration", "body": "This pull request aims to demonstrate the power of [VarCI](https://var.ci/), the missing assistant for GitHub issues.\n\n--\n_Automated response by [Var.CI](https://var.ci)_ :robot:", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3982", "title": "Added failing test cases for #3885", "body": "The tests were contributed by @robpogorzelski\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nAttempt to fix #3885 without hurting eager execution of the canvas.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PromyLOPh": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4480", "title": "Application is not thread-safe", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.4\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:rpc:///\r\n\r\nresult_backend: 'rpc:///'\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\n```\r\n\r\n## Steps to reproduce\r\n\r\nUsing code from the user manual, but two threads running concurrently:\r\n\r\n```python\r\nfrom celery import Celery\r\nimport time\r\nfrom threading import Thread\r\n\r\napp = Celery('test', broker='amqp://guest@localhost//', backend='rpc://')\r\n\r\n@app.task(bind=True)\r\ndef hello(self, a, b):\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 50})\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 90})\r\n    time.sleep(1)\r\n    return 'hello world: %i' % (a+b)\r\n\r\nif __name__ == '__main__':\r\n    def run ():\r\n        handle = hello.delay (1, 2)\r\n        print (handle.get ())\r\n    t1 = Thread (target=run)\r\n    t2 = Thread (target=run)\r\n    t1.start ()\r\n    t2.start ()\r\n    t1.join ()\r\n    t2.join ()\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\nhello world: 3\r\nhello world: 3\r\n```\r\n\r\n## Actual behavior\r\n\r\nDifferent exceptions, depending on timing. For instance:\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 456, in channel\r\n    return self.channels[channel_id]\r\nKeyError: None\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 56, in start\r\n    self._connection.default_channel, [initial_queue],\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 821, in default_channel\r\n    self._default_channel = self.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 266, in channel\r\n    chan = self.transport.create_channel(self.connection)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/transport/pyamqp.py\", line 100, in create_channel\r\n    return connection.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 459, in channel\r\n    channel.open()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 432, in open\r\n    spec.Channel.Open, 's', ('',), wait=spec.Channel.OpenOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 468, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 473, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 252, in read_frame\r\n    payload = read(size)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 417, in _read\r\n    s = recv(n - len(rbuf))\r\nsocket.timeout: timed out\r\n```\r\nor\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 59, in start\r\n    self._consumer.consume()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 477, in consume\r\n    self._basic_consume(T, no_ack=no_ack, nowait=False)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 598, in _basic_consume\r\n    no_ack=no_ack, nowait=nowait)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/entity.py\", line 737, in consume\r\n    arguments=self.consumer_arguments)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 1564, in basic_consume\r\n    wait=None if nowait else spec.Basic.ConsumeOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 471, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 476, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 254, in read_frame\r\n    'Received {0:#04x} while expecting 0xce'.format(ch))\r\namqp.exceptions.UnexpectedFrame: Received 0x3c while expecting 0xce\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kzidane": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4471", "title": "DisabledBackend when starting flask --with-threads?", "body": "I'm trying to use Celery for one of my applications and experiencing a strange behavior that I'm not sure why it's caused.\r\n\r\nTo replicate, here's a simple Flask app:\r\n\r\n    # application.py\r\n    from celery.contrib.abortable import AbortableAsyncResult\r\n    from flask import Flask\r\n    from tasks import add\r\n\r\n    app = Flask(__name__)\r\n\r\n    @app.route(\"/\")\r\n    def index():\r\n        # start the task and return its id\r\n        return add.delay(42, 50).task_id\r\n\r\n\r\n    @app.route(\"/state/<task_id>\")\r\n    def result(task_id):\r\n        # return current task state\r\n        return AbortableAsyncResult(task_id).state\r\n\r\nand here's a Celery app and a task:\r\n\r\n    # tasks.py\r\n    from celery import Celery\r\n    from celery.contrib.abortable import AbortableTask\r\n\r\n\r\n    app = Celery(\r\n        \"tasks\",\r\n        # use sqlite database as result backend (also tried rpc://)\r\n        backend=\"db+sqlite:///celerydb.sqlite\",\r\n        broker=\"pyamqp://localhost\"\r\n    )\r\n\r\n    @app.task(bind=True, base=AbortableTask)\r\n    def add(self, x, y):\r\n        return x + y\r\n\r\nRunning the Celery worker:\r\n\r\n    $ celery -A tasks worker --loglevel=info\r\n     -------------- celery@7677a80760b4 v4.1.0 (latentcall)\r\n    ---- **** ----- \r\n    --- * ***  * -- Linux-4.10.0-42-generic-x86_64-with-debian-jessie-sid 2018-01-02 20:22:48\r\n    -- * - **** --- \r\n    - ** ---------- [config]\r\n    - ** ---------- .> app:         tasks:0x7f339a52dfd0\r\n    - ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n    - ** ---------- .> results:     sqlite:///celerydb.sqlite\r\n    - *** --- * --- .> concurrency: 8 (prefork)\r\n    -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n    --- ***** ----- \r\n     -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n    [tasks]\r\n      . tasks.add\r\n\r\n    [2018-01-02 20:22:48,585: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n    [2018-01-02 20:22:48,592: INFO/MainProcess] mingle: searching for neighbors\r\n    [2018-01-02 20:22:49,608: INFO/MainProcess] mingle: all alone\r\n    [2018-01-02 20:22:49,636: INFO/MainProcess] celery@7677a80760b4 ready.\r\n\r\n\r\nRunning the Flask app:\r\n\r\n    $ FLASK_APP=application.py flask run --with-threads\r\n      * Serving Flask app \"application\"\r\n      * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\r\n\r\n\r\nHitting `/` with `curl` starts the task and returns its id without any problems:\r\n\r\n    $ curl http://localhost:5000\r\n    f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nCelery's output at this point:\r\n\r\n    [2018-01-02 20:29:28,974: INFO/MainProcess] Received task: tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef]\r\n    [2018-01-02 20:29:29,000: INFO/ForkPoolWorker-1] Task tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef] succeeded in 0.02414485500776209s: 92\r\n\r\nBut trying to get the state of the task \r\n\r\n    $ curl http://localhost:5000/state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nresults in the following error:\r\n\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n\r\neven though the backend seems to be configured per the `backend` argument to `Celery` and its output? I also tried setting `CELERY_RESULT_BACKEND` and `result_backend` using `app.conf.update`, but no luck!\r\n\r\nWhat's interesting is that this problem disappears if I drop the `--with-threads` option from the `flask run` command. Any idea why this might be caused and how to work around it if possible?\r\n\r\nAdditional details:\r\n\r\n    $ celery --version\r\n    4.1.0 (latentcall)\r\n    $ flask --version\r\n    Flask 0.12.2\r\n    Python 3.6.0 (default, Oct 30 2017, 05:46:44) \r\n    [GCC 4.8.4]\r\n\r\nFull traceback:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1982, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1614, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1517, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/_compat.py\", line 33, in reraise\r\n    raise value\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1612, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1598, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n      File \"/root/application.py\", line 14, in result\r\n    return AbortableAsyncResult(task_id).state\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 436, in state\r\n    return self._get_task_meta()['status']\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 375, in _get_task_meta\r\n    return self._maybe_set_cache(self.backend.get_task_meta(self.id))\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/backends/base.py\", line 352, in get_task_meta\r\n    meta = self._get_task_meta_for(task_id)\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n    127.0.0.1 - - [02/Jan/2018 20:46:28] \"GET /state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef HTTP/1.1\" 500 -\r\n\r\nThank you!", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fnordian": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4465", "title": "Celery.close() leaks redis connections", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.3\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\n\r\n```python\r\nimport time\r\nfrom celery import Celery\r\n\r\ndef run_celery_task(taskname):\r\n    with Celery(broker='redis://redis:6379/0', backend='redis://redis:6379/0') as celery:\r\n        res = celery.send_task(taskname)\r\n        print(res)\r\n\r\nfor i in range(0, 100):\r\n    run_celery_task(\"test\")\r\n\r\ntime.sleep(100)\r\n```\r\n\r\n```bash\r\nnetstat -tn | grep 6379 | grep ESTABLISHED | wc -l\r\n```\r\n## Expected behavior\r\n\r\nWhen the `with Celery`-block terminates, I expect all connections to redis being closed.\r\n\r\n## Actual behavior\r\n\r\nNot all connections are closed. When the for-loop finishes, there > 100 open connections to redis.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kimice": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4464", "title": "Maximum recursion depth exceeded while calling a Python object", "body": "Hi, when I call celery apply_async function, sometimes it raise Exception like this. It seems like getting config failed. This bug can't always reappear. I guess celery may be not init correctly. I'm so confused with this bug.\r\n\r\ncelery==4.1.0\r\n\r\nI init celery with flask like this.\r\n\r\n```\r\ndef make_celery(app):\r\n    celery = Celery(app.import_name, backend=app.config['CELERY_RESULT_BACKEND'],\r\n                    broker=app.config['CELERY_BROKER_URL'])\r\n    celery.conf.update(app.config)\r\n    celery.config_from_object('App.celery_custom.celery_config')\r\n    TaskBase = celery.Task\r\n    class ContextTask(TaskBase):\r\n        abstract = True\r\n        def __call__(self, *args, **kwargs):\r\n            with app.app_context():\r\n                return TaskBase.__call__(self, *args, **kwargs)\r\n    celery.Task = ContextTask\r\n    return celery\r\n\r\ncelery_app = make_celery(flask_app)\r\n\r\n@celery_app.task(bind=True)\r\ndef checkInstance(self, a, b):\r\n    pass\r\n\r\ncheck_task = checkInstance.apply_async(args=['123', '123'], queue='123')\r\n```\r\n\r\n```\r\nLOG:\r\n[2017-12-26 08:42:58,659]: logs_util.py[line:64] [pid:25680] ERROR Traceback (most recent call last):\r\n  File \"./App/views/experiment_views.py\", line 377, in create_experiment_and_run\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/task.py\", line 521, in apply_async\r\n    if app.conf.task_always_eager:\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 431, in __getitem__\r\n    return getitem(k)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 280, in __getitem__\r\n    return mapping[_key]\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/utils/objects.py\", line 44, in __get__\r\n    value = obj.__dict__[self.__name__] = self.__get(obj)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 148, in data\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 911, in _finalize_pending_conf\r\n    conf = self._conf = self._load_config()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 921, in _load_config\r\n    self.loader.config_from_object(self._config_source)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 128, in config_from_object\r\n    obj = self._smart_import(obj, imp=self.import_from_cwd)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 146, in _smart_import\r\n    return imp(path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 106, in import_from_cwd\r\n    package=package,\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/imports.py\", line 100, in import_from_cwd\r\n    with cwd_in_path():\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 84, in helper\r\n    return GeneratorContextManager(func(*args, **kwds))\r\nRuntimeError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lexabug": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4462", "title": "INFO log messages land to stderr", "body": "I've set up a basic application with Django 2.0 and Celery 4.1.0 with debug task (as it described [here](http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html#using-celery-with-django)) and one custom task in application tasks module.\r\nMy module code look like this:\r\n```\r\n# Create your tasks here\r\nfrom __future__ import absolute_import, unicode_literals\r\nfrom celery import shared_task\r\nfrom django.conf import settings\r\nfrom celery.utils.log import get_task_logger\r\n\r\nlogger = get_task_logger(__name__)\r\n\r\n@shared_task(name='validate_user_email', ignore_result=True, bind=True)\r\ndef validate_user_email(self, user_id):\r\n    logger.info('%s email verified', user_id)\r\n```\r\n\r\nWhen I redirect streams (stdout and stderr) to different logs  (info.log and error.log) info.log is silent while error.log contains log entries with levels WARNING and INFO.\r\n\r\nCommand I execute celery with is: `celery -A email_validation worker -Q validate --concurrency 5 --maxtasksperchild 100 -l info > info.log 2> error.log`\r\n\r\nCelery config:\r\n```\r\nCELERY_BROKER_URL = '******************'\r\nCELERY_BROKER_HEARTBEAT = 900\r\nCELERY_BROKER_HEARTBEAT_CHECKRATE = 15\r\nCELERY_RESULT_BACKEND = 'amqp'\r\nCELERY_WORKER_PREFETCH_MULTIPLIER = 1\r\nCELERY_WORKER_MAX_TASKS_PER_CHILD = 100\r\nCELERY_TASK_ACKS_LATE = True\r\nCELERY_ENABLE_UTC = False\r\nCELERY_TIMEZONE = 'US/Eastern'\r\nCELERY_WORKER_DISABLE_RATE_LIMITS = True\r\nCELERY_EVENT_QUEUE_TTL = 1\r\nCELERY_EVENT_QUEUE_EXPIRES = 60\r\nCELERY_RESULT_EXPIRES = 3600\r\nCELERY_TASK_IGNORE_RESULT = True\r\nCELERY_WORKER_HIJACK_ROOT_LOGGER = True\r\n```\r\n\r\nHow can make celery to post INFO log messages to stdout? ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cajbecu": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4457", "title": "Connection to broker lost. Trying to re-establish the connection: OSError: [Errno 9] Bad file descriptor", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Software\r\ncelery==4.1.0\r\nkombu==4.1.0\r\namqp==2.2.2\r\nPython 3.6.1\r\nbroker: rabbitmq 3.6.14\r\nresult backend: redis\r\n\r\n## Steps to reproduce\r\n1. celery -A proj worker -Q Q1 --autoscale=10,1 -Ofair --without-gossip --without-mingle --heartbeat-interval=60 -n Q1\r\n2. celery lost connection to broker\r\n3. after restarting affected worker the connection is successfully re-established and the worker starts processing tasks\r\n\r\n## Expected behavior\r\ncelery should re-establish connection to broker\r\n\r\n## Actual behavior\r\ncelery tries to re-establish connection to broker but fails with this error message (which is repeated every second) until manually restarted:\r\n```\r\n[user] celery.worker.consumer.consumer WARNING 2017-12-18 00:38:27,078 consumer: \r\nConnection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/loops.py\", line 47, in asynloop\r\n    obj.controller.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/worker.py\", line 217, in register_with_event_loop\r\n    description='hub.register',\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 151, in send_all\r\n    fun(parent, *args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/components.py\", line 178, in register_with_event_loop\r\n    w.pool.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/prefork.py\", line 134, in register_with_event_loop\r\n    return reg(loop)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in register_with_event_loop\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in <listcomp>\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 207, in add_reader\r\n    return self.add(fds, callback, READ | ERR, args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 158, in add\r\n    self.poller.register(fd, flags)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/utils/eventio.py\", line 67, in register\r\n    self._epoll.register(fd, events)\r\nOSError: [Errno 9] Bad file descriptor\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4457/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thiagogalesi": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4454", "title": "Celery does not consider authSource on mongodb backend URLs", "body": "Version: Celery 4.0.2 (from looking at the changes since then it seems there is no change addressing this issue here: https://github.com/celery/celery/commits/master/celery/backends/mongodb.py )\r\n\r\n(Edit) Confirmed with the following versions as well:\r\namqp==2.2.2\r\nbilliard==3.5.0.3\r\ncelery==4.1.0\r\nkombu==4.1.0\r\npymongo==3.6.0\r\n\r\nCelery Report\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.8\r\n            billiard:3.5.0.3 py-amqp:2.2.1\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\n\r\n## Steps to reproduce\r\n\r\nGive Celery a Backend URL pointing to a MongoDB instance with authentication and username/password (user/pwd set on the Admin DB by default) in the format:\r\n\r\nmongodb://user:pass@your-server/your_db?authSource=admin\r\n\r\n(Please see http://api.mongodb.com/python/current/examples/authentication.html#default-database-and-authsource and http://api.mongodb.com/python/current/api/pymongo/mongo_client.html?highlight=authsource )\r\n\r\n## Expected behavior\r\n\r\nCelery authenticates the user in the admin database (this is the same as passing --authenticationDatabase to the mongo client or the same url to MongoClient)\r\n\r\n## Actual behavior\r\n\r\nCelery tries to authenticate the user on the your_db database (failing to authenticate)\r\n\r\n## Workaround (not recommended)\r\n\r\nChange the db on the URL to /admin (this db shouldn't be used to store arbitrary data normally)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yanliguo": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4451", "title": "Celery (4.1.0) worker stops to consume new message when actives is empty  ", "body": "Hi there,\r\n   I was using celery to dispatch some long running task recently, and finding a way to disable prefetch. Now the config is:\r\n\r\nacks_late = True\r\nconcurrency = 1\r\nprefetch_multiplier = 1\r\n-Ofair\r\n\r\nActually, workers are still prefetching tasks. And I also have a monitor job to revoke tasks when a task is stuck in reserved state for a long time (let's say 5 minutes) or the task is outputing valid result.  \r\n\r\nOne thing wired is that, some workers stopped consuming new tasks when the actives is empty and the reseved task is revoked.  Has anybody ever met with this issue ? any help will be appreciated.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4450", "title": "PENDING state, what if it meant just one thing?", "body": "According to docs, PENDING state has the following meaning:\r\n\r\n> Task is waiting for execution or unknown. Any task id that\u2019s not known is implied to be in the pending state.\r\n\r\nAre there plans to make pending mean one thing only? It is quite confusing to handle a state that can mean \"waiting\" or \"lost\". ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfresh613": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4449", "title": "ValueFormatError when processing chords with couchbase result backend", "body": "Hi it seems like when I attempt to process groups of chords, the couchbase result backend is consistently failing to unlock the chord when reading from the db:\r\n\r\n`celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()`\r\n\r\nThis behavior does not occur with the redis result backend, i can switch between them and see that the error unlocking only occurs on couchbase.\r\n\r\n## Steps to reproduce\r\nAttempt to process a chord with couchbase backend using pickle serialization.\r\n\r\n## Expected behavior\r\nChords process correctly, and resulting data is fed to the next task\r\n\r\n## Actual behavior\r\nCelery is unable to unlock the chord from the result backend\r\n\r\n## Celery project info: \r\n```\r\ncelery -A ipaassteprunner report\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.10\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:couchbase://isadmin:**@localhost:8091/tasks\r\n\r\ntask_serializer: 'pickle'\r\nresult_serializer: 'pickle'\r\ndbconfig: <ipaascommon.ipaas_config.DatabaseConfig object at 0x10fbbfe10>\r\ndb_pass: u'********'\r\nIpaasConfig: <class 'ipaascommon.ipaas_config.IpaasConfig'>\r\nimports:\r\n    ('ipaassteprunner.tasks',)\r\nworker_redirect_stdouts: False\r\nDatabaseConfig: u'********'\r\ndb_port: '8091'\r\nipaas_constants: <module 'ipaascommon.ipaas_constants' from '/Library/Python/2.7/site-packages/ipaascommon/ipaas_constants.pyc'>\r\nenable_utc: True\r\ndb_user: 'isadmin'\r\ndb_host: 'localhost'\r\nresult_backend: u'couchbase://isadmin:********@localhost:8091/tasks'\r\nresult_expires: 3600\r\niconfig: <ipaascommon.ipaas_config.IpaasConfig object at 0x10fbbfd90>\r\nbroker_url: u'amqp://guest:********@localhost:5672//'\r\ntask_bucket: 'tasks'\r\naccept_content: ['pickle']\r\n```\r\n### Additional Debug output\r\n```\r\n[2017-12-13 15:39:57,860: INFO/MainProcess] Received task: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2]  ETA:[2017-12-13 20:39:58.853535+00:00] \r\n[2017-12-13 15:39:57,861: DEBUG/MainProcess] basic.qos: prefetch_count->27\r\n[2017-12-13 15:39:58,859: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x10b410b90> (args:('celery.chord_unlock', 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', {'origin': 'gen53678@silo2460', 'lang': 'py', 'task': 'celery.chord_unlock', 'group': None, 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', u'delivery_info': {u'priority': None, u'redelivered': False, u'routing_key': u'celery', u'exchange': u''}, 'expires': None, u'correlation_id': 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', 'retries': 311, 'timelimit': [None, None], 'argsrepr': \"('90c64bef-21ba-42f9-be75-fdd724375a7a', {'chord_size': 2, 'task': 'ipaassteprunner.tasks.transfer_data', 'subtask_type': None, 'kwargs': {}, 'args': (), 'options': {'chord_size': None, 'chain': [...], 'task_id': '9c6b5e1c-2089-4db7-9590-117aeaf782c7', 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', 'reply_to': '0a58093c-6fdd-3458-9a34-7d5e094ac6a8'}, 'immutable': False})\", 'eta': '2017-12-13T20:39:58.853535+00:00', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', u'reply_to':... kwargs:{})\r\n[2017-12-13 15:40:00,061: DEBUG/MainProcess] basic.qos: prefetch_count->26\r\n[2017-12-13 15:40:00,065: DEBUG/MainProcess] Task accepted: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] pid:53679\r\n[2017-12-13 15:40:00,076: INFO/ForkPoolWorker-6] Task celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()\r\n```\r\n\r\n### Stack trace from chord unlocking failure\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/builtins.py\", line 75, in unlock_chord\r\n    raise self.retry(countdown=interval, max_retries=max_retries)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/task.py\", line 689, in retry\r\n    raise ret\r\nRetry: Retry in 1s\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yutkin": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4438", "title": "Celery doesn't write RECEIVED state into MongoDB", "body": "When a number of tasks in a queue surpass a number of workers, new added tasks are not writing in a backend. In other words, I want to write task state (RECEIVED) into DB immediately after its invocation. It is possible?\r\n\r\nP.S. I'm using MongoDB as backend. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "canassa": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4426", "title": "Task is executed twice when the worker restarts", "body": "Currently using Celery 4.1.0\r\n\r\n## Steps to reproduce\r\n\r\nStart a new project using RabbitMQ and register the following task:\r\n\r\n```python\r\nfrom django.core.cache import cache\r\n\r\n@shared_task(bind=True)\r\ndef test_1(self):\r\n    if not cache.add(self.request.id, 1):\r\n        raise Exception('Duplicated task {}'.format(self.request.id))\r\n```\r\n\r\nNow start 2 workers. I used gevent with a concurrency of 25 for this test:\r\n\r\n```\r\ncelery worker -A my_proj -Q my_queue -P gevent -c 25\r\n```\r\n\r\nOpen a python shell and fire a a bunch of tasks:\r\n\r\n```python\r\nfrom myproj.tasks import test_1\r\n\r\nfor i in range(10000):\r\n    test_1.apply_async()\r\n```\r\n\r\nNow quickly do a warm shutdown (Ctrl+c) in one of the workers while it's still processing the tasks, you should see the errors popping in the second worker:\r\n\r\n```\r\nERROR    Task my_proj.tasks.test_1[e28e6760-1371-49c9-af87-d196c59375e9] raised unexpected: Exception('Duplicated task e28e6760-1371-49c9-af87-d196c59375e9',)\r\nTraceback (most recent call last):\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/code/scp/python/my_proj/tasks.py\", line 33, in test_1\r\n    raise Exception('Duplicated task {}'.format(self.request.id))\r\nException: Duplicated task e28e6760-1371-49c9-af87-d196c59375e9\r\n```\r\n\r\n## Expected behavior\r\n\r\nSince I am not using late acknowledgment and I am not killing the workers I wasn't expecting the tasks to execute again.\r\n\r\n## Actual behavior\r\n\r\nThe tasking are being executed twice, this is causing some problems in our servers because we restart our works every 15 minutes or so in order to avoid memory leaks.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kn-id": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4424", "title": "Celery Crash: Unrecoverable error when using QApplication in main process", "body": "Error happens when there's some queues already added before worker started and max task per child is 1\r\n\r\n## Checklist\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.12\r\n                         billiard:3.5.0.3 py-amqp:2.2.2\r\n      platform -> system:Linux arch:64bit, ELF imp:CPython\r\n      loader   -> celery.loaders.app.AppLoader\r\n      settings -> transport:amqp results:disabled\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n1. create instance of QApplication when main worker started\r\n```\r\n@celeryd_after_setup.connect\r\nfrom PyQt4.QtGui import QApplication\r\ndef init_worker(sender, **k):\r\n    QApplication([])\r\n```\r\n2. create task\r\n```\r\n@app.task\r\ndef job():\r\n    print 'hello'\r\n```\r\n3. add some queues (2 - 3 per thread. so if there's 4 worker child then there's 8 or more queues)\r\n\r\n4. start worker with max task per child 1\r\n```\r\ncelery worker -A proj --loglevel=INFO --max-tasks-per-child=1\r\n```\r\n\r\n## Expected behavior\r\n- run queues successfuly\r\n## Actual behavior\r\nCelery Crash after 1 queue per child\r\n```\r\n2017-12-07 10:20:19,594: INFO/MainProcess] Received task: proj.tasks.job[c6413dc2-ad62-4033-a69b-39556276f789]  \r\n[2017-12-07 10:20:19,595: INFO/MainProcess] Received task: proj.tasks.job[f1c10b1c-03ae-4220-9c7f-2cdf4afc61e3]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[d54f4554-4517-470f-8e14-adedcb93a46e]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[6255e5e6-d4c8-4d87-8075-642bca9e6a6d]  \r\n[2017-12-07 10:20:19,700: INFO/ForkPoolWorker-1] Task proj.tasks.job[ca856d5c-f3cc-45d4-9fbc-665753f5d1d2] succeeded in 0.00115608799388s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-4] Task proj.tasks.job[9ae27611-e8e5-4e08-9815-1e56e2ad1565] succeeded in 0.00130111300678s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-3] Task proj.tasks.job[f5aa7c6a-4142-4a38-8814-c60424196826] succeeded in 0.00129756200477s: None\r\n[2017-12-07 10:20:19,702: INFO/ForkPoolWorker-2] Task proj.tasks.job[eb13b5c5-8865-4992-8b9e-6672c909fd59] succeeded in 0.00100053900678s: None\r\n[2017-12-07 10:20:19,710: INFO/MainProcess] Received task: proj.tasks.job[01700061-c69c-4f4c-abf2-e6ba200772bd]  \r\n[2017-12-07 10:20:19,711: INFO/MainProcess] Received task: proj.tasks.job[a27d7a7b-2c58-4689-8b98-2c0a4ceaea9f]  \r\n[2017-12-07 10:20:19,713: INFO/MainProcess] Received task: proj.tasks.job[4c4a5685-23d5-4178-89cc-9ce4ad5a3509]  \r\n[2017-12-07 10:20:19,714: INFO/MainProcess] Received task: proj.tasks.job[44a079a3-aacf-48c5-a76b-a061bdced1d6]\r\n[2017-12-07 01:41:03,591: CRITICAL/MainProcess] Unrecoverable error: AttributeError(\"'error' object has no attribute 'errno'\",)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/async/hub.py\", line 354, in create_loop\r\n    cb(*cbargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 444, in _event_process_exit\r\n    self.maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1307, in maintain_pool\r\n    self._maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1298, in _maintain_pool\r\n    joined = self._join_exited_workers()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1165, in _join_exited_workers\r\n    self.process_flush_queues(worker)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 1175, in process_flush_queues\r\n    readable, _, _ = _select(fds, None, fds, timeout=0.01)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 183, in _select\r\n    if exc.errno == errno.EINTR:\r\nAttributeError: 'error' object has no attribute 'errno'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jenstroeger": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4420", "title": "How to unpack serialzied task arguments?", "body": "When iterate over all currently scheduled tasks\r\n```python\r\nfor task in chain.from_iterable(my_app.control.inspect().scheduled().values()):\r\n    print(task)\r\n```\r\nI get a dictionary `task['request']` which contains a serialization of the tasks\u2019 arguments in `task['request']['args']` (and `'kwargs`). Both are strings:\r\n```\r\n'args': \"('5', {'a': 'b'})\",\r\n'kwargs': '{}', \r\n```\r\nIt\u2019s not [JSON](https://www.json.org/) nor [msgpack](https://msgpack.org/). How can I unpack that `args` string into a Python tuple again? Celery must have a helper function for that somewhere? (Anything to do with `argsrepr` and `kwargsrepr` and [`saferepr`](https://github.com/celery/celery/blob/master/celery/utils/saferepr.py)?)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Chris7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/ba2dec7956782c84068ef779e554fb07de524beb", "message": "Propagate arguments to chains inside groups (#4481)\n\n* Remove self._frozen from _chain run method\r\n\r\n* Add in explicit test for group results in chain"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4482", "title": "Add docker-compose and base dockerfile for development", "body": "This adds a docker based development environment. This removes the need for users to install their own rabbit/redis/virtual environment/etc. to begin development of celery. I use this myself (since after moving to docker I have essentially nothing installed on my computer anymore) and saw interest in it from issue #4334 so thought a PR may be appropriate to share my setup.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "auvipy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/028dbe4a4d6786d56ed30ea49971cc5415fffb4b", "message": "update version to 4.2.0"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4459", "title": "[wip] #3021 bug fix ", "body": "Fixes #3021", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zpl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/442f42b7084ff03cb730ca4f452c3a47d9b8d701", "message": "task_replace chord inside chord fix (fixes #4368) (#4369)\n\n* task_replace chord inside chord fix\r\n\r\n* Complete fix for replace inside chords with tests\r\n\r\n* Add integration tests for add_to_chord\r\n\r\n* Fix JSON serialisation in tests\r\n\r\n* Raise exception when replacing signature has a chord"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4302", "title": "Ignore celery.exception.Ignore on autoretry", "body": "Autoretry for task should ignore celery.exception.Ignore, which is generated by self.replace()\r\n\r\notherwise, it goes to infinite loop.\r\n\r\n```python\r\n@app.task(autoretry_for=(Exception,), default_retry_delay=1,\r\n          max_retries=None,\r\n          bind=True,acks_late=True)\r\ndef TaskA(self):\r\n    raise self.replace(TaskB.s()) # Always retrying because of replace\r\n\r\n```\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdufresne": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/5eba340aae2e994091afb7a0ed7839e7d944ee13", "message": "Pass python_requires argument to setuptools (#4479)\n\nHelps pip decide what version of the library to install.\r\n\r\nhttps://packaging.python.org/tutorials/distributing-packages/#python-requires\r\n\r\n> If your project only runs on certain Python versions, setting the\r\n> python_requires argument to the appropriate PEP 440 version specifier\r\n> string will prevent pip from installing the project on other Python\r\n> versions.\r\n\r\nhttps://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords\r\n\r\n> python_requires\r\n>\r\n> A string corresponding to a version specifier (as defined in PEP 440)\r\n> for the Python version, used to specify the Requires-Python defined in\r\n> PEP 345."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "freakboy3742": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a4abe149aa00b0f85024a6cac64fd984cb2d0a6b", "message": "Refs #4356: Handle \"hybrid\" messages that have moved between Celery versions (#4358)\n\n* handle \"hybrid\" messages which have passed through a protocol 1 and protocol 2 consumer in its life.\r\n\r\nwe detected an edgecase which is proofed out in https://gist.github.com/ewdurbin/ddf4b0f0c0a4b190251a4a23859dd13c#file-readme-md which mishandles messages which have been retried by a 3.1.25, then a 4.1.0, then again by a 3.1.25 consumer. as an extension, this patch handles the \"next\" iteration of these mutant payloads.\r\n\r\n* explicitly construct proto2 from \"hybrid\" messages\r\n\r\n* remove unused kwarg\r\n\r\n* fix pydocstyle check\r\n\r\n* flake8 fixes\r\n\r\n* correct fix for misread pydocstyle error"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "djw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9ce3df9962830a6f9e0e68005bdeec1092e314e4", "message": "Corrected the default visibility timeout (#4476)\n\nAccording to kombu, the default visibility timeout is 30 minutes.\r\n\r\nhttps://github.com/celery/kombu/blob/3a7cdb07c9bf75b54282274d711af15ca6ad5d9f/kombu/transport/SQS.py#L85"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alexgarel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0ffd36fbf9343fe2f6ef7744a14ebfbec5ac86b6", "message": "request on_timeout now ignores soft time limit exception (fixes #4412) (#4473)\n\n* request on_timeout now ignores soft time limit exception (closes #4412)\r\n\r\n* fix quality"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "georgepsarakis": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a7915054d0e1e896c9ccf5ff0497dd8e3d5ed541", "message": "Integration test to verify PubSub unsubscriptions (#4468)\n\n* [Redis Backend] Integration test to verify PubSub unsubscriptions\r\n\r\n* Import sequence for isort check\r\n\r\n* Re-order integration tasks import"}, {"url": "https://api.github.com/repos/celery/celery/commits/9ab0971fe28462b667895d459d198ef6dd761c89", "message": "Add --diff flag in isort in order to display changes (#4469)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pachewise": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c8f9b7fbab3fe8a8de5cbae388fca4edf54bf503", "message": "Fixes #4452 - Clearer Django settings documentation (#4467)\n\n* reword django settings section in first steps\r\n\r\n* anchor link for django admonition\r\n\r\n* mention django-specific settings config"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hclihn": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/3ca1a54e65762ccd61ce728b3e3dfcb622fc0c90", "message": "Allow the shadow kwarg and the shadow_name method to set shadow properly (#4381)\n\n* Allow the shadow kwarg and the shadow_name method to set shadow properly \r\n\r\nThe shadow_name option in the @app.task() decorator (which overrides the shadow_name method in the Task class) and the shadow keyword argument of Task.apply_async() don't work as advertised.\r\nThis moves the shadow=... out of the 'if self.__self__ is not None:' block and allows shadow to be set by the shadow keyword argument of Task.apply_async() or the shadow_name method in the Task class (via, say, the shadow_name option in the @app.task() decorator).\r\n\r\n* Added a test to cover calling shadow_name().\r\n\r\n* Sort imports.\r\n\r\n* Fix missing import."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AlexHill": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fde58ad677a1e28effd1ac13f1f08f7132392463", "message": "Run chord_unlock on same queue as chord body - fixes #4337 (#4448)"}, {"url": "https://api.github.com/repos/celery/celery/commits/25f5e29610b2224122cf10d5252de92b4efe3e81", "message": "Support chords with empty headers (#4443)"}, {"url": "https://api.github.com/repos/celery/celery/commits/7ef809f41c1e0db2f6813c9c3a66553ca83c0c69", "message": "Add bandit baseline file with contents this time"}, {"url": "https://api.github.com/repos/celery/celery/commits/fdf0928b9b5698622c3b8806e2bca2d134df7fa3", "message": "Add bandit baseline file"}, {"url": "https://api.github.com/repos/celery/celery/commits/10f06ea1df75f109bf08fb8d42f9977cabcd7e0e", "message": "Fix length-1 and nested chords (#4393 #4055 #3885 #3597 #3574 #3323) (#4437)\n\n* Don't convert single-task chord to chain\r\n\r\n* Fix evaluation of nested chords"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pokoli": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/63c747889640bdea7753e83373a3a3e0dffc4bd9", "message": "Add celery_tryton integration on framework list (#4446)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "azaitsev": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/83872030b00a1ac75597ed3fc0ed34d9f664c6c1", "message": "Fixed wrong value in example of celery chain (#4444)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matteius": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/976515108a4357397a3821332e944bb85550dfa2", "message": "make astimezone call in localize more safe (#4324)\n\nmake astimezone call in localize more safe; with tests"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "myw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/4dc8c001d063a448d598f4bbc94056812cf15fc8", "message": "Add Mikhail Wolfson to CONTRIBUTORS.txt (#4439)"}, {"url": "https://api.github.com/repos/celery/celery/commits/dd2cdd9c4f8688f965d7b5658fa4956d083a7b8b", "message": "Resolve TypeError on `.get` from nested groups (#4432)\n\n* Accept and pass along the `on_interval` in ResultSet.get\r\n\r\nOtherwise, calls to .get or .join on ResultSets fail on nested groups.\r\nFixes #4274\r\n\r\n* Add a unit test that verifies the fixed behavior\r\n\r\nVerified that the unit test fails on master, but passes on the patched version. The\r\nnested structure of results was borrowed from #4274\r\n\r\n* Wrap long lines\r\n\r\n* Add integration test for #4274 use case\r\n\r\n* Switch to a simpler, group-only-based integration test\r\n\r\n* Flatten expected integration test result\r\n\r\n* Added back testcase from #4274 and skip it if the backend under test does not support native joins.\r\n\r\n* Fix lint.\r\n\r\n* Enable only if chords are allowed.\r\n\r\n* Fix access to message."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "johnarnold": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4490", "title": "Add task properties to AsyncResult, store in backend", "body": "*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PauloPeres": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4484", "title": "Adding the CMDS for Celery and Celery Beat to Run on Azure WebJob", "body": "This is what worked for us!\r\n\r\n## Description\r\nCreated the .cmd files to be used on Azure.\r\nJust going into Web Jobs on Azure servers and creating a Continuous Web Job, and uploading the zip files of the celery should work.\r\nEach folder should have a separeted Web Job.\r\nAlso whoever use should take a look where their celery package is\r\n\r\n\r\nThis pull request don't fix any bugs, it's just a new \"Helper\" for the ones who want to use Celery into Azure servers.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zengdahuaqusong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4475", "title": "separate backend database from login database", "body": "by setting 'backend_database' in MONGODB_BACKEND_SETTINGS, users can separate backend database from login database. Which means they can authenticate mongodb with 'database', while data is actually writing to 'backend_database'\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4474", "title": "Replace TaskFormatter with TaskFilter", "body": "Replacing `TaskFormatter` with a `TaskFilter` as that can be more easily reused when overriding the logging system.\r\n\r\nI've left the `TaskFormatter` in but marked it as deprecated in case people are using it.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "neaket360pi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4472", "title": "Cleanup the mailbox's producer pool after forking", "body": "Fixes https://github.com/celery/celery/issues/3751\r\n\r\nIf the mailbox is used before forking the workers will\r\nnot be able to broadcast messages.  This is because the producer pool\r\ninstance on the mail box will be `closed.`  This fix will cause the\r\nmailbox to load the producer pool again after forking.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charettes": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4456", "title": "Perform a serialization roundtrip on eager apply_async.", "body": "Fixes #4008 \r\n\r\n/cc @AlexHill ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jurrian": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4442", "title": "Fix for get() follow_parents exception handling (#1014)", "body": "## Description\r\n\r\nWhen working with chains, the situation might be that the first chain task has failed. Calling `x.get(propagate=True, follow_parents=True)` should cope with that since it checks if the parent tasks have raised exceptions.\r\n\r\nHowever, in my experience, when `x.get(propagate=True, follow_parents=True)` is called the failed task is still pending at that moment and will become failed seconds later. This creates a bug in the behaviour of `follow_parents`.\r\n\r\nThis fix calls `get(propagate=True, follow_parents=False)` on each parent instead, which will cause it to raise directly when the parent task becomes failed. It looks like `maybe_throw()` can be safely replaced since it is called by `get()` at some other place.\r\n\r\nIn order to be more comprehensive, I extended the documentation for `follow_parents`.\r\n\r\nI am not experienced enough in this project to see the whole picture, so please advise on possible problems that might arise with this fix for #1014 .\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Checkroth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4285", "title": "Add failing test for broker url priority ref issue #4284", "body": "## Description\r\nThis PR is to assist in showing the issue described here: https://github.com/celery/celery/issues/4284\r\n\r\nAll this PR does is add a failing test that _should_ be passing, if the issue above is resolved.\r\n\r\nThis PR does not solve the issue mentioned. I leave that up to the discretion of a more knowledgeable party, if they decide that it is indeed an issue at all. I do not expect this PR to be merged unless the issue is resolved and you want this test to remain in the repository.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jamesmallen": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4262", "title": "Disable backend for beat", "body": "## Description\r\n\r\nWhen using `beat` (at least in its standalone form), it is not necessary to subscribe to events on the result backend. These subscriptions happen in the `on_task_call` method of the backend. This PR ensures that no `SUBSCRIBE` messages are sent.\r\n\r\nFixes #4261 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bluestann": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4241", "title": "Use Cherami as a broker", "body": "Hi, \r\n\r\nI have added support for celery to use [Cherami](https://eng.uber.com/cherami/) as a broker. \r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "harukaeru": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4239", "title": "Changed raise RuntimeError to RuntimeWarning when task_always_eager is True", "body": "## Description\r\nThis PR changes RuntimeError to RuntimeWarning when `task_always_eager` is True.\r\n\r\nSometimes I tested what is related to celery task using `task_always_eager` in celery v3.\r\n(The tests are not only celery task but codes using celery task and other codes)\r\n\r\nI know the config cannot be completely tested async task but it's useful when doing rush works.\r\nI never use them in the production code but in the test, I use.\r\n\r\nI read the issue (https://github.com/celery/celery/issues/2275) was produced and the commit (https://github.com/celery/celery/commit/c71cd08fc72742efbfc846a81020939aa3692501) resolved the above.\r\n\r\n\r\nI almost agree with them but people who want to test perfectly only don't turn on `task_always_eager`.\r\nOthers who want to test synchronously also want to use `task_always_eager`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "erebus1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4227", "title": "fix crontab description of month_of_year behaviour", "body": "\r\n## Description\r\nIn [docs](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#crontab-schedules) said:\r\n\r\n> crontab(0, 0, month_of_year='*/3') -> Execute on the first month of every quarter.\r\n\r\nWhich is a bit confused, because, in fact it will run task each day on the first month of every quarter.\r\n\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rpkilby": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4212", "title": "Fix celery_worker test fixture", "body": "This is an attempt to fix #4088. Thanks to @karenc for providing a [breakdown](https://github.com/celery/celery/issues/4088#issuecomment-321287239) of what's happening.\r\n\r\nSide note:\r\nUsing `celery_worker` over `celery_session_worker` in the integration tests is a bit slower, given the worker startup/teardown for each test. This could be faster if the worker was scoped at a module level, but it's not possible to change the scope a fixture. The [recommendation](https://github.com/pytest-dev/pytest/issues/2300) is to simply create a fixture per scope. \r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bbgwilbur": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4077", "title": "Catch NotRegistered exception for errback", "body": "If the errback task is not registered in the currently running worker, the arity_greater check will fail. We can just assume its going to work as an old-style signature and deal with the possible error if it doesn't later.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChillarAnand": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4013", "title": "Updated commands to kill celery workers", "body": "```\r\nchillar+  1696 26093  1 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:MainProcess] -active- (worker -l info -A t)\r\nchillar+  1715  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-1]\r\nchillar+  1716  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-2]\r\nchillar+  1717  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-3]\r\nchillar+  1718  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-4]\r\n```\r\nWith latest version, celery worker process names seems changed. So, the commands used to kill those process needs to be updated.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Taywee": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3852", "title": "celery.beat.Scheduler: fix _when to make tz-aware (#3851)", "body": "Fixes #3851 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JackDanger": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3838", "title": "Removing last references to task_method", "body": "AFAICT the code removed in this PR only served to support the\n(now-removed) celery.contrib.methods.task_method() function.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gugu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3834", "title": "Handle ignore_result", "body": "## Description\r\n\r\nCurrently (according to my checks and code grep) ignore_result option is a stub, it does nothing. This patch skips backend calls for tasks with `ignore_result=True`\r\n\r\nThis is needed to minimize effect of broken redis backend support", "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3815", "title": "Chord does not clean up subscribed channels in redis. Fixes #3812", "body": "This pull requests fixes issue, when chord gets result for subtasks, but does not do `UNSUBSCRIBE` command for them.\r\n\r\n**What does this patch fix**:\r\n\r\nWithout this patch `chord(task() for i in range(50))` creates 51 subscriptions. After patch it will create 1\r\n\r\n**What this patch does not fix**:\r\n\r\nEvery task, which result is not consumed, creates redis subscription. Even if `ignore_result` is specified. I plan to submit separate patch for this issue\r\n\r\nI think, that changing old slow redis behavior to new fast and broken was not a good idea, but as soon as choice made, we need to make it usable", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "justdoit0823": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3757", "title": "Add supporting for task execution working with tornado coroutine", "body": "With this future, we can use tornado coroutine in celery app task when pool implementation is gevent. The main idea here is using a standalone tornado ioloop thread to execute coroutine task. If a task is a coroutine, the executor will register a callback on the tornado ioloop and switch to the related gevent hub. Here using a callback means that the coroutine will be totally executed inside the tornado ioloop thread. When the coroutine is finished, the executor will spawn a new greenlet which will switch to the previous task greenlet with the coroutine's result. Then task greenlet returns the result to the outer function inside the same greenlet. \r\n\r\nWe can write code in celery task as following:\r\n\r\n\r\n```python\r\nfrom celery import app\r\nfrom tornado import gen\r\n\r\n@app.task\r\n@gen.coroutine\r\ndef task_A():\r\n\r\n    process_1()\r\n    res = yield get_something()\r\n    do_something_with_res()\r\n    return res\r\n\r\n@gen.coroutine\r\ndef get_something():\r\n\r\n    res = {}\r\n    return res\r\n```\r\n\r\nThis is interesting when we use the tornado framework to write a project, we can easily divide partial logic into a celery task without any modifications. And in tornado 5.0, which will support Python 3 native coroutine, this future can connect more things together. I think people will like this.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "regisb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3684", "title": "Refer worker request info to absolute time", "body": "Previously, the time_start attribute of worker request objects refered\r\nto a timestamp relative to the monotonic time value. This caused\r\ntime_start attributes to be at a time far in the past. We fix this by\r\ncalling the on_accepted callback with an absolute time_accepted\r\nattribute.\r\n\r\nNote that the current commit does not change the\r\ncelery.concurrency.base API, although it would probably make sense to\r\nrename the \"monotonic\" named argument to \"time\".\r\n\r\nThis fixes the problem described in http://stackoverflow.com/questions/20091505/celery-task-with-a-time-start-attribute-in-1970/\r\n\r\nNote that there are many different ways to solve this problem; in the proposed implementation I choose to modify the default value of a keyword argument that is AFAIK never used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcsaaddupuy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3592", "title": "fixes exception deserialization when not using pickle", "body": "fix for #3586 \r\n\r\nThis PR try to fix how exceptions are deserialized when using a serializer different than pickle.\r\n\r\nThis avoid to [create new types](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L46) for exceptions, by doing 2 things : \r\n\r\n- store the exception module in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L243-L245) and reuse it in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L249-L258) instead of using raw `__name__` as module name\r\n\r\n- in [ celery/celery/utils/serialization.py::create_exception_cls](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L86-L98), try to find the exception class either from  `__builtins__`, or from the excetion module. Fallback on current behavior (which may still be wrong)\r\n\r\nAlso, it uses `exc.args` in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L247) and pass it to the exception constructor in [celery/celery/backends/base.py::Backend::exception_to_python](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L255), instead of using `str(exc.message)` which could lead to unwanted behavior\r\n\r\nThis won't work in every cases. If a class is defined locally in a function, this code won't be able to import the exception class using `import_module` and the old (wrong) behavior will still be used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "astewart-twist": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3293", "title": "Deserialize json string prior to inclusion in CouchDB doc", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chadrik": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3043", "title": "group and chord: subtasks are not executed until entire generator is consumed", "body": "When passing a generator to `group` or `chord`, sub-tasks are not submitted to the backend for execution until the entire generator is consumed.  The expected result is that subtasks are submitted during iteration, as soon as they are yielded.  This can have a big impact on performance if generators yield subtasks over a long period of time.\n\nI also opened issue #3021 on the subject.  [This explanation](https://github.com/celery/celery/issues/3021#issuecomment-176729202) from @eli-green I think is pretty on point.\n\nSo far I've only added support for redis.  I started looking at the other backends but I ran out of time.  It would be great to get some feedback on what I have so far and to get some thoughts on how difficult it will be to add for the other backends.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "m4ddav3": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2881", "title": "Database Backend: Use configured serializer, instead of always pickling.", "body": "Use the BLOB as an sa.BLOB\nSerialise the result an add to the db as bytes\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ask": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15545", "body": "README: Fix typo \"task.register\" -> \"tasks.register\". Closed by 8b5685e5b11f8987ba56c28ccb47f6c139541384. Thanks gregoirecachet)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546", "body": "What version is this? I thought I had fixed this already.\n\nThe examples in README should be updated. I think the best way of defining tasks now is using a Task class:\n\n```\n from celery.task import Task\n\n class MyTask(Task):\n     name = \"myapp.mytask\"\n\n     def run(self, x, y):\n         return x * y\n```\n\nand then\n\n```\n>>> from myapp.tasks import MyTask\n>>> MyTask.delay(2, 2)\n```\n\nas this makes it easier later to define a default routing_key for your task etc.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205", "body": "This works now. I remember it didn't at some point, and I remember I fixed it, so unless it still doesn't work for you I'm closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554", "body": "oh, that's bad. Got to get rid of yadayada dependency anyway, it's been an old trashbag for utilities, and all that is used from it now is the PickledObjectField.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555", "body": "Remove yadayada dependency. that means we've copy+pasted the\nPickledObjectField, when will djangosnippets ever die? :( Closed by fb582312905c5a1e001b6713be78ee2154b13204. Thansk\ngregoirecachet!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182", "body": "I'm not sure if that's so bad. Test requirements is not the same as install requirements. Sad the Django test runner is so broken. Maybe I can get it in somewhere else. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353", "body": "Add the test-runner from yadayada into the repo so we don't depend on yadayada\nanymore. Closed by af9ba75e195fc740493c9af6dbe84105b369d640.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428", "body": "Seems to be fine now. We'll re-open the issue if anyone says otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255", "body": "Implemented by 048d67f4bfb37c75f0a5d3dd4d0b4e05da400185 +  89626c59ee3a4da1e36612449f43362799ac0305\nAnd it really _is fast_ compared to the database/key-value store backends which uses polling to wait for the result.\n\nTo enable this back-end add the following setting to your settings.py/celeryconf.py:\n\n```\nCELERY_BACKEND=\"amqp\"\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017", "body": "A sample implementation has been commited (794582ebb278b2f96080a4cf4a68f1e77c3b003b + 93f6c1810c1051f8bdea6a7eae21d111997388d00 + fe62c47cb04723af738192087b40caefd27cab6a ) but not tested yet. Currently seeking anyone willing to test this feature.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115", "body": "Task retries seems to be working with tests passing. Closed by 41a38bb25fcacb48ee925e4319d35af9ab89d2bf\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721", "body": "Use basic.consume instead of basic.get to receive messages. Closed by 72fa505c7dfcf52c3215c276de67e10728898e70. This\nmeans the CELERY_QUEUE_WAKEUP_AFTER and CELERY_EMPTY_MSG_EMIT_EVERY settings,\nand the -w|--wakeup-after command line option to celeryd has been removed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902", "body": "If delay() is hanging, I'm guessing it's not because of the database, but because it can't get a connection to the AMQP server. (amqplib's default timeout must be very high). Is the broker running? Are you running RabbitMQ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071", "body": "I set the default connection timeout to be 4 seconds. Closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627", "body": "Re-opening the issue as setting the amqplib connection timeout didn't resolve it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160", "body": "This is actually an issue with RabbitMQ and will be fixed in the 1.7 release. I added the following to the celery FAQ:\n\n RabbitMQ hangs if it isn't able to authenticate the current user,\nthe password doesn't match or the user does not have access to the vhost\nspecified.  Be sure to check your RabbitMQ logs\n(`/var/log/rabbitmq/rabbit.log` on most systems), it usually contains a\nmessage describing the reason.\n\nThis will be fixed in the RabbitMQ 1.7 release.\n\nFor more information see the relevant thread on the rabbitmq-discuss mailing\nlist: http://bit.ly/iTTbD\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924", "body": "AsyncResult.ready() was always True. Closed by 4775a4c279179c17784bb72dc329f9a9d442ff0a. Thanks talentless.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110", "body": "Oh. That's indeed a problem with the installation. Could you try to install it using pip?\n\n```\n$ easy_install pip\n$ pip install celery\n```\n\nI will fix it as soon as possible, but in the mean time you could use pip.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111", "body": "Only use README as long_description if the file exists so easy_install don't\nbreak. Closed by e8845afc1a53aeab5b30d82dea29de32eb46b1d6. Thanks tobycatlin\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169", "body": "The consumerset branch was merged into master in c01a9885bbb8c83846b3770364fe208977a093fd (original contribution: screeley/celery@e2d0a56c913c66f69bf0040c9b76f74f0bb7dbd8). Big thanks to Sean Creeley.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472", "body": "Fixed in  faaa58ca717f230fe8b65e4804ad709265b18d5a\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432", "body": "By the way; This worked before we started using auto_ack=True, and basic.get.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448", "body": "Wait with message acknowledgement until the task has actually been executed.\nClosed by ef3f82bcf3b4b3de6584dcfc4b189ddadb4f50e6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017", "body": "This now merged into master. On second thought Munin plugins for these stats doesn't make sense, at least not generally. You could use this to make munin-plugins though, it's more like the groundwork for something more interesting later. (and it's useful for profiling right away)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966", "body": "Make TaskSet.run() respect message options from the Task class. Closed by 03d30a32de3502b73bca370cb0e70863c0ad3dd2.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29867", "body": "Thanks for pointing that out. It's a bug, indeed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38526", "body": "I added the importlib backport module as a dependency, is that >= 2.6 only? I guess I could add django==1.1 as a dependency, but I think there's someone using 1.0.x still.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38984", "body": "2.7 you mean? Yeah, but I added http://pypi.python.org/pypi/importlib as a dependency. Does it work with Python >= 2.4?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38985", "body": "Ah, I just saw the trove classifiers, all the way down to 2.3.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/51698", "body": "This commit was actually authored by me, just forgot to reset my `GIT_AUTHOR_NAME`, and `GIT_AUTHOR_EMAIL` env variables.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/51698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/52587", "body": "damn. Seems I forgot to reset my GIT_AUTHOR_\\* settings :(\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/52587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/82477", "body": "Ah, ok. That's good to know! I wasn't sure about this. Also, I'm wondering what holds the connection, is it the engine or the session?\nCreating a new connection for every operation is probably a bad idea, but not sure if it does some connection pooling by default.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91900", "body": "lol, yeah. I guess this is borderline.\n\nYou have the ability to supply your own connection, if you do we can't close the connection for you.\n\nAbove it says:\n\n```\nconn = connection or establish_connection(connect_timeout=connect_timeout)\n```\n\nso `connection or conn.close`, is \"if user didn't supply a connection, close the connection we established\".\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91937", "body": "Forgot that it's using the `@with_connection decorator` which takes care of this automatically :/\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/352668", "body": "Thanks!  Fixed in 154431f2c4ff04515000462ede70e205672e1751\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280", "body": "Is this deliberate?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588", "body": "Why did you remove the `Content-Type`?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074", "body": "Think there's a race here if it enters `time.sleep()`, and the putlock is released when the pools state != RUN.  The task will be sent to the queue in this case.  I'll fix it before I merge.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078", "body": "Also changed the interval to 1.0 as we discussed on IRC.  The shutdown process is already almost always delayed by at least one second because of other thread sleeps, so it doesn't make a difference (apart from less CPU usage in online mode).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181", "body": "Actually, the Pool implementation shouldn't depend on Celery, it's more of a patch for fixes and features we need for multiprocessing.\nSo this option should be added to `Pool.__init__`, then passed on from `celery.worker.WorkController`, then `celery.concurrency.processes.TaskPool`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183", "body": "and, oh yeah, `celery.conf` is deprecated to be removed in 3.0, so you don't have to add anything to it.\n\nWhen added to `WorkController`, it works in the case where you instantiate the worker without configuration too,\ne.g: `WorkController(worker_lost_wait=20)`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375", "body": "Ah, this was just a typo, I copied the document...\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380", "body": "It's on my TODO, I have to write it\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043", "body": "You can't terminate a reserved task, as it hasn't been executed yet.\n\nIt adds the id of the task to the list of revoked tasks, this is then checked again when the worker is about to execute the task (moves from reserved -> active).  If you only search the reserved requests then the terminate functionality would be useless (it wouldn't have any process to kill)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052", "body": "the other changes look great\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034", "body": "Revoked check for reserved/ETA tasks happens here:\nhttps://github.com/celery/celery/blob/3.0/celery/worker/job.py#L185-186\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089", "body": "> Also, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n\nStill, you shouldn't terminate a reserved task.  If a reserved task is revoked it should be dropped _before_ any pool worker starts to execute it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191", "body": "Hmm, maybe we could remove the `set` here, so that it preserves the original order.\n\nIt's not terribly bad if it imports the same module twice after all\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552", "body": "For Python 2.6 you have to include positions: {0} {1} {2}\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484", "body": "Celery related code should not be called by this method as it should be decoupled from Celery.  You need to find a way to support this by exposing it in the API.\nE.g. `Worker(on_shutdown=signals.worker_process_shutdown.send)`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390", "body": "Since created is True, it would also call 'on_node_join' in this instance, even if it just left.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331", "body": "Should this be logged?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "gcachet": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15549", "body": "It's master. I figured out this way to define tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557", "body": "Thanks! Removing yadayada dependency was my first though also, but I wasn't sure about the implications.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "talentless": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18501", "body": "No problem!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tobycatlin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18135", "body": "pip installed ok. I haven't tried the source code out of git. \n\nThanks for the quick response\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "brosner": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18434", "body": "My best guess is we've called terminate more than once? Will need some investigation.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "nikitka": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29856", "body": "why after **import** you use import celeryconfig ? this is work?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "brettcannon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/38473", "body": "Django as of (I believe) 1.1 has importlib included w/ it under django.util.importlib, so you don't need to rely on Python 2.6 to get import_module.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/38959", "body": "importlib was added in Python 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "paltman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/79488", "body": "This is why the test now fails -- now that is no longer 29 minutes past the hour, but 30, it is now due to run, while the assertion was to make sure it handled the case when it wasn't due, which when the mocked value was 29 minutes past the hour it properly returned False.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/79488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/204916", "body": "Was there a test for this failure before the fix?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/204916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nvie": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/81557", "body": "Nice and tidy.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/81557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "haridsv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/82469", "body": "Is this creating a new engine every time a new session is needed? As per sqlalchemy documentation, engine should be created only one time, unless of course the connection URL itself is changing.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/82478", "body": "Yes, the engine by default has a connection pool enabled: http://www.sqlalchemy.org/docs/05/reference/sqlalchemy/pooling.html#connection-pool-configuration\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jonozzz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/91703", "body": "I don't quite understand this one... why \"connection\" and why \"or\" ?\nI think it should be:\n    conn and conn.close()\n\nBecause when connection is None, conn.close() will be executed anyway.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "simonz05": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/103991", "body": "Nice, the previous one was aweful. Good to have the side-bar back.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/103991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Kami": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/104086", "body": "I agree, great work.\n\nThe blue (first?) version wasn't bad either, but the previous version was kinda step in the wrong direction.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/104086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adamn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/123743", "body": "$VIRTUALENV was removed - does it still work with a virtualenv?  It doesn't seem to for me on Ubuntu 10.04\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/123743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zen4ever": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/131915", "body": "Thanks for the fix. That was fast.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/131915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "joshdrake": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136184", "body": "Was just about to post an issue on this. I was surprised by this too, but even subclasses of accepted types must have adapters registered for database backends. Here's documentation on the process for pyscopg2:\n\nhttp://initd.org/psycopg/docs/advanced.html#adapting-new-types\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zzzeek": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136191", "body": "you need to look at TypeDecorator:\n\nhttp://www.sqlalchemy.org/docs/reference/sqlalchemy/types.html?highlight=typedecorator#sqlalchemy.types.TypeDecorator\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "passy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/147340", "body": "The dot should be inside the comment. It's a syntax error otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/147340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dcramer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/175143", "body": "I should note I have no idea what this does, I stole it form somewhere on the internet and it fixed the problems :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/175143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "shimon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/270710", "body": "Why did the date_done entry get removed? This is useful information that other backends seem to provide.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/270710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "enlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/352650", "body": "this should be _kill(...)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mher": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/595269", "body": "https://bitbucket.org/jezdez/sphinx-pypi-upload/issue/1/minor-patch-for-namespace-modules-etc patch should be applied before launching  paver upload_pypi_docs. upload_pypi_docs fails if .build contains empty subdirectories.\n\nIs there a way to automate this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/595269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810", "body": "I think it would be better to move registry._set_default_serializer('auth') to setup_security\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "steeve": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600", "body": "yes, this allows for the result backend to sub on this, allowing it not to poll\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "mitar": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921", "body": "As I explained, the content is urlencoded combination of parameters, not JSON. Why it would be `application/json`? We do not serialize to JSON (at this stage) anywhere. But urllib does urlencode it. If we remove manual override, then urllib does the right thing and sets it to `application/x-www-form-urlencoded` (it also sets `Content-Length` properly). This can Django (or any other receiver) then properly decode. So, urllib sends content as `application/x-www-form-urlencoded` and header should match that.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brendoncrawford": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182", "body": "Ok, I'll submit a new patch within the next week or so.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188", "body": "Ok, ill take a stab at it. Might take a few tries, but I think I can get it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ztlpn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747", "body": "Well I have not been able to check it with the latest version yet, but at least on version 3.0.6 this does not happen! If reserved but not yet active task is revoked, no check against revoked list is made when task becomes active. Instead it executes normally.\n\nAlso, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ambv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809", "body": "SIGTERM is not 9.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810", "body": "Ditto.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553", "body": "This style of string formatting is only available to Python 2.7+. I believe Celery is still supporting 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "VRGhost": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911", "body": "Well, it should be\n\n```\nwarnings.warn(\"%s consumed store_result call with args %s, %s\" % (self.__class__.__name__, args, kwargs))\n```\n\nthan. :-)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "dmtaub": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779", "body": "Ok, I will work on an api-based version soon. Depending on whether I need\nto write zmq interprocess communication this week, it might end up being\nless important to merge our branches at this particular moment :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "andrewkittredge": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195", "body": "this is wrong.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ionelmc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835", "body": "I added this to help with debugging #1785. I still think we should have it (it could indicate other problems).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}}, "2": {"xunto": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4491", "title": "[BUG] ready() always returns False for groups", "body": "## Steps to reproduce\r\n\r\n```python\r\nsent = group(\r\n    celery_app.signature('task1', args=arg),\r\n    celery_app.signature('task2', args=arg),\r\n    celery_app.signature('task3', args=arg)\r\n).apply_async()\r\n\r\nwhile not sent.ready():\r\n    pass\r\n\r\nprint(\"test\")\r\n```\r\n\r\n## Expected behavior\r\nI expect ```ready()``` to return ```True``` when all tasks are finished.\r\n\r\n## Actual behavior\r\nLooks like this code will never finish execution as ```ready()``` always return ```False``` even if all tasks are finished. Tested on celery master branch.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nitinmeharia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4489", "title": "django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread", "body": "### Application details:\r\n```\r\n    software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.4\r\n            billiard:3.5.0.3 redis:2.10.6\r\n    platform -> system:Darwin arch:64bit imp:CPython\r\n    loader   -> celery.loaders.app.AppLoader\r\n    settings -> transport:redis results:redis://localhost:6379/2\r\n```\r\n\r\n### Error log\r\n```\r\n    Signal handler <bound method DjangoWorkerFixup.on_worker_process_init of <celery.fixups.django.DjangoWorkerFixup object at 0x108e46fd0>> raised: DatabaseError(\"DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\",)\r\n    Traceback (most recent call last):\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/utils/dispatch/signal.py\", line 227, in send\r\n        response = receiver(signal=self, sender=sender, **named)\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 154, in on_worker_process_init\r\n        self._close_database()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 186, in _close_database\r\n        conn.close()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 283, in close\r\n        self.validate_thread_sharing()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 542, in validate_thread_sharing\r\n        % (self.alias, self._thread_ident, thread.get_ident())\r\n    django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\r\n```\r\n\r\n## Steps to reproduce\r\n```\r\ncelery multi restart w1 -A proj -l info\r\n```\r\nIts a django 1.11 application where we are getting this error.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kurara": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4488", "title": "launching worker from python: error with configuration from object.", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nVersion celery: 4.1.0 (latentcall)\r\n\r\nI'm trying to launch a worker from python code. When I use the class CeleryCommand with 'worker' option, it works. But if I add the option '--detach' or 'multi' the broker configuration is wrong. The code is:\r\n\r\n```\r\napp.config_from_object(config_module)\r\ncelerycmd = CeleryCommand(app)\r\ncelerycmd.execute_from_commandline(argv=[prog_name, 'worker', 'app_name', pidfile, logfile, loglevel])\r\n```\r\nor (which is not working)\r\n\r\n`celerycmd.execute_from_commandline(argv=[prog_name, 'multi', 'start', 'app_name', pidfile, logfile, loglevel])`\r\n\r\n# Expected behavior\r\nGet the same broker as in the configuration, not the default one\r\n\r\n## Actual behavior\r\n\r\nlog_file:\r\n\r\n```\r\n[2018-01-15 15:08:58,471: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2018-01-15 15:08:58,506: INFO/MainProcess] mingle: searching for neighbors\r\n[2018-01-15 15:08:59,647: INFO/MainProcess] mingle: all alone\r\n[2018-01-15 15:08:59,672: INFO/MainProcess] app_name@hostname ready.\r\n```\r\n\r\nFYI: I'm using lower case configuration. \r\n\r\nI could provide you what the object has in debug mode. Just ask me and I post it. When I debugged, I think the 'app' had the configuration of the file when it was inside the function _execute_from_commandline_, I can't understand when it loses it.\r\n\r\nI tought that maybe the problem is that I don't provide the configuration at the begining of the file, where app is declared, but in a function. So I tested to add the broker directly when I create the app: \r\n`app = Celery('app_name', broker_url='broker@...')`\r\nbut it didn't work either.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MShekow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4486", "title": "Memory hogging in client when using RPC (with RabbitMQ)", "body": "## Description\r\nIn my scenario I have a client program that puts thousands of tasks on the queue (`generate_data.delay()`). The workers produce a result that is of considerable size (suppose each result uses 1 MB of memory). The result is pickled back, and the client processes the results in some way, whenever results are available. In other words, once `AsyncResult.ready() == True`, I `get()` the result and do something with it.\r\nUsing `objgraph` I found out that celery never releases the result data.\r\n\r\n## My configuration\r\n```\r\nSoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.2\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Windows arch:32bit, WindowsPE imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:rpc:///\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\nresult_backend: 'rpc:///'\r\nresult_serializer: 'pickle'\r\ntask_serializer: 'pickle'\r\naccept_content: ['pickle']\r\n```\r\n\r\n\r\n## Steps to reproduce\r\nWorker program simply returns large objects, e.g.:\r\n```\r\nclass Data:\r\n    def __init__(self):\r\n        self.data = b'1' * 1024 * 1024\r\n\r\n@app.task\r\ndef generate_data() -> Data:\r\n    time.sleep(random.uniform(0, 0.2))\r\n    d = Data()\r\n    return d\r\n```\r\nThe client program retrieves results whenever they are available:\r\n```\r\nresults = {}\r\n\r\nfor i in range(2000):\r\n    async_result = generate_data.delay()\r\n    results[async_result] = True\r\n\r\nlogger.info(\"Put {} jobs on process queue!\".format(len(results)))\r\n\r\nresults_collected = 0\r\nwhile True:\r\n    # get results that are ready now\r\n    ready_results = [async_result for async_result, _ in results.items() if async_result.ready()]\r\n    if not ready_results:\r\n        time.sleep(10)\r\n        continue\r\n\r\n    results_collected += len(ready_results)\r\n\r\n    logger.info(\"Processing {} results. Got {} results so far\".format(len(ready_results), results_collected))\r\n    for ready_result in ready_results:\r\n        # we don't actually use the data - a real program would process the data somehow\r\n        result_data = ready_result.get()  # type: Data\r\n        del results[ready_result]\r\n\r\n    gc.collect()\r\n\r\n    # Exit loop once all results were processed:\r\n    if not results:\r\n        break\r\nlogger.info(\"Finished collecting all results\")\r\n```\r\n\r\n## Expected behavior\r\nWhen my client no longer has a reference to neither the actual data returned by `AsyncResult.get()`, nor the `AsyncResult` object itself, the memory of the data and the `AsyncResult` should be freed by celery.\r\n\r\n## Actual behavior\r\nMemory is being hogged so quickly that my client process (32-bit) dies soon. The reason is the huge size of `MESSAGE_BUFFER_MAX` (8192) and the oddly-hardcoded `bufmaxsize` (1000) in `BufferMap`. Are there any reasons for these huge buffers?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thedrow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4483", "title": "Add test coverage for #4356", "body": "#4356 is a critical fix for a bug that occurs when messages migrate between Celery 3 and Celery 4 clusters.\r\nDue to it's severity It was merged without proper test coverage.\r\nWe need to ensure this code is covered by the appropriate unit tests.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4434", "title": "Run the integration tests in a different build stage", "body": "We need to find a way to run the integration tests in a different build stage so that the unit tests will before them.\r\nThe unit tests are much quicker to execute and thus should run first to free up build resources for other contributors.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4423", "title": "Document that tasks are now documented automatically by celery", "body": "See https://github.com/celery/celery/pull/4422", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2547666a1ea13b27bc13ef296ae43a163ecd4ab3", "message": "Don't cover this branch as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/3af6a635cf90a4452db4e87c2326579ebad750c2", "message": "Merge branch 'master' into master"}, {"url": "https://api.github.com/repos/celery/celery/commits/bd0ed23c81b20fd75c0e2188fcf78e4d74898953", "message": "Use editable installs to measure code coverage correctly."}, {"url": "https://api.github.com/repos/celery/celery/commits/0a0fc0fbf698d30e9b6be29661e0d447548cc47c", "message": "Report coverage to terminal as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/4b4bf2a4eee65871d3e9c0e96d94b23aa2ee602c", "message": "Report coverage correctly (#4445)\n\n* Report coverage correctly.\r\n\r\nAs it turns out this repository does not report coverage to codecov at all since the path to the executables has changed at some point.\r\nThis should fix the problem.\r\n\r\n* Readd code coverage badge."}, {"url": "https://api.github.com/repos/celery/celery/commits/dbd59d9fc988ad0f04ae710c32066c5ff62374ef", "message": "Added bandit to the build matrix."}, {"url": "https://api.github.com/repos/celery/celery/commits/2ae00362179897968cfbd1fc8d61b648356769a7", "message": "Added bandit to lint for security issues."}, {"url": "https://api.github.com/repos/celery/celery/commits/56b94c327244fad4933706fbddc02eeea508d21a", "message": "Prettify test output."}, {"url": "https://api.github.com/repos/celery/celery/commits/ebd98fa4d36bb8003c2f46dbd16e9888af13720f", "message": "Parallel doc lints (#4435)\n\n* Bump sphinx.\r\n\r\n* Update copyright year. Mark the celerydocs & the celery.contrib.sphinx extensions as read_parallel_safe.\r\n\r\n* Install from git for now :(\r\n\r\n* Fix flake8 errors."}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4268", "title": "Add Var.CI integration", "body": "This pull request aims to demonstrate the power of [VarCI](https://var.ci/), the missing assistant for GitHub issues.\n\n--\n_Automated response by [Var.CI](https://var.ci)_ :robot:", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3982", "title": "Added failing test cases for #3885", "body": "The tests were contributed by @robpogorzelski\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nAttempt to fix #3885 without hurting eager execution of the canvas.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PromyLOPh": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4480", "title": "Application is not thread-safe", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.4\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:rpc:///\r\n\r\nresult_backend: 'rpc:///'\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\n```\r\n\r\n## Steps to reproduce\r\n\r\nUsing code from the user manual, but two threads running concurrently:\r\n\r\n```python\r\nfrom celery import Celery\r\nimport time\r\nfrom threading import Thread\r\n\r\napp = Celery('test', broker='amqp://guest@localhost//', backend='rpc://')\r\n\r\n@app.task(bind=True)\r\ndef hello(self, a, b):\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 50})\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 90})\r\n    time.sleep(1)\r\n    return 'hello world: %i' % (a+b)\r\n\r\nif __name__ == '__main__':\r\n    def run ():\r\n        handle = hello.delay (1, 2)\r\n        print (handle.get ())\r\n    t1 = Thread (target=run)\r\n    t2 = Thread (target=run)\r\n    t1.start ()\r\n    t2.start ()\r\n    t1.join ()\r\n    t2.join ()\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\nhello world: 3\r\nhello world: 3\r\n```\r\n\r\n## Actual behavior\r\n\r\nDifferent exceptions, depending on timing. For instance:\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 456, in channel\r\n    return self.channels[channel_id]\r\nKeyError: None\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 56, in start\r\n    self._connection.default_channel, [initial_queue],\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 821, in default_channel\r\n    self._default_channel = self.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 266, in channel\r\n    chan = self.transport.create_channel(self.connection)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/transport/pyamqp.py\", line 100, in create_channel\r\n    return connection.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 459, in channel\r\n    channel.open()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 432, in open\r\n    spec.Channel.Open, 's', ('',), wait=spec.Channel.OpenOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 468, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 473, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 252, in read_frame\r\n    payload = read(size)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 417, in _read\r\n    s = recv(n - len(rbuf))\r\nsocket.timeout: timed out\r\n```\r\nor\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 59, in start\r\n    self._consumer.consume()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 477, in consume\r\n    self._basic_consume(T, no_ack=no_ack, nowait=False)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 598, in _basic_consume\r\n    no_ack=no_ack, nowait=nowait)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/entity.py\", line 737, in consume\r\n    arguments=self.consumer_arguments)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 1564, in basic_consume\r\n    wait=None if nowait else spec.Basic.ConsumeOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 471, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 476, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 254, in read_frame\r\n    'Received {0:#04x} while expecting 0xce'.format(ch))\r\namqp.exceptions.UnexpectedFrame: Received 0x3c while expecting 0xce\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kzidane": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4471", "title": "DisabledBackend when starting flask --with-threads?", "body": "I'm trying to use Celery for one of my applications and experiencing a strange behavior that I'm not sure why it's caused.\r\n\r\nTo replicate, here's a simple Flask app:\r\n\r\n    # application.py\r\n    from celery.contrib.abortable import AbortableAsyncResult\r\n    from flask import Flask\r\n    from tasks import add\r\n\r\n    app = Flask(__name__)\r\n\r\n    @app.route(\"/\")\r\n    def index():\r\n        # start the task and return its id\r\n        return add.delay(42, 50).task_id\r\n\r\n\r\n    @app.route(\"/state/<task_id>\")\r\n    def result(task_id):\r\n        # return current task state\r\n        return AbortableAsyncResult(task_id).state\r\n\r\nand here's a Celery app and a task:\r\n\r\n    # tasks.py\r\n    from celery import Celery\r\n    from celery.contrib.abortable import AbortableTask\r\n\r\n\r\n    app = Celery(\r\n        \"tasks\",\r\n        # use sqlite database as result backend (also tried rpc://)\r\n        backend=\"db+sqlite:///celerydb.sqlite\",\r\n        broker=\"pyamqp://localhost\"\r\n    )\r\n\r\n    @app.task(bind=True, base=AbortableTask)\r\n    def add(self, x, y):\r\n        return x + y\r\n\r\nRunning the Celery worker:\r\n\r\n    $ celery -A tasks worker --loglevel=info\r\n     -------------- celery@7677a80760b4 v4.1.0 (latentcall)\r\n    ---- **** ----- \r\n    --- * ***  * -- Linux-4.10.0-42-generic-x86_64-with-debian-jessie-sid 2018-01-02 20:22:48\r\n    -- * - **** --- \r\n    - ** ---------- [config]\r\n    - ** ---------- .> app:         tasks:0x7f339a52dfd0\r\n    - ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n    - ** ---------- .> results:     sqlite:///celerydb.sqlite\r\n    - *** --- * --- .> concurrency: 8 (prefork)\r\n    -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n    --- ***** ----- \r\n     -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n    [tasks]\r\n      . tasks.add\r\n\r\n    [2018-01-02 20:22:48,585: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n    [2018-01-02 20:22:48,592: INFO/MainProcess] mingle: searching for neighbors\r\n    [2018-01-02 20:22:49,608: INFO/MainProcess] mingle: all alone\r\n    [2018-01-02 20:22:49,636: INFO/MainProcess] celery@7677a80760b4 ready.\r\n\r\n\r\nRunning the Flask app:\r\n\r\n    $ FLASK_APP=application.py flask run --with-threads\r\n      * Serving Flask app \"application\"\r\n      * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\r\n\r\n\r\nHitting `/` with `curl` starts the task and returns its id without any problems:\r\n\r\n    $ curl http://localhost:5000\r\n    f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nCelery's output at this point:\r\n\r\n    [2018-01-02 20:29:28,974: INFO/MainProcess] Received task: tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef]\r\n    [2018-01-02 20:29:29,000: INFO/ForkPoolWorker-1] Task tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef] succeeded in 0.02414485500776209s: 92\r\n\r\nBut trying to get the state of the task \r\n\r\n    $ curl http://localhost:5000/state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nresults in the following error:\r\n\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n\r\neven though the backend seems to be configured per the `backend` argument to `Celery` and its output? I also tried setting `CELERY_RESULT_BACKEND` and `result_backend` using `app.conf.update`, but no luck!\r\n\r\nWhat's interesting is that this problem disappears if I drop the `--with-threads` option from the `flask run` command. Any idea why this might be caused and how to work around it if possible?\r\n\r\nAdditional details:\r\n\r\n    $ celery --version\r\n    4.1.0 (latentcall)\r\n    $ flask --version\r\n    Flask 0.12.2\r\n    Python 3.6.0 (default, Oct 30 2017, 05:46:44) \r\n    [GCC 4.8.4]\r\n\r\nFull traceback:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1982, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1614, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1517, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/_compat.py\", line 33, in reraise\r\n    raise value\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1612, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1598, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n      File \"/root/application.py\", line 14, in result\r\n    return AbortableAsyncResult(task_id).state\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 436, in state\r\n    return self._get_task_meta()['status']\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 375, in _get_task_meta\r\n    return self._maybe_set_cache(self.backend.get_task_meta(self.id))\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/backends/base.py\", line 352, in get_task_meta\r\n    meta = self._get_task_meta_for(task_id)\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n    127.0.0.1 - - [02/Jan/2018 20:46:28] \"GET /state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef HTTP/1.1\" 500 -\r\n\r\nThank you!", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fnordian": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4465", "title": "Celery.close() leaks redis connections", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.3\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\n\r\n```python\r\nimport time\r\nfrom celery import Celery\r\n\r\ndef run_celery_task(taskname):\r\n    with Celery(broker='redis://redis:6379/0', backend='redis://redis:6379/0') as celery:\r\n        res = celery.send_task(taskname)\r\n        print(res)\r\n\r\nfor i in range(0, 100):\r\n    run_celery_task(\"test\")\r\n\r\ntime.sleep(100)\r\n```\r\n\r\n```bash\r\nnetstat -tn | grep 6379 | grep ESTABLISHED | wc -l\r\n```\r\n## Expected behavior\r\n\r\nWhen the `with Celery`-block terminates, I expect all connections to redis being closed.\r\n\r\n## Actual behavior\r\n\r\nNot all connections are closed. When the for-loop finishes, there > 100 open connections to redis.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kimice": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4464", "title": "Maximum recursion depth exceeded while calling a Python object", "body": "Hi, when I call celery apply_async function, sometimes it raise Exception like this. It seems like getting config failed. This bug can't always reappear. I guess celery may be not init correctly. I'm so confused with this bug.\r\n\r\ncelery==4.1.0\r\n\r\nI init celery with flask like this.\r\n\r\n```\r\ndef make_celery(app):\r\n    celery = Celery(app.import_name, backend=app.config['CELERY_RESULT_BACKEND'],\r\n                    broker=app.config['CELERY_BROKER_URL'])\r\n    celery.conf.update(app.config)\r\n    celery.config_from_object('App.celery_custom.celery_config')\r\n    TaskBase = celery.Task\r\n    class ContextTask(TaskBase):\r\n        abstract = True\r\n        def __call__(self, *args, **kwargs):\r\n            with app.app_context():\r\n                return TaskBase.__call__(self, *args, **kwargs)\r\n    celery.Task = ContextTask\r\n    return celery\r\n\r\ncelery_app = make_celery(flask_app)\r\n\r\n@celery_app.task(bind=True)\r\ndef checkInstance(self, a, b):\r\n    pass\r\n\r\ncheck_task = checkInstance.apply_async(args=['123', '123'], queue='123')\r\n```\r\n\r\n```\r\nLOG:\r\n[2017-12-26 08:42:58,659]: logs_util.py[line:64] [pid:25680] ERROR Traceback (most recent call last):\r\n  File \"./App/views/experiment_views.py\", line 377, in create_experiment_and_run\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/task.py\", line 521, in apply_async\r\n    if app.conf.task_always_eager:\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 431, in __getitem__\r\n    return getitem(k)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 280, in __getitem__\r\n    return mapping[_key]\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/utils/objects.py\", line 44, in __get__\r\n    value = obj.__dict__[self.__name__] = self.__get(obj)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 148, in data\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 911, in _finalize_pending_conf\r\n    conf = self._conf = self._load_config()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 921, in _load_config\r\n    self.loader.config_from_object(self._config_source)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 128, in config_from_object\r\n    obj = self._smart_import(obj, imp=self.import_from_cwd)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 146, in _smart_import\r\n    return imp(path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 106, in import_from_cwd\r\n    package=package,\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/imports.py\", line 100, in import_from_cwd\r\n    with cwd_in_path():\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 84, in helper\r\n    return GeneratorContextManager(func(*args, **kwds))\r\nRuntimeError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lexabug": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4462", "title": "INFO log messages land to stderr", "body": "I've set up a basic application with Django 2.0 and Celery 4.1.0 with debug task (as it described [here](http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html#using-celery-with-django)) and one custom task in application tasks module.\r\nMy module code look like this:\r\n```\r\n# Create your tasks here\r\nfrom __future__ import absolute_import, unicode_literals\r\nfrom celery import shared_task\r\nfrom django.conf import settings\r\nfrom celery.utils.log import get_task_logger\r\n\r\nlogger = get_task_logger(__name__)\r\n\r\n@shared_task(name='validate_user_email', ignore_result=True, bind=True)\r\ndef validate_user_email(self, user_id):\r\n    logger.info('%s email verified', user_id)\r\n```\r\n\r\nWhen I redirect streams (stdout and stderr) to different logs  (info.log and error.log) info.log is silent while error.log contains log entries with levels WARNING and INFO.\r\n\r\nCommand I execute celery with is: `celery -A email_validation worker -Q validate --concurrency 5 --maxtasksperchild 100 -l info > info.log 2> error.log`\r\n\r\nCelery config:\r\n```\r\nCELERY_BROKER_URL = '******************'\r\nCELERY_BROKER_HEARTBEAT = 900\r\nCELERY_BROKER_HEARTBEAT_CHECKRATE = 15\r\nCELERY_RESULT_BACKEND = 'amqp'\r\nCELERY_WORKER_PREFETCH_MULTIPLIER = 1\r\nCELERY_WORKER_MAX_TASKS_PER_CHILD = 100\r\nCELERY_TASK_ACKS_LATE = True\r\nCELERY_ENABLE_UTC = False\r\nCELERY_TIMEZONE = 'US/Eastern'\r\nCELERY_WORKER_DISABLE_RATE_LIMITS = True\r\nCELERY_EVENT_QUEUE_TTL = 1\r\nCELERY_EVENT_QUEUE_EXPIRES = 60\r\nCELERY_RESULT_EXPIRES = 3600\r\nCELERY_TASK_IGNORE_RESULT = True\r\nCELERY_WORKER_HIJACK_ROOT_LOGGER = True\r\n```\r\n\r\nHow can make celery to post INFO log messages to stdout? ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cajbecu": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4457", "title": "Connection to broker lost. Trying to re-establish the connection: OSError: [Errno 9] Bad file descriptor", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Software\r\ncelery==4.1.0\r\nkombu==4.1.0\r\namqp==2.2.2\r\nPython 3.6.1\r\nbroker: rabbitmq 3.6.14\r\nresult backend: redis\r\n\r\n## Steps to reproduce\r\n1. celery -A proj worker -Q Q1 --autoscale=10,1 -Ofair --without-gossip --without-mingle --heartbeat-interval=60 -n Q1\r\n2. celery lost connection to broker\r\n3. after restarting affected worker the connection is successfully re-established and the worker starts processing tasks\r\n\r\n## Expected behavior\r\ncelery should re-establish connection to broker\r\n\r\n## Actual behavior\r\ncelery tries to re-establish connection to broker but fails with this error message (which is repeated every second) until manually restarted:\r\n```\r\n[user] celery.worker.consumer.consumer WARNING 2017-12-18 00:38:27,078 consumer: \r\nConnection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/loops.py\", line 47, in asynloop\r\n    obj.controller.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/worker.py\", line 217, in register_with_event_loop\r\n    description='hub.register',\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 151, in send_all\r\n    fun(parent, *args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/components.py\", line 178, in register_with_event_loop\r\n    w.pool.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/prefork.py\", line 134, in register_with_event_loop\r\n    return reg(loop)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in register_with_event_loop\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in <listcomp>\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 207, in add_reader\r\n    return self.add(fds, callback, READ | ERR, args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 158, in add\r\n    self.poller.register(fd, flags)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/utils/eventio.py\", line 67, in register\r\n    self._epoll.register(fd, events)\r\nOSError: [Errno 9] Bad file descriptor\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4457/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thiagogalesi": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4454", "title": "Celery does not consider authSource on mongodb backend URLs", "body": "Version: Celery 4.0.2 (from looking at the changes since then it seems there is no change addressing this issue here: https://github.com/celery/celery/commits/master/celery/backends/mongodb.py )\r\n\r\n(Edit) Confirmed with the following versions as well:\r\namqp==2.2.2\r\nbilliard==3.5.0.3\r\ncelery==4.1.0\r\nkombu==4.1.0\r\npymongo==3.6.0\r\n\r\nCelery Report\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.8\r\n            billiard:3.5.0.3 py-amqp:2.2.1\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\n\r\n## Steps to reproduce\r\n\r\nGive Celery a Backend URL pointing to a MongoDB instance with authentication and username/password (user/pwd set on the Admin DB by default) in the format:\r\n\r\nmongodb://user:pass@your-server/your_db?authSource=admin\r\n\r\n(Please see http://api.mongodb.com/python/current/examples/authentication.html#default-database-and-authsource and http://api.mongodb.com/python/current/api/pymongo/mongo_client.html?highlight=authsource )\r\n\r\n## Expected behavior\r\n\r\nCelery authenticates the user in the admin database (this is the same as passing --authenticationDatabase to the mongo client or the same url to MongoClient)\r\n\r\n## Actual behavior\r\n\r\nCelery tries to authenticate the user on the your_db database (failing to authenticate)\r\n\r\n## Workaround (not recommended)\r\n\r\nChange the db on the URL to /admin (this db shouldn't be used to store arbitrary data normally)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yanliguo": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4451", "title": "Celery (4.1.0) worker stops to consume new message when actives is empty  ", "body": "Hi there,\r\n   I was using celery to dispatch some long running task recently, and finding a way to disable prefetch. Now the config is:\r\n\r\nacks_late = True\r\nconcurrency = 1\r\nprefetch_multiplier = 1\r\n-Ofair\r\n\r\nActually, workers are still prefetching tasks. And I also have a monitor job to revoke tasks when a task is stuck in reserved state for a long time (let's say 5 minutes) or the task is outputing valid result.  \r\n\r\nOne thing wired is that, some workers stopped consuming new tasks when the actives is empty and the reseved task is revoked.  Has anybody ever met with this issue ? any help will be appreciated.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4450", "title": "PENDING state, what if it meant just one thing?", "body": "According to docs, PENDING state has the following meaning:\r\n\r\n> Task is waiting for execution or unknown. Any task id that\u2019s not known is implied to be in the pending state.\r\n\r\nAre there plans to make pending mean one thing only? It is quite confusing to handle a state that can mean \"waiting\" or \"lost\". ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfresh613": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4449", "title": "ValueFormatError when processing chords with couchbase result backend", "body": "Hi it seems like when I attempt to process groups of chords, the couchbase result backend is consistently failing to unlock the chord when reading from the db:\r\n\r\n`celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()`\r\n\r\nThis behavior does not occur with the redis result backend, i can switch between them and see that the error unlocking only occurs on couchbase.\r\n\r\n## Steps to reproduce\r\nAttempt to process a chord with couchbase backend using pickle serialization.\r\n\r\n## Expected behavior\r\nChords process correctly, and resulting data is fed to the next task\r\n\r\n## Actual behavior\r\nCelery is unable to unlock the chord from the result backend\r\n\r\n## Celery project info: \r\n```\r\ncelery -A ipaassteprunner report\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.10\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:couchbase://isadmin:**@localhost:8091/tasks\r\n\r\ntask_serializer: 'pickle'\r\nresult_serializer: 'pickle'\r\ndbconfig: <ipaascommon.ipaas_config.DatabaseConfig object at 0x10fbbfe10>\r\ndb_pass: u'********'\r\nIpaasConfig: <class 'ipaascommon.ipaas_config.IpaasConfig'>\r\nimports:\r\n    ('ipaassteprunner.tasks',)\r\nworker_redirect_stdouts: False\r\nDatabaseConfig: u'********'\r\ndb_port: '8091'\r\nipaas_constants: <module 'ipaascommon.ipaas_constants' from '/Library/Python/2.7/site-packages/ipaascommon/ipaas_constants.pyc'>\r\nenable_utc: True\r\ndb_user: 'isadmin'\r\ndb_host: 'localhost'\r\nresult_backend: u'couchbase://isadmin:********@localhost:8091/tasks'\r\nresult_expires: 3600\r\niconfig: <ipaascommon.ipaas_config.IpaasConfig object at 0x10fbbfd90>\r\nbroker_url: u'amqp://guest:********@localhost:5672//'\r\ntask_bucket: 'tasks'\r\naccept_content: ['pickle']\r\n```\r\n### Additional Debug output\r\n```\r\n[2017-12-13 15:39:57,860: INFO/MainProcess] Received task: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2]  ETA:[2017-12-13 20:39:58.853535+00:00] \r\n[2017-12-13 15:39:57,861: DEBUG/MainProcess] basic.qos: prefetch_count->27\r\n[2017-12-13 15:39:58,859: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x10b410b90> (args:('celery.chord_unlock', 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', {'origin': 'gen53678@silo2460', 'lang': 'py', 'task': 'celery.chord_unlock', 'group': None, 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', u'delivery_info': {u'priority': None, u'redelivered': False, u'routing_key': u'celery', u'exchange': u''}, 'expires': None, u'correlation_id': 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', 'retries': 311, 'timelimit': [None, None], 'argsrepr': \"('90c64bef-21ba-42f9-be75-fdd724375a7a', {'chord_size': 2, 'task': 'ipaassteprunner.tasks.transfer_data', 'subtask_type': None, 'kwargs': {}, 'args': (), 'options': {'chord_size': None, 'chain': [...], 'task_id': '9c6b5e1c-2089-4db7-9590-117aeaf782c7', 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', 'reply_to': '0a58093c-6fdd-3458-9a34-7d5e094ac6a8'}, 'immutable': False})\", 'eta': '2017-12-13T20:39:58.853535+00:00', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', u'reply_to':... kwargs:{})\r\n[2017-12-13 15:40:00,061: DEBUG/MainProcess] basic.qos: prefetch_count->26\r\n[2017-12-13 15:40:00,065: DEBUG/MainProcess] Task accepted: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] pid:53679\r\n[2017-12-13 15:40:00,076: INFO/ForkPoolWorker-6] Task celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()\r\n```\r\n\r\n### Stack trace from chord unlocking failure\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/builtins.py\", line 75, in unlock_chord\r\n    raise self.retry(countdown=interval, max_retries=max_retries)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/task.py\", line 689, in retry\r\n    raise ret\r\nRetry: Retry in 1s\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yutkin": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4438", "title": "Celery doesn't write RECEIVED state into MongoDB", "body": "When a number of tasks in a queue surpass a number of workers, new added tasks are not writing in a backend. In other words, I want to write task state (RECEIVED) into DB immediately after its invocation. It is possible?\r\n\r\nP.S. I'm using MongoDB as backend. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "canassa": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4426", "title": "Task is executed twice when the worker restarts", "body": "Currently using Celery 4.1.0\r\n\r\n## Steps to reproduce\r\n\r\nStart a new project using RabbitMQ and register the following task:\r\n\r\n```python\r\nfrom django.core.cache import cache\r\n\r\n@shared_task(bind=True)\r\ndef test_1(self):\r\n    if not cache.add(self.request.id, 1):\r\n        raise Exception('Duplicated task {}'.format(self.request.id))\r\n```\r\n\r\nNow start 2 workers. I used gevent with a concurrency of 25 for this test:\r\n\r\n```\r\ncelery worker -A my_proj -Q my_queue -P gevent -c 25\r\n```\r\n\r\nOpen a python shell and fire a a bunch of tasks:\r\n\r\n```python\r\nfrom myproj.tasks import test_1\r\n\r\nfor i in range(10000):\r\n    test_1.apply_async()\r\n```\r\n\r\nNow quickly do a warm shutdown (Ctrl+c) in one of the workers while it's still processing the tasks, you should see the errors popping in the second worker:\r\n\r\n```\r\nERROR    Task my_proj.tasks.test_1[e28e6760-1371-49c9-af87-d196c59375e9] raised unexpected: Exception('Duplicated task e28e6760-1371-49c9-af87-d196c59375e9',)\r\nTraceback (most recent call last):\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/code/scp/python/my_proj/tasks.py\", line 33, in test_1\r\n    raise Exception('Duplicated task {}'.format(self.request.id))\r\nException: Duplicated task e28e6760-1371-49c9-af87-d196c59375e9\r\n```\r\n\r\n## Expected behavior\r\n\r\nSince I am not using late acknowledgment and I am not killing the workers I wasn't expecting the tasks to execute again.\r\n\r\n## Actual behavior\r\n\r\nThe tasking are being executed twice, this is causing some problems in our servers because we restart our works every 15 minutes or so in order to avoid memory leaks.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kn-id": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4424", "title": "Celery Crash: Unrecoverable error when using QApplication in main process", "body": "Error happens when there's some queues already added before worker started and max task per child is 1\r\n\r\n## Checklist\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.12\r\n                         billiard:3.5.0.3 py-amqp:2.2.2\r\n      platform -> system:Linux arch:64bit, ELF imp:CPython\r\n      loader   -> celery.loaders.app.AppLoader\r\n      settings -> transport:amqp results:disabled\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n1. create instance of QApplication when main worker started\r\n```\r\n@celeryd_after_setup.connect\r\nfrom PyQt4.QtGui import QApplication\r\ndef init_worker(sender, **k):\r\n    QApplication([])\r\n```\r\n2. create task\r\n```\r\n@app.task\r\ndef job():\r\n    print 'hello'\r\n```\r\n3. add some queues (2 - 3 per thread. so if there's 4 worker child then there's 8 or more queues)\r\n\r\n4. start worker with max task per child 1\r\n```\r\ncelery worker -A proj --loglevel=INFO --max-tasks-per-child=1\r\n```\r\n\r\n## Expected behavior\r\n- run queues successfuly\r\n## Actual behavior\r\nCelery Crash after 1 queue per child\r\n```\r\n2017-12-07 10:20:19,594: INFO/MainProcess] Received task: proj.tasks.job[c6413dc2-ad62-4033-a69b-39556276f789]  \r\n[2017-12-07 10:20:19,595: INFO/MainProcess] Received task: proj.tasks.job[f1c10b1c-03ae-4220-9c7f-2cdf4afc61e3]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[d54f4554-4517-470f-8e14-adedcb93a46e]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[6255e5e6-d4c8-4d87-8075-642bca9e6a6d]  \r\n[2017-12-07 10:20:19,700: INFO/ForkPoolWorker-1] Task proj.tasks.job[ca856d5c-f3cc-45d4-9fbc-665753f5d1d2] succeeded in 0.00115608799388s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-4] Task proj.tasks.job[9ae27611-e8e5-4e08-9815-1e56e2ad1565] succeeded in 0.00130111300678s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-3] Task proj.tasks.job[f5aa7c6a-4142-4a38-8814-c60424196826] succeeded in 0.00129756200477s: None\r\n[2017-12-07 10:20:19,702: INFO/ForkPoolWorker-2] Task proj.tasks.job[eb13b5c5-8865-4992-8b9e-6672c909fd59] succeeded in 0.00100053900678s: None\r\n[2017-12-07 10:20:19,710: INFO/MainProcess] Received task: proj.tasks.job[01700061-c69c-4f4c-abf2-e6ba200772bd]  \r\n[2017-12-07 10:20:19,711: INFO/MainProcess] Received task: proj.tasks.job[a27d7a7b-2c58-4689-8b98-2c0a4ceaea9f]  \r\n[2017-12-07 10:20:19,713: INFO/MainProcess] Received task: proj.tasks.job[4c4a5685-23d5-4178-89cc-9ce4ad5a3509]  \r\n[2017-12-07 10:20:19,714: INFO/MainProcess] Received task: proj.tasks.job[44a079a3-aacf-48c5-a76b-a061bdced1d6]\r\n[2017-12-07 01:41:03,591: CRITICAL/MainProcess] Unrecoverable error: AttributeError(\"'error' object has no attribute 'errno'\",)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/async/hub.py\", line 354, in create_loop\r\n    cb(*cbargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 444, in _event_process_exit\r\n    self.maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1307, in maintain_pool\r\n    self._maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1298, in _maintain_pool\r\n    joined = self._join_exited_workers()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1165, in _join_exited_workers\r\n    self.process_flush_queues(worker)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 1175, in process_flush_queues\r\n    readable, _, _ = _select(fds, None, fds, timeout=0.01)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 183, in _select\r\n    if exc.errno == errno.EINTR:\r\nAttributeError: 'error' object has no attribute 'errno'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jenstroeger": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4420", "title": "How to unpack serialzied task arguments?", "body": "When iterate over all currently scheduled tasks\r\n```python\r\nfor task in chain.from_iterable(my_app.control.inspect().scheduled().values()):\r\n    print(task)\r\n```\r\nI get a dictionary `task['request']` which contains a serialization of the tasks\u2019 arguments in `task['request']['args']` (and `'kwargs`). Both are strings:\r\n```\r\n'args': \"('5', {'a': 'b'})\",\r\n'kwargs': '{}', \r\n```\r\nIt\u2019s not [JSON](https://www.json.org/) nor [msgpack](https://msgpack.org/). How can I unpack that `args` string into a Python tuple again? Celery must have a helper function for that somewhere? (Anything to do with `argsrepr` and `kwargsrepr` and [`saferepr`](https://github.com/celery/celery/blob/master/celery/utils/saferepr.py)?)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Chris7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/ba2dec7956782c84068ef779e554fb07de524beb", "message": "Propagate arguments to chains inside groups (#4481)\n\n* Remove self._frozen from _chain run method\r\n\r\n* Add in explicit test for group results in chain"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4482", "title": "Add docker-compose and base dockerfile for development", "body": "This adds a docker based development environment. This removes the need for users to install their own rabbit/redis/virtual environment/etc. to begin development of celery. I use this myself (since after moving to docker I have essentially nothing installed on my computer anymore) and saw interest in it from issue #4334 so thought a PR may be appropriate to share my setup.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "auvipy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/028dbe4a4d6786d56ed30ea49971cc5415fffb4b", "message": "update version to 4.2.0"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4459", "title": "[wip] #3021 bug fix ", "body": "Fixes #3021", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zpl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/442f42b7084ff03cb730ca4f452c3a47d9b8d701", "message": "task_replace chord inside chord fix (fixes #4368) (#4369)\n\n* task_replace chord inside chord fix\r\n\r\n* Complete fix for replace inside chords with tests\r\n\r\n* Add integration tests for add_to_chord\r\n\r\n* Fix JSON serialisation in tests\r\n\r\n* Raise exception when replacing signature has a chord"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4302", "title": "Ignore celery.exception.Ignore on autoretry", "body": "Autoretry for task should ignore celery.exception.Ignore, which is generated by self.replace()\r\n\r\notherwise, it goes to infinite loop.\r\n\r\n```python\r\n@app.task(autoretry_for=(Exception,), default_retry_delay=1,\r\n          max_retries=None,\r\n          bind=True,acks_late=True)\r\ndef TaskA(self):\r\n    raise self.replace(TaskB.s()) # Always retrying because of replace\r\n\r\n```\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdufresne": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/5eba340aae2e994091afb7a0ed7839e7d944ee13", "message": "Pass python_requires argument to setuptools (#4479)\n\nHelps pip decide what version of the library to install.\r\n\r\nhttps://packaging.python.org/tutorials/distributing-packages/#python-requires\r\n\r\n> If your project only runs on certain Python versions, setting the\r\n> python_requires argument to the appropriate PEP 440 version specifier\r\n> string will prevent pip from installing the project on other Python\r\n> versions.\r\n\r\nhttps://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords\r\n\r\n> python_requires\r\n>\r\n> A string corresponding to a version specifier (as defined in PEP 440)\r\n> for the Python version, used to specify the Requires-Python defined in\r\n> PEP 345."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "freakboy3742": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a4abe149aa00b0f85024a6cac64fd984cb2d0a6b", "message": "Refs #4356: Handle \"hybrid\" messages that have moved between Celery versions (#4358)\n\n* handle \"hybrid\" messages which have passed through a protocol 1 and protocol 2 consumer in its life.\r\n\r\nwe detected an edgecase which is proofed out in https://gist.github.com/ewdurbin/ddf4b0f0c0a4b190251a4a23859dd13c#file-readme-md which mishandles messages which have been retried by a 3.1.25, then a 4.1.0, then again by a 3.1.25 consumer. as an extension, this patch handles the \"next\" iteration of these mutant payloads.\r\n\r\n* explicitly construct proto2 from \"hybrid\" messages\r\n\r\n* remove unused kwarg\r\n\r\n* fix pydocstyle check\r\n\r\n* flake8 fixes\r\n\r\n* correct fix for misread pydocstyle error"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "djw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9ce3df9962830a6f9e0e68005bdeec1092e314e4", "message": "Corrected the default visibility timeout (#4476)\n\nAccording to kombu, the default visibility timeout is 30 minutes.\r\n\r\nhttps://github.com/celery/kombu/blob/3a7cdb07c9bf75b54282274d711af15ca6ad5d9f/kombu/transport/SQS.py#L85"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alexgarel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0ffd36fbf9343fe2f6ef7744a14ebfbec5ac86b6", "message": "request on_timeout now ignores soft time limit exception (fixes #4412) (#4473)\n\n* request on_timeout now ignores soft time limit exception (closes #4412)\r\n\r\n* fix quality"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "georgepsarakis": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a7915054d0e1e896c9ccf5ff0497dd8e3d5ed541", "message": "Integration test to verify PubSub unsubscriptions (#4468)\n\n* [Redis Backend] Integration test to verify PubSub unsubscriptions\r\n\r\n* Import sequence for isort check\r\n\r\n* Re-order integration tasks import"}, {"url": "https://api.github.com/repos/celery/celery/commits/9ab0971fe28462b667895d459d198ef6dd761c89", "message": "Add --diff flag in isort in order to display changes (#4469)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pachewise": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c8f9b7fbab3fe8a8de5cbae388fca4edf54bf503", "message": "Fixes #4452 - Clearer Django settings documentation (#4467)\n\n* reword django settings section in first steps\r\n\r\n* anchor link for django admonition\r\n\r\n* mention django-specific settings config"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hclihn": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/3ca1a54e65762ccd61ce728b3e3dfcb622fc0c90", "message": "Allow the shadow kwarg and the shadow_name method to set shadow properly (#4381)\n\n* Allow the shadow kwarg and the shadow_name method to set shadow properly \r\n\r\nThe shadow_name option in the @app.task() decorator (which overrides the shadow_name method in the Task class) and the shadow keyword argument of Task.apply_async() don't work as advertised.\r\nThis moves the shadow=... out of the 'if self.__self__ is not None:' block and allows shadow to be set by the shadow keyword argument of Task.apply_async() or the shadow_name method in the Task class (via, say, the shadow_name option in the @app.task() decorator).\r\n\r\n* Added a test to cover calling shadow_name().\r\n\r\n* Sort imports.\r\n\r\n* Fix missing import."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AlexHill": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fde58ad677a1e28effd1ac13f1f08f7132392463", "message": "Run chord_unlock on same queue as chord body - fixes #4337 (#4448)"}, {"url": "https://api.github.com/repos/celery/celery/commits/25f5e29610b2224122cf10d5252de92b4efe3e81", "message": "Support chords with empty headers (#4443)"}, {"url": "https://api.github.com/repos/celery/celery/commits/7ef809f41c1e0db2f6813c9c3a66553ca83c0c69", "message": "Add bandit baseline file with contents this time"}, {"url": "https://api.github.com/repos/celery/celery/commits/fdf0928b9b5698622c3b8806e2bca2d134df7fa3", "message": "Add bandit baseline file"}, {"url": "https://api.github.com/repos/celery/celery/commits/10f06ea1df75f109bf08fb8d42f9977cabcd7e0e", "message": "Fix length-1 and nested chords (#4393 #4055 #3885 #3597 #3574 #3323) (#4437)\n\n* Don't convert single-task chord to chain\r\n\r\n* Fix evaluation of nested chords"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pokoli": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/63c747889640bdea7753e83373a3a3e0dffc4bd9", "message": "Add celery_tryton integration on framework list (#4446)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "azaitsev": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/83872030b00a1ac75597ed3fc0ed34d9f664c6c1", "message": "Fixed wrong value in example of celery chain (#4444)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matteius": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/976515108a4357397a3821332e944bb85550dfa2", "message": "make astimezone call in localize more safe (#4324)\n\nmake astimezone call in localize more safe; with tests"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "myw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/4dc8c001d063a448d598f4bbc94056812cf15fc8", "message": "Add Mikhail Wolfson to CONTRIBUTORS.txt (#4439)"}, {"url": "https://api.github.com/repos/celery/celery/commits/dd2cdd9c4f8688f965d7b5658fa4956d083a7b8b", "message": "Resolve TypeError on `.get` from nested groups (#4432)\n\n* Accept and pass along the `on_interval` in ResultSet.get\r\n\r\nOtherwise, calls to .get or .join on ResultSets fail on nested groups.\r\nFixes #4274\r\n\r\n* Add a unit test that verifies the fixed behavior\r\n\r\nVerified that the unit test fails on master, but passes on the patched version. The\r\nnested structure of results was borrowed from #4274\r\n\r\n* Wrap long lines\r\n\r\n* Add integration test for #4274 use case\r\n\r\n* Switch to a simpler, group-only-based integration test\r\n\r\n* Flatten expected integration test result\r\n\r\n* Added back testcase from #4274 and skip it if the backend under test does not support native joins.\r\n\r\n* Fix lint.\r\n\r\n* Enable only if chords are allowed.\r\n\r\n* Fix access to message."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "johnarnold": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4490", "title": "Add task properties to AsyncResult, store in backend", "body": "*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PauloPeres": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4484", "title": "Adding the CMDS for Celery and Celery Beat to Run on Azure WebJob", "body": "This is what worked for us!\r\n\r\n## Description\r\nCreated the .cmd files to be used on Azure.\r\nJust going into Web Jobs on Azure servers and creating a Continuous Web Job, and uploading the zip files of the celery should work.\r\nEach folder should have a separeted Web Job.\r\nAlso whoever use should take a look where their celery package is\r\n\r\n\r\nThis pull request don't fix any bugs, it's just a new \"Helper\" for the ones who want to use Celery into Azure servers.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zengdahuaqusong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4475", "title": "separate backend database from login database", "body": "by setting 'backend_database' in MONGODB_BACKEND_SETTINGS, users can separate backend database from login database. Which means they can authenticate mongodb with 'database', while data is actually writing to 'backend_database'\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4474", "title": "Replace TaskFormatter with TaskFilter", "body": "Replacing `TaskFormatter` with a `TaskFilter` as that can be more easily reused when overriding the logging system.\r\n\r\nI've left the `TaskFormatter` in but marked it as deprecated in case people are using it.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "neaket360pi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4472", "title": "Cleanup the mailbox's producer pool after forking", "body": "Fixes https://github.com/celery/celery/issues/3751\r\n\r\nIf the mailbox is used before forking the workers will\r\nnot be able to broadcast messages.  This is because the producer pool\r\ninstance on the mail box will be `closed.`  This fix will cause the\r\nmailbox to load the producer pool again after forking.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charettes": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4456", "title": "Perform a serialization roundtrip on eager apply_async.", "body": "Fixes #4008 \r\n\r\n/cc @AlexHill ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jurrian": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4442", "title": "Fix for get() follow_parents exception handling (#1014)", "body": "## Description\r\n\r\nWhen working with chains, the situation might be that the first chain task has failed. Calling `x.get(propagate=True, follow_parents=True)` should cope with that since it checks if the parent tasks have raised exceptions.\r\n\r\nHowever, in my experience, when `x.get(propagate=True, follow_parents=True)` is called the failed task is still pending at that moment and will become failed seconds later. This creates a bug in the behaviour of `follow_parents`.\r\n\r\nThis fix calls `get(propagate=True, follow_parents=False)` on each parent instead, which will cause it to raise directly when the parent task becomes failed. It looks like `maybe_throw()` can be safely replaced since it is called by `get()` at some other place.\r\n\r\nIn order to be more comprehensive, I extended the documentation for `follow_parents`.\r\n\r\nI am not experienced enough in this project to see the whole picture, so please advise on possible problems that might arise with this fix for #1014 .\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Checkroth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4285", "title": "Add failing test for broker url priority ref issue #4284", "body": "## Description\r\nThis PR is to assist in showing the issue described here: https://github.com/celery/celery/issues/4284\r\n\r\nAll this PR does is add a failing test that _should_ be passing, if the issue above is resolved.\r\n\r\nThis PR does not solve the issue mentioned. I leave that up to the discretion of a more knowledgeable party, if they decide that it is indeed an issue at all. I do not expect this PR to be merged unless the issue is resolved and you want this test to remain in the repository.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jamesmallen": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4262", "title": "Disable backend for beat", "body": "## Description\r\n\r\nWhen using `beat` (at least in its standalone form), it is not necessary to subscribe to events on the result backend. These subscriptions happen in the `on_task_call` method of the backend. This PR ensures that no `SUBSCRIBE` messages are sent.\r\n\r\nFixes #4261 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bluestann": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4241", "title": "Use Cherami as a broker", "body": "Hi, \r\n\r\nI have added support for celery to use [Cherami](https://eng.uber.com/cherami/) as a broker. \r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "harukaeru": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4239", "title": "Changed raise RuntimeError to RuntimeWarning when task_always_eager is True", "body": "## Description\r\nThis PR changes RuntimeError to RuntimeWarning when `task_always_eager` is True.\r\n\r\nSometimes I tested what is related to celery task using `task_always_eager` in celery v3.\r\n(The tests are not only celery task but codes using celery task and other codes)\r\n\r\nI know the config cannot be completely tested async task but it's useful when doing rush works.\r\nI never use them in the production code but in the test, I use.\r\n\r\nI read the issue (https://github.com/celery/celery/issues/2275) was produced and the commit (https://github.com/celery/celery/commit/c71cd08fc72742efbfc846a81020939aa3692501) resolved the above.\r\n\r\n\r\nI almost agree with them but people who want to test perfectly only don't turn on `task_always_eager`.\r\nOthers who want to test synchronously also want to use `task_always_eager`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "erebus1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4227", "title": "fix crontab description of month_of_year behaviour", "body": "\r\n## Description\r\nIn [docs](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#crontab-schedules) said:\r\n\r\n> crontab(0, 0, month_of_year='*/3') -> Execute on the first month of every quarter.\r\n\r\nWhich is a bit confused, because, in fact it will run task each day on the first month of every quarter.\r\n\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rpkilby": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4212", "title": "Fix celery_worker test fixture", "body": "This is an attempt to fix #4088. Thanks to @karenc for providing a [breakdown](https://github.com/celery/celery/issues/4088#issuecomment-321287239) of what's happening.\r\n\r\nSide note:\r\nUsing `celery_worker` over `celery_session_worker` in the integration tests is a bit slower, given the worker startup/teardown for each test. This could be faster if the worker was scoped at a module level, but it's not possible to change the scope a fixture. The [recommendation](https://github.com/pytest-dev/pytest/issues/2300) is to simply create a fixture per scope. \r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bbgwilbur": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4077", "title": "Catch NotRegistered exception for errback", "body": "If the errback task is not registered in the currently running worker, the arity_greater check will fail. We can just assume its going to work as an old-style signature and deal with the possible error if it doesn't later.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChillarAnand": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4013", "title": "Updated commands to kill celery workers", "body": "```\r\nchillar+  1696 26093  1 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:MainProcess] -active- (worker -l info -A t)\r\nchillar+  1715  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-1]\r\nchillar+  1716  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-2]\r\nchillar+  1717  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-3]\r\nchillar+  1718  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-4]\r\n```\r\nWith latest version, celery worker process names seems changed. So, the commands used to kill those process needs to be updated.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Taywee": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3852", "title": "celery.beat.Scheduler: fix _when to make tz-aware (#3851)", "body": "Fixes #3851 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JackDanger": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3838", "title": "Removing last references to task_method", "body": "AFAICT the code removed in this PR only served to support the\n(now-removed) celery.contrib.methods.task_method() function.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gugu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3834", "title": "Handle ignore_result", "body": "## Description\r\n\r\nCurrently (according to my checks and code grep) ignore_result option is a stub, it does nothing. This patch skips backend calls for tasks with `ignore_result=True`\r\n\r\nThis is needed to minimize effect of broken redis backend support", "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3815", "title": "Chord does not clean up subscribed channels in redis. Fixes #3812", "body": "This pull requests fixes issue, when chord gets result for subtasks, but does not do `UNSUBSCRIBE` command for them.\r\n\r\n**What does this patch fix**:\r\n\r\nWithout this patch `chord(task() for i in range(50))` creates 51 subscriptions. After patch it will create 1\r\n\r\n**What this patch does not fix**:\r\n\r\nEvery task, which result is not consumed, creates redis subscription. Even if `ignore_result` is specified. I plan to submit separate patch for this issue\r\n\r\nI think, that changing old slow redis behavior to new fast and broken was not a good idea, but as soon as choice made, we need to make it usable", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "justdoit0823": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3757", "title": "Add supporting for task execution working with tornado coroutine", "body": "With this future, we can use tornado coroutine in celery app task when pool implementation is gevent. The main idea here is using a standalone tornado ioloop thread to execute coroutine task. If a task is a coroutine, the executor will register a callback on the tornado ioloop and switch to the related gevent hub. Here using a callback means that the coroutine will be totally executed inside the tornado ioloop thread. When the coroutine is finished, the executor will spawn a new greenlet which will switch to the previous task greenlet with the coroutine's result. Then task greenlet returns the result to the outer function inside the same greenlet. \r\n\r\nWe can write code in celery task as following:\r\n\r\n\r\n```python\r\nfrom celery import app\r\nfrom tornado import gen\r\n\r\n@app.task\r\n@gen.coroutine\r\ndef task_A():\r\n\r\n    process_1()\r\n    res = yield get_something()\r\n    do_something_with_res()\r\n    return res\r\n\r\n@gen.coroutine\r\ndef get_something():\r\n\r\n    res = {}\r\n    return res\r\n```\r\n\r\nThis is interesting when we use the tornado framework to write a project, we can easily divide partial logic into a celery task without any modifications. And in tornado 5.0, which will support Python 3 native coroutine, this future can connect more things together. I think people will like this.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "regisb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3684", "title": "Refer worker request info to absolute time", "body": "Previously, the time_start attribute of worker request objects refered\r\nto a timestamp relative to the monotonic time value. This caused\r\ntime_start attributes to be at a time far in the past. We fix this by\r\ncalling the on_accepted callback with an absolute time_accepted\r\nattribute.\r\n\r\nNote that the current commit does not change the\r\ncelery.concurrency.base API, although it would probably make sense to\r\nrename the \"monotonic\" named argument to \"time\".\r\n\r\nThis fixes the problem described in http://stackoverflow.com/questions/20091505/celery-task-with-a-time-start-attribute-in-1970/\r\n\r\nNote that there are many different ways to solve this problem; in the proposed implementation I choose to modify the default value of a keyword argument that is AFAIK never used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcsaaddupuy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3592", "title": "fixes exception deserialization when not using pickle", "body": "fix for #3586 \r\n\r\nThis PR try to fix how exceptions are deserialized when using a serializer different than pickle.\r\n\r\nThis avoid to [create new types](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L46) for exceptions, by doing 2 things : \r\n\r\n- store the exception module in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L243-L245) and reuse it in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L249-L258) instead of using raw `__name__` as module name\r\n\r\n- in [ celery/celery/utils/serialization.py::create_exception_cls](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L86-L98), try to find the exception class either from  `__builtins__`, or from the excetion module. Fallback on current behavior (which may still be wrong)\r\n\r\nAlso, it uses `exc.args` in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L247) and pass it to the exception constructor in [celery/celery/backends/base.py::Backend::exception_to_python](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L255), instead of using `str(exc.message)` which could lead to unwanted behavior\r\n\r\nThis won't work in every cases. If a class is defined locally in a function, this code won't be able to import the exception class using `import_module` and the old (wrong) behavior will still be used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "astewart-twist": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3293", "title": "Deserialize json string prior to inclusion in CouchDB doc", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chadrik": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3043", "title": "group and chord: subtasks are not executed until entire generator is consumed", "body": "When passing a generator to `group` or `chord`, sub-tasks are not submitted to the backend for execution until the entire generator is consumed.  The expected result is that subtasks are submitted during iteration, as soon as they are yielded.  This can have a big impact on performance if generators yield subtasks over a long period of time.\n\nI also opened issue #3021 on the subject.  [This explanation](https://github.com/celery/celery/issues/3021#issuecomment-176729202) from @eli-green I think is pretty on point.\n\nSo far I've only added support for redis.  I started looking at the other backends but I ran out of time.  It would be great to get some feedback on what I have so far and to get some thoughts on how difficult it will be to add for the other backends.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "m4ddav3": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2881", "title": "Database Backend: Use configured serializer, instead of always pickling.", "body": "Use the BLOB as an sa.BLOB\nSerialise the result an add to the db as bytes\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ask": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15545", "body": "README: Fix typo \"task.register\" -> \"tasks.register\". Closed by 8b5685e5b11f8987ba56c28ccb47f6c139541384. Thanks gregoirecachet)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546", "body": "What version is this? I thought I had fixed this already.\n\nThe examples in README should be updated. I think the best way of defining tasks now is using a Task class:\n\n```\n from celery.task import Task\n\n class MyTask(Task):\n     name = \"myapp.mytask\"\n\n     def run(self, x, y):\n         return x * y\n```\n\nand then\n\n```\n>>> from myapp.tasks import MyTask\n>>> MyTask.delay(2, 2)\n```\n\nas this makes it easier later to define a default routing_key for your task etc.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205", "body": "This works now. I remember it didn't at some point, and I remember I fixed it, so unless it still doesn't work for you I'm closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554", "body": "oh, that's bad. Got to get rid of yadayada dependency anyway, it's been an old trashbag for utilities, and all that is used from it now is the PickledObjectField.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555", "body": "Remove yadayada dependency. that means we've copy+pasted the\nPickledObjectField, when will djangosnippets ever die? :( Closed by fb582312905c5a1e001b6713be78ee2154b13204. Thansk\ngregoirecachet!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182", "body": "I'm not sure if that's so bad. Test requirements is not the same as install requirements. Sad the Django test runner is so broken. Maybe I can get it in somewhere else. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353", "body": "Add the test-runner from yadayada into the repo so we don't depend on yadayada\nanymore. Closed by af9ba75e195fc740493c9af6dbe84105b369d640.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428", "body": "Seems to be fine now. We'll re-open the issue if anyone says otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255", "body": "Implemented by 048d67f4bfb37c75f0a5d3dd4d0b4e05da400185 +  89626c59ee3a4da1e36612449f43362799ac0305\nAnd it really _is fast_ compared to the database/key-value store backends which uses polling to wait for the result.\n\nTo enable this back-end add the following setting to your settings.py/celeryconf.py:\n\n```\nCELERY_BACKEND=\"amqp\"\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017", "body": "A sample implementation has been commited (794582ebb278b2f96080a4cf4a68f1e77c3b003b + 93f6c1810c1051f8bdea6a7eae21d111997388d00 + fe62c47cb04723af738192087b40caefd27cab6a ) but not tested yet. Currently seeking anyone willing to test this feature.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115", "body": "Task retries seems to be working with tests passing. Closed by 41a38bb25fcacb48ee925e4319d35af9ab89d2bf\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721", "body": "Use basic.consume instead of basic.get to receive messages. Closed by 72fa505c7dfcf52c3215c276de67e10728898e70. This\nmeans the CELERY_QUEUE_WAKEUP_AFTER and CELERY_EMPTY_MSG_EMIT_EVERY settings,\nand the -w|--wakeup-after command line option to celeryd has been removed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902", "body": "If delay() is hanging, I'm guessing it's not because of the database, but because it can't get a connection to the AMQP server. (amqplib's default timeout must be very high). Is the broker running? Are you running RabbitMQ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071", "body": "I set the default connection timeout to be 4 seconds. Closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627", "body": "Re-opening the issue as setting the amqplib connection timeout didn't resolve it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160", "body": "This is actually an issue with RabbitMQ and will be fixed in the 1.7 release. I added the following to the celery FAQ:\n\n RabbitMQ hangs if it isn't able to authenticate the current user,\nthe password doesn't match or the user does not have access to the vhost\nspecified.  Be sure to check your RabbitMQ logs\n(`/var/log/rabbitmq/rabbit.log` on most systems), it usually contains a\nmessage describing the reason.\n\nThis will be fixed in the RabbitMQ 1.7 release.\n\nFor more information see the relevant thread on the rabbitmq-discuss mailing\nlist: http://bit.ly/iTTbD\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924", "body": "AsyncResult.ready() was always True. Closed by 4775a4c279179c17784bb72dc329f9a9d442ff0a. Thanks talentless.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110", "body": "Oh. That's indeed a problem with the installation. Could you try to install it using pip?\n\n```\n$ easy_install pip\n$ pip install celery\n```\n\nI will fix it as soon as possible, but in the mean time you could use pip.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111", "body": "Only use README as long_description if the file exists so easy_install don't\nbreak. Closed by e8845afc1a53aeab5b30d82dea29de32eb46b1d6. Thanks tobycatlin\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169", "body": "The consumerset branch was merged into master in c01a9885bbb8c83846b3770364fe208977a093fd (original contribution: screeley/celery@e2d0a56c913c66f69bf0040c9b76f74f0bb7dbd8). Big thanks to Sean Creeley.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472", "body": "Fixed in  faaa58ca717f230fe8b65e4804ad709265b18d5a\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432", "body": "By the way; This worked before we started using auto_ack=True, and basic.get.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448", "body": "Wait with message acknowledgement until the task has actually been executed.\nClosed by ef3f82bcf3b4b3de6584dcfc4b189ddadb4f50e6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017", "body": "This now merged into master. On second thought Munin plugins for these stats doesn't make sense, at least not generally. You could use this to make munin-plugins though, it's more like the groundwork for something more interesting later. (and it's useful for profiling right away)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966", "body": "Make TaskSet.run() respect message options from the Task class. Closed by 03d30a32de3502b73bca370cb0e70863c0ad3dd2.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29867", "body": "Thanks for pointing that out. It's a bug, indeed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38526", "body": "I added the importlib backport module as a dependency, is that >= 2.6 only? I guess I could add django==1.1 as a dependency, but I think there's someone using 1.0.x still.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38984", "body": "2.7 you mean? Yeah, but I added http://pypi.python.org/pypi/importlib as a dependency. Does it work with Python >= 2.4?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38985", "body": "Ah, I just saw the trove classifiers, all the way down to 2.3.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/51698", "body": "This commit was actually authored by me, just forgot to reset my `GIT_AUTHOR_NAME`, and `GIT_AUTHOR_EMAIL` env variables.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/51698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/52587", "body": "damn. Seems I forgot to reset my GIT_AUTHOR_\\* settings :(\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/52587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/82477", "body": "Ah, ok. That's good to know! I wasn't sure about this. Also, I'm wondering what holds the connection, is it the engine or the session?\nCreating a new connection for every operation is probably a bad idea, but not sure if it does some connection pooling by default.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91900", "body": "lol, yeah. I guess this is borderline.\n\nYou have the ability to supply your own connection, if you do we can't close the connection for you.\n\nAbove it says:\n\n```\nconn = connection or establish_connection(connect_timeout=connect_timeout)\n```\n\nso `connection or conn.close`, is \"if user didn't supply a connection, close the connection we established\".\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91937", "body": "Forgot that it's using the `@with_connection decorator` which takes care of this automatically :/\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/352668", "body": "Thanks!  Fixed in 154431f2c4ff04515000462ede70e205672e1751\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280", "body": "Is this deliberate?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588", "body": "Why did you remove the `Content-Type`?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074", "body": "Think there's a race here if it enters `time.sleep()`, and the putlock is released when the pools state != RUN.  The task will be sent to the queue in this case.  I'll fix it before I merge.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078", "body": "Also changed the interval to 1.0 as we discussed on IRC.  The shutdown process is already almost always delayed by at least one second because of other thread sleeps, so it doesn't make a difference (apart from less CPU usage in online mode).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181", "body": "Actually, the Pool implementation shouldn't depend on Celery, it's more of a patch for fixes and features we need for multiprocessing.\nSo this option should be added to `Pool.__init__`, then passed on from `celery.worker.WorkController`, then `celery.concurrency.processes.TaskPool`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183", "body": "and, oh yeah, `celery.conf` is deprecated to be removed in 3.0, so you don't have to add anything to it.\n\nWhen added to `WorkController`, it works in the case where you instantiate the worker without configuration too,\ne.g: `WorkController(worker_lost_wait=20)`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375", "body": "Ah, this was just a typo, I copied the document...\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380", "body": "It's on my TODO, I have to write it\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043", "body": "You can't terminate a reserved task, as it hasn't been executed yet.\n\nIt adds the id of the task to the list of revoked tasks, this is then checked again when the worker is about to execute the task (moves from reserved -> active).  If you only search the reserved requests then the terminate functionality would be useless (it wouldn't have any process to kill)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052", "body": "the other changes look great\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034", "body": "Revoked check for reserved/ETA tasks happens here:\nhttps://github.com/celery/celery/blob/3.0/celery/worker/job.py#L185-186\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089", "body": "> Also, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n\nStill, you shouldn't terminate a reserved task.  If a reserved task is revoked it should be dropped _before_ any pool worker starts to execute it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191", "body": "Hmm, maybe we could remove the `set` here, so that it preserves the original order.\n\nIt's not terribly bad if it imports the same module twice after all\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552", "body": "For Python 2.6 you have to include positions: {0} {1} {2}\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484", "body": "Celery related code should not be called by this method as it should be decoupled from Celery.  You need to find a way to support this by exposing it in the API.\nE.g. `Worker(on_shutdown=signals.worker_process_shutdown.send)`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390", "body": "Since created is True, it would also call 'on_node_join' in this instance, even if it just left.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331", "body": "Should this be logged?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "gcachet": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15549", "body": "It's master. I figured out this way to define tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557", "body": "Thanks! Removing yadayada dependency was my first though also, but I wasn't sure about the implications.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "talentless": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18501", "body": "No problem!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tobycatlin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18135", "body": "pip installed ok. I haven't tried the source code out of git. \n\nThanks for the quick response\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "brosner": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18434", "body": "My best guess is we've called terminate more than once? Will need some investigation.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "nikitka": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29856", "body": "why after **import** you use import celeryconfig ? this is work?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "brettcannon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/38473", "body": "Django as of (I believe) 1.1 has importlib included w/ it under django.util.importlib, so you don't need to rely on Python 2.6 to get import_module.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/38959", "body": "importlib was added in Python 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "paltman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/79488", "body": "This is why the test now fails -- now that is no longer 29 minutes past the hour, but 30, it is now due to run, while the assertion was to make sure it handled the case when it wasn't due, which when the mocked value was 29 minutes past the hour it properly returned False.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/79488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/204916", "body": "Was there a test for this failure before the fix?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/204916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nvie": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/81557", "body": "Nice and tidy.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/81557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "haridsv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/82469", "body": "Is this creating a new engine every time a new session is needed? As per sqlalchemy documentation, engine should be created only one time, unless of course the connection URL itself is changing.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/82478", "body": "Yes, the engine by default has a connection pool enabled: http://www.sqlalchemy.org/docs/05/reference/sqlalchemy/pooling.html#connection-pool-configuration\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jonozzz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/91703", "body": "I don't quite understand this one... why \"connection\" and why \"or\" ?\nI think it should be:\n    conn and conn.close()\n\nBecause when connection is None, conn.close() will be executed anyway.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "simonz05": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/103991", "body": "Nice, the previous one was aweful. Good to have the side-bar back.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/103991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Kami": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/104086", "body": "I agree, great work.\n\nThe blue (first?) version wasn't bad either, but the previous version was kinda step in the wrong direction.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/104086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adamn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/123743", "body": "$VIRTUALENV was removed - does it still work with a virtualenv?  It doesn't seem to for me on Ubuntu 10.04\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/123743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zen4ever": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/131915", "body": "Thanks for the fix. That was fast.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/131915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "joshdrake": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136184", "body": "Was just about to post an issue on this. I was surprised by this too, but even subclasses of accepted types must have adapters registered for database backends. Here's documentation on the process for pyscopg2:\n\nhttp://initd.org/psycopg/docs/advanced.html#adapting-new-types\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zzzeek": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136191", "body": "you need to look at TypeDecorator:\n\nhttp://www.sqlalchemy.org/docs/reference/sqlalchemy/types.html?highlight=typedecorator#sqlalchemy.types.TypeDecorator\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "passy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/147340", "body": "The dot should be inside the comment. It's a syntax error otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/147340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dcramer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/175143", "body": "I should note I have no idea what this does, I stole it form somewhere on the internet and it fixed the problems :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/175143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "shimon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/270710", "body": "Why did the date_done entry get removed? This is useful information that other backends seem to provide.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/270710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "enlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/352650", "body": "this should be _kill(...)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mher": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/595269", "body": "https://bitbucket.org/jezdez/sphinx-pypi-upload/issue/1/minor-patch-for-namespace-modules-etc patch should be applied before launching  paver upload_pypi_docs. upload_pypi_docs fails if .build contains empty subdirectories.\n\nIs there a way to automate this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/595269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810", "body": "I think it would be better to move registry._set_default_serializer('auth') to setup_security\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "steeve": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600", "body": "yes, this allows for the result backend to sub on this, allowing it not to poll\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "mitar": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921", "body": "As I explained, the content is urlencoded combination of parameters, not JSON. Why it would be `application/json`? We do not serialize to JSON (at this stage) anywhere. But urllib does urlencode it. If we remove manual override, then urllib does the right thing and sets it to `application/x-www-form-urlencoded` (it also sets `Content-Length` properly). This can Django (or any other receiver) then properly decode. So, urllib sends content as `application/x-www-form-urlencoded` and header should match that.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brendoncrawford": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182", "body": "Ok, I'll submit a new patch within the next week or so.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188", "body": "Ok, ill take a stab at it. Might take a few tries, but I think I can get it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ztlpn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747", "body": "Well I have not been able to check it with the latest version yet, but at least on version 3.0.6 this does not happen! If reserved but not yet active task is revoked, no check against revoked list is made when task becomes active. Instead it executes normally.\n\nAlso, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ambv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809", "body": "SIGTERM is not 9.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810", "body": "Ditto.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553", "body": "This style of string formatting is only available to Python 2.7+. I believe Celery is still supporting 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "VRGhost": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911", "body": "Well, it should be\n\n```\nwarnings.warn(\"%s consumed store_result call with args %s, %s\" % (self.__class__.__name__, args, kwargs))\n```\n\nthan. :-)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "dmtaub": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779", "body": "Ok, I will work on an api-based version soon. Depending on whether I need\nto write zmq interprocess communication this week, it might end up being\nless important to merge our branches at this particular moment :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "andrewkittredge": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195", "body": "this is wrong.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ionelmc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835", "body": "I added this to help with debugging #1785. I still think we should have it (it could indicate other problems).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}}, "3": {"xunto": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4491", "title": "[BUG] ready() always returns False for groups", "body": "## Steps to reproduce\r\n\r\n```python\r\nsent = group(\r\n    celery_app.signature('task1', args=arg),\r\n    celery_app.signature('task2', args=arg),\r\n    celery_app.signature('task3', args=arg)\r\n).apply_async()\r\n\r\nwhile not sent.ready():\r\n    pass\r\n\r\nprint(\"test\")\r\n```\r\n\r\n## Expected behavior\r\nI expect ```ready()``` to return ```True``` when all tasks are finished.\r\n\r\n## Actual behavior\r\nLooks like this code will never finish execution as ```ready()``` always return ```False``` even if all tasks are finished. Tested on celery master branch.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nitinmeharia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4489", "title": "django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread", "body": "### Application details:\r\n```\r\n    software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.4\r\n            billiard:3.5.0.3 redis:2.10.6\r\n    platform -> system:Darwin arch:64bit imp:CPython\r\n    loader   -> celery.loaders.app.AppLoader\r\n    settings -> transport:redis results:redis://localhost:6379/2\r\n```\r\n\r\n### Error log\r\n```\r\n    Signal handler <bound method DjangoWorkerFixup.on_worker_process_init of <celery.fixups.django.DjangoWorkerFixup object at 0x108e46fd0>> raised: DatabaseError(\"DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\",)\r\n    Traceback (most recent call last):\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/utils/dispatch/signal.py\", line 227, in send\r\n        response = receiver(signal=self, sender=sender, **named)\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 154, in on_worker_process_init\r\n        self._close_database()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 186, in _close_database\r\n        conn.close()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 283, in close\r\n        self.validate_thread_sharing()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 542, in validate_thread_sharing\r\n        % (self.alias, self._thread_ident, thread.get_ident())\r\n    django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\r\n```\r\n\r\n## Steps to reproduce\r\n```\r\ncelery multi restart w1 -A proj -l info\r\n```\r\nIts a django 1.11 application where we are getting this error.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kurara": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4488", "title": "launching worker from python: error with configuration from object.", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nVersion celery: 4.1.0 (latentcall)\r\n\r\nI'm trying to launch a worker from python code. When I use the class CeleryCommand with 'worker' option, it works. But if I add the option '--detach' or 'multi' the broker configuration is wrong. The code is:\r\n\r\n```\r\napp.config_from_object(config_module)\r\ncelerycmd = CeleryCommand(app)\r\ncelerycmd.execute_from_commandline(argv=[prog_name, 'worker', 'app_name', pidfile, logfile, loglevel])\r\n```\r\nor (which is not working)\r\n\r\n`celerycmd.execute_from_commandline(argv=[prog_name, 'multi', 'start', 'app_name', pidfile, logfile, loglevel])`\r\n\r\n# Expected behavior\r\nGet the same broker as in the configuration, not the default one\r\n\r\n## Actual behavior\r\n\r\nlog_file:\r\n\r\n```\r\n[2018-01-15 15:08:58,471: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2018-01-15 15:08:58,506: INFO/MainProcess] mingle: searching for neighbors\r\n[2018-01-15 15:08:59,647: INFO/MainProcess] mingle: all alone\r\n[2018-01-15 15:08:59,672: INFO/MainProcess] app_name@hostname ready.\r\n```\r\n\r\nFYI: I'm using lower case configuration. \r\n\r\nI could provide you what the object has in debug mode. Just ask me and I post it. When I debugged, I think the 'app' had the configuration of the file when it was inside the function _execute_from_commandline_, I can't understand when it loses it.\r\n\r\nI tought that maybe the problem is that I don't provide the configuration at the begining of the file, where app is declared, but in a function. So I tested to add the broker directly when I create the app: \r\n`app = Celery('app_name', broker_url='broker@...')`\r\nbut it didn't work either.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MShekow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4486", "title": "Memory hogging in client when using RPC (with RabbitMQ)", "body": "## Description\r\nIn my scenario I have a client program that puts thousands of tasks on the queue (`generate_data.delay()`). The workers produce a result that is of considerable size (suppose each result uses 1 MB of memory). The result is pickled back, and the client processes the results in some way, whenever results are available. In other words, once `AsyncResult.ready() == True`, I `get()` the result and do something with it.\r\nUsing `objgraph` I found out that celery never releases the result data.\r\n\r\n## My configuration\r\n```\r\nSoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.2\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Windows arch:32bit, WindowsPE imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:rpc:///\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\nresult_backend: 'rpc:///'\r\nresult_serializer: 'pickle'\r\ntask_serializer: 'pickle'\r\naccept_content: ['pickle']\r\n```\r\n\r\n\r\n## Steps to reproduce\r\nWorker program simply returns large objects, e.g.:\r\n```\r\nclass Data:\r\n    def __init__(self):\r\n        self.data = b'1' * 1024 * 1024\r\n\r\n@app.task\r\ndef generate_data() -> Data:\r\n    time.sleep(random.uniform(0, 0.2))\r\n    d = Data()\r\n    return d\r\n```\r\nThe client program retrieves results whenever they are available:\r\n```\r\nresults = {}\r\n\r\nfor i in range(2000):\r\n    async_result = generate_data.delay()\r\n    results[async_result] = True\r\n\r\nlogger.info(\"Put {} jobs on process queue!\".format(len(results)))\r\n\r\nresults_collected = 0\r\nwhile True:\r\n    # get results that are ready now\r\n    ready_results = [async_result for async_result, _ in results.items() if async_result.ready()]\r\n    if not ready_results:\r\n        time.sleep(10)\r\n        continue\r\n\r\n    results_collected += len(ready_results)\r\n\r\n    logger.info(\"Processing {} results. Got {} results so far\".format(len(ready_results), results_collected))\r\n    for ready_result in ready_results:\r\n        # we don't actually use the data - a real program would process the data somehow\r\n        result_data = ready_result.get()  # type: Data\r\n        del results[ready_result]\r\n\r\n    gc.collect()\r\n\r\n    # Exit loop once all results were processed:\r\n    if not results:\r\n        break\r\nlogger.info(\"Finished collecting all results\")\r\n```\r\n\r\n## Expected behavior\r\nWhen my client no longer has a reference to neither the actual data returned by `AsyncResult.get()`, nor the `AsyncResult` object itself, the memory of the data and the `AsyncResult` should be freed by celery.\r\n\r\n## Actual behavior\r\nMemory is being hogged so quickly that my client process (32-bit) dies soon. The reason is the huge size of `MESSAGE_BUFFER_MAX` (8192) and the oddly-hardcoded `bufmaxsize` (1000) in `BufferMap`. Are there any reasons for these huge buffers?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thedrow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4483", "title": "Add test coverage for #4356", "body": "#4356 is a critical fix for a bug that occurs when messages migrate between Celery 3 and Celery 4 clusters.\r\nDue to it's severity It was merged without proper test coverage.\r\nWe need to ensure this code is covered by the appropriate unit tests.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4434", "title": "Run the integration tests in a different build stage", "body": "We need to find a way to run the integration tests in a different build stage so that the unit tests will before them.\r\nThe unit tests are much quicker to execute and thus should run first to free up build resources for other contributors.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4423", "title": "Document that tasks are now documented automatically by celery", "body": "See https://github.com/celery/celery/pull/4422", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2547666a1ea13b27bc13ef296ae43a163ecd4ab3", "message": "Don't cover this branch as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/3af6a635cf90a4452db4e87c2326579ebad750c2", "message": "Merge branch 'master' into master"}, {"url": "https://api.github.com/repos/celery/celery/commits/bd0ed23c81b20fd75c0e2188fcf78e4d74898953", "message": "Use editable installs to measure code coverage correctly."}, {"url": "https://api.github.com/repos/celery/celery/commits/0a0fc0fbf698d30e9b6be29661e0d447548cc47c", "message": "Report coverage to terminal as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/4b4bf2a4eee65871d3e9c0e96d94b23aa2ee602c", "message": "Report coverage correctly (#4445)\n\n* Report coverage correctly.\r\n\r\nAs it turns out this repository does not report coverage to codecov at all since the path to the executables has changed at some point.\r\nThis should fix the problem.\r\n\r\n* Readd code coverage badge."}, {"url": "https://api.github.com/repos/celery/celery/commits/dbd59d9fc988ad0f04ae710c32066c5ff62374ef", "message": "Added bandit to the build matrix."}, {"url": "https://api.github.com/repos/celery/celery/commits/2ae00362179897968cfbd1fc8d61b648356769a7", "message": "Added bandit to lint for security issues."}, {"url": "https://api.github.com/repos/celery/celery/commits/56b94c327244fad4933706fbddc02eeea508d21a", "message": "Prettify test output."}, {"url": "https://api.github.com/repos/celery/celery/commits/ebd98fa4d36bb8003c2f46dbd16e9888af13720f", "message": "Parallel doc lints (#4435)\n\n* Bump sphinx.\r\n\r\n* Update copyright year. Mark the celerydocs & the celery.contrib.sphinx extensions as read_parallel_safe.\r\n\r\n* Install from git for now :(\r\n\r\n* Fix flake8 errors."}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4268", "title": "Add Var.CI integration", "body": "This pull request aims to demonstrate the power of [VarCI](https://var.ci/), the missing assistant for GitHub issues.\n\n--\n_Automated response by [Var.CI](https://var.ci)_ :robot:", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3982", "title": "Added failing test cases for #3885", "body": "The tests were contributed by @robpogorzelski\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nAttempt to fix #3885 without hurting eager execution of the canvas.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PromyLOPh": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4480", "title": "Application is not thread-safe", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.4\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:rpc:///\r\n\r\nresult_backend: 'rpc:///'\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\n```\r\n\r\n## Steps to reproduce\r\n\r\nUsing code from the user manual, but two threads running concurrently:\r\n\r\n```python\r\nfrom celery import Celery\r\nimport time\r\nfrom threading import Thread\r\n\r\napp = Celery('test', broker='amqp://guest@localhost//', backend='rpc://')\r\n\r\n@app.task(bind=True)\r\ndef hello(self, a, b):\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 50})\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 90})\r\n    time.sleep(1)\r\n    return 'hello world: %i' % (a+b)\r\n\r\nif __name__ == '__main__':\r\n    def run ():\r\n        handle = hello.delay (1, 2)\r\n        print (handle.get ())\r\n    t1 = Thread (target=run)\r\n    t2 = Thread (target=run)\r\n    t1.start ()\r\n    t2.start ()\r\n    t1.join ()\r\n    t2.join ()\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\nhello world: 3\r\nhello world: 3\r\n```\r\n\r\n## Actual behavior\r\n\r\nDifferent exceptions, depending on timing. For instance:\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 456, in channel\r\n    return self.channels[channel_id]\r\nKeyError: None\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 56, in start\r\n    self._connection.default_channel, [initial_queue],\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 821, in default_channel\r\n    self._default_channel = self.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 266, in channel\r\n    chan = self.transport.create_channel(self.connection)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/transport/pyamqp.py\", line 100, in create_channel\r\n    return connection.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 459, in channel\r\n    channel.open()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 432, in open\r\n    spec.Channel.Open, 's', ('',), wait=spec.Channel.OpenOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 468, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 473, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 252, in read_frame\r\n    payload = read(size)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 417, in _read\r\n    s = recv(n - len(rbuf))\r\nsocket.timeout: timed out\r\n```\r\nor\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 59, in start\r\n    self._consumer.consume()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 477, in consume\r\n    self._basic_consume(T, no_ack=no_ack, nowait=False)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 598, in _basic_consume\r\n    no_ack=no_ack, nowait=nowait)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/entity.py\", line 737, in consume\r\n    arguments=self.consumer_arguments)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 1564, in basic_consume\r\n    wait=None if nowait else spec.Basic.ConsumeOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 471, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 476, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 254, in read_frame\r\n    'Received {0:#04x} while expecting 0xce'.format(ch))\r\namqp.exceptions.UnexpectedFrame: Received 0x3c while expecting 0xce\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kzidane": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4471", "title": "DisabledBackend when starting flask --with-threads?", "body": "I'm trying to use Celery for one of my applications and experiencing a strange behavior that I'm not sure why it's caused.\r\n\r\nTo replicate, here's a simple Flask app:\r\n\r\n    # application.py\r\n    from celery.contrib.abortable import AbortableAsyncResult\r\n    from flask import Flask\r\n    from tasks import add\r\n\r\n    app = Flask(__name__)\r\n\r\n    @app.route(\"/\")\r\n    def index():\r\n        # start the task and return its id\r\n        return add.delay(42, 50).task_id\r\n\r\n\r\n    @app.route(\"/state/<task_id>\")\r\n    def result(task_id):\r\n        # return current task state\r\n        return AbortableAsyncResult(task_id).state\r\n\r\nand here's a Celery app and a task:\r\n\r\n    # tasks.py\r\n    from celery import Celery\r\n    from celery.contrib.abortable import AbortableTask\r\n\r\n\r\n    app = Celery(\r\n        \"tasks\",\r\n        # use sqlite database as result backend (also tried rpc://)\r\n        backend=\"db+sqlite:///celerydb.sqlite\",\r\n        broker=\"pyamqp://localhost\"\r\n    )\r\n\r\n    @app.task(bind=True, base=AbortableTask)\r\n    def add(self, x, y):\r\n        return x + y\r\n\r\nRunning the Celery worker:\r\n\r\n    $ celery -A tasks worker --loglevel=info\r\n     -------------- celery@7677a80760b4 v4.1.0 (latentcall)\r\n    ---- **** ----- \r\n    --- * ***  * -- Linux-4.10.0-42-generic-x86_64-with-debian-jessie-sid 2018-01-02 20:22:48\r\n    -- * - **** --- \r\n    - ** ---------- [config]\r\n    - ** ---------- .> app:         tasks:0x7f339a52dfd0\r\n    - ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n    - ** ---------- .> results:     sqlite:///celerydb.sqlite\r\n    - *** --- * --- .> concurrency: 8 (prefork)\r\n    -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n    --- ***** ----- \r\n     -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n    [tasks]\r\n      . tasks.add\r\n\r\n    [2018-01-02 20:22:48,585: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n    [2018-01-02 20:22:48,592: INFO/MainProcess] mingle: searching for neighbors\r\n    [2018-01-02 20:22:49,608: INFO/MainProcess] mingle: all alone\r\n    [2018-01-02 20:22:49,636: INFO/MainProcess] celery@7677a80760b4 ready.\r\n\r\n\r\nRunning the Flask app:\r\n\r\n    $ FLASK_APP=application.py flask run --with-threads\r\n      * Serving Flask app \"application\"\r\n      * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\r\n\r\n\r\nHitting `/` with `curl` starts the task and returns its id without any problems:\r\n\r\n    $ curl http://localhost:5000\r\n    f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nCelery's output at this point:\r\n\r\n    [2018-01-02 20:29:28,974: INFO/MainProcess] Received task: tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef]\r\n    [2018-01-02 20:29:29,000: INFO/ForkPoolWorker-1] Task tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef] succeeded in 0.02414485500776209s: 92\r\n\r\nBut trying to get the state of the task \r\n\r\n    $ curl http://localhost:5000/state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nresults in the following error:\r\n\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n\r\neven though the backend seems to be configured per the `backend` argument to `Celery` and its output? I also tried setting `CELERY_RESULT_BACKEND` and `result_backend` using `app.conf.update`, but no luck!\r\n\r\nWhat's interesting is that this problem disappears if I drop the `--with-threads` option from the `flask run` command. Any idea why this might be caused and how to work around it if possible?\r\n\r\nAdditional details:\r\n\r\n    $ celery --version\r\n    4.1.0 (latentcall)\r\n    $ flask --version\r\n    Flask 0.12.2\r\n    Python 3.6.0 (default, Oct 30 2017, 05:46:44) \r\n    [GCC 4.8.4]\r\n\r\nFull traceback:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1982, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1614, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1517, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/_compat.py\", line 33, in reraise\r\n    raise value\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1612, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1598, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n      File \"/root/application.py\", line 14, in result\r\n    return AbortableAsyncResult(task_id).state\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 436, in state\r\n    return self._get_task_meta()['status']\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 375, in _get_task_meta\r\n    return self._maybe_set_cache(self.backend.get_task_meta(self.id))\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/backends/base.py\", line 352, in get_task_meta\r\n    meta = self._get_task_meta_for(task_id)\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n    127.0.0.1 - - [02/Jan/2018 20:46:28] \"GET /state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef HTTP/1.1\" 500 -\r\n\r\nThank you!", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fnordian": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4465", "title": "Celery.close() leaks redis connections", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.3\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\n\r\n```python\r\nimport time\r\nfrom celery import Celery\r\n\r\ndef run_celery_task(taskname):\r\n    with Celery(broker='redis://redis:6379/0', backend='redis://redis:6379/0') as celery:\r\n        res = celery.send_task(taskname)\r\n        print(res)\r\n\r\nfor i in range(0, 100):\r\n    run_celery_task(\"test\")\r\n\r\ntime.sleep(100)\r\n```\r\n\r\n```bash\r\nnetstat -tn | grep 6379 | grep ESTABLISHED | wc -l\r\n```\r\n## Expected behavior\r\n\r\nWhen the `with Celery`-block terminates, I expect all connections to redis being closed.\r\n\r\n## Actual behavior\r\n\r\nNot all connections are closed. When the for-loop finishes, there > 100 open connections to redis.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kimice": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4464", "title": "Maximum recursion depth exceeded while calling a Python object", "body": "Hi, when I call celery apply_async function, sometimes it raise Exception like this. It seems like getting config failed. This bug can't always reappear. I guess celery may be not init correctly. I'm so confused with this bug.\r\n\r\ncelery==4.1.0\r\n\r\nI init celery with flask like this.\r\n\r\n```\r\ndef make_celery(app):\r\n    celery = Celery(app.import_name, backend=app.config['CELERY_RESULT_BACKEND'],\r\n                    broker=app.config['CELERY_BROKER_URL'])\r\n    celery.conf.update(app.config)\r\n    celery.config_from_object('App.celery_custom.celery_config')\r\n    TaskBase = celery.Task\r\n    class ContextTask(TaskBase):\r\n        abstract = True\r\n        def __call__(self, *args, **kwargs):\r\n            with app.app_context():\r\n                return TaskBase.__call__(self, *args, **kwargs)\r\n    celery.Task = ContextTask\r\n    return celery\r\n\r\ncelery_app = make_celery(flask_app)\r\n\r\n@celery_app.task(bind=True)\r\ndef checkInstance(self, a, b):\r\n    pass\r\n\r\ncheck_task = checkInstance.apply_async(args=['123', '123'], queue='123')\r\n```\r\n\r\n```\r\nLOG:\r\n[2017-12-26 08:42:58,659]: logs_util.py[line:64] [pid:25680] ERROR Traceback (most recent call last):\r\n  File \"./App/views/experiment_views.py\", line 377, in create_experiment_and_run\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/task.py\", line 521, in apply_async\r\n    if app.conf.task_always_eager:\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 431, in __getitem__\r\n    return getitem(k)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 280, in __getitem__\r\n    return mapping[_key]\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/utils/objects.py\", line 44, in __get__\r\n    value = obj.__dict__[self.__name__] = self.__get(obj)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 148, in data\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 911, in _finalize_pending_conf\r\n    conf = self._conf = self._load_config()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 921, in _load_config\r\n    self.loader.config_from_object(self._config_source)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 128, in config_from_object\r\n    obj = self._smart_import(obj, imp=self.import_from_cwd)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 146, in _smart_import\r\n    return imp(path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 106, in import_from_cwd\r\n    package=package,\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/imports.py\", line 100, in import_from_cwd\r\n    with cwd_in_path():\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 84, in helper\r\n    return GeneratorContextManager(func(*args, **kwds))\r\nRuntimeError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lexabug": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4462", "title": "INFO log messages land to stderr", "body": "I've set up a basic application with Django 2.0 and Celery 4.1.0 with debug task (as it described [here](http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html#using-celery-with-django)) and one custom task in application tasks module.\r\nMy module code look like this:\r\n```\r\n# Create your tasks here\r\nfrom __future__ import absolute_import, unicode_literals\r\nfrom celery import shared_task\r\nfrom django.conf import settings\r\nfrom celery.utils.log import get_task_logger\r\n\r\nlogger = get_task_logger(__name__)\r\n\r\n@shared_task(name='validate_user_email', ignore_result=True, bind=True)\r\ndef validate_user_email(self, user_id):\r\n    logger.info('%s email verified', user_id)\r\n```\r\n\r\nWhen I redirect streams (stdout and stderr) to different logs  (info.log and error.log) info.log is silent while error.log contains log entries with levels WARNING and INFO.\r\n\r\nCommand I execute celery with is: `celery -A email_validation worker -Q validate --concurrency 5 --maxtasksperchild 100 -l info > info.log 2> error.log`\r\n\r\nCelery config:\r\n```\r\nCELERY_BROKER_URL = '******************'\r\nCELERY_BROKER_HEARTBEAT = 900\r\nCELERY_BROKER_HEARTBEAT_CHECKRATE = 15\r\nCELERY_RESULT_BACKEND = 'amqp'\r\nCELERY_WORKER_PREFETCH_MULTIPLIER = 1\r\nCELERY_WORKER_MAX_TASKS_PER_CHILD = 100\r\nCELERY_TASK_ACKS_LATE = True\r\nCELERY_ENABLE_UTC = False\r\nCELERY_TIMEZONE = 'US/Eastern'\r\nCELERY_WORKER_DISABLE_RATE_LIMITS = True\r\nCELERY_EVENT_QUEUE_TTL = 1\r\nCELERY_EVENT_QUEUE_EXPIRES = 60\r\nCELERY_RESULT_EXPIRES = 3600\r\nCELERY_TASK_IGNORE_RESULT = True\r\nCELERY_WORKER_HIJACK_ROOT_LOGGER = True\r\n```\r\n\r\nHow can make celery to post INFO log messages to stdout? ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cajbecu": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4457", "title": "Connection to broker lost. Trying to re-establish the connection: OSError: [Errno 9] Bad file descriptor", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Software\r\ncelery==4.1.0\r\nkombu==4.1.0\r\namqp==2.2.2\r\nPython 3.6.1\r\nbroker: rabbitmq 3.6.14\r\nresult backend: redis\r\n\r\n## Steps to reproduce\r\n1. celery -A proj worker -Q Q1 --autoscale=10,1 -Ofair --without-gossip --without-mingle --heartbeat-interval=60 -n Q1\r\n2. celery lost connection to broker\r\n3. after restarting affected worker the connection is successfully re-established and the worker starts processing tasks\r\n\r\n## Expected behavior\r\ncelery should re-establish connection to broker\r\n\r\n## Actual behavior\r\ncelery tries to re-establish connection to broker but fails with this error message (which is repeated every second) until manually restarted:\r\n```\r\n[user] celery.worker.consumer.consumer WARNING 2017-12-18 00:38:27,078 consumer: \r\nConnection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/loops.py\", line 47, in asynloop\r\n    obj.controller.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/worker.py\", line 217, in register_with_event_loop\r\n    description='hub.register',\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 151, in send_all\r\n    fun(parent, *args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/components.py\", line 178, in register_with_event_loop\r\n    w.pool.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/prefork.py\", line 134, in register_with_event_loop\r\n    return reg(loop)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in register_with_event_loop\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in <listcomp>\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 207, in add_reader\r\n    return self.add(fds, callback, READ | ERR, args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 158, in add\r\n    self.poller.register(fd, flags)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/utils/eventio.py\", line 67, in register\r\n    self._epoll.register(fd, events)\r\nOSError: [Errno 9] Bad file descriptor\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4457/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thiagogalesi": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4454", "title": "Celery does not consider authSource on mongodb backend URLs", "body": "Version: Celery 4.0.2 (from looking at the changes since then it seems there is no change addressing this issue here: https://github.com/celery/celery/commits/master/celery/backends/mongodb.py )\r\n\r\n(Edit) Confirmed with the following versions as well:\r\namqp==2.2.2\r\nbilliard==3.5.0.3\r\ncelery==4.1.0\r\nkombu==4.1.0\r\npymongo==3.6.0\r\n\r\nCelery Report\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.8\r\n            billiard:3.5.0.3 py-amqp:2.2.1\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\n\r\n## Steps to reproduce\r\n\r\nGive Celery a Backend URL pointing to a MongoDB instance with authentication and username/password (user/pwd set on the Admin DB by default) in the format:\r\n\r\nmongodb://user:pass@your-server/your_db?authSource=admin\r\n\r\n(Please see http://api.mongodb.com/python/current/examples/authentication.html#default-database-and-authsource and http://api.mongodb.com/python/current/api/pymongo/mongo_client.html?highlight=authsource )\r\n\r\n## Expected behavior\r\n\r\nCelery authenticates the user in the admin database (this is the same as passing --authenticationDatabase to the mongo client or the same url to MongoClient)\r\n\r\n## Actual behavior\r\n\r\nCelery tries to authenticate the user on the your_db database (failing to authenticate)\r\n\r\n## Workaround (not recommended)\r\n\r\nChange the db on the URL to /admin (this db shouldn't be used to store arbitrary data normally)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yanliguo": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4451", "title": "Celery (4.1.0) worker stops to consume new message when actives is empty  ", "body": "Hi there,\r\n   I was using celery to dispatch some long running task recently, and finding a way to disable prefetch. Now the config is:\r\n\r\nacks_late = True\r\nconcurrency = 1\r\nprefetch_multiplier = 1\r\n-Ofair\r\n\r\nActually, workers are still prefetching tasks. And I also have a monitor job to revoke tasks when a task is stuck in reserved state for a long time (let's say 5 minutes) or the task is outputing valid result.  \r\n\r\nOne thing wired is that, some workers stopped consuming new tasks when the actives is empty and the reseved task is revoked.  Has anybody ever met with this issue ? any help will be appreciated.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4450", "title": "PENDING state, what if it meant just one thing?", "body": "According to docs, PENDING state has the following meaning:\r\n\r\n> Task is waiting for execution or unknown. Any task id that\u2019s not known is implied to be in the pending state.\r\n\r\nAre there plans to make pending mean one thing only? It is quite confusing to handle a state that can mean \"waiting\" or \"lost\". ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfresh613": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4449", "title": "ValueFormatError when processing chords with couchbase result backend", "body": "Hi it seems like when I attempt to process groups of chords, the couchbase result backend is consistently failing to unlock the chord when reading from the db:\r\n\r\n`celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()`\r\n\r\nThis behavior does not occur with the redis result backend, i can switch between them and see that the error unlocking only occurs on couchbase.\r\n\r\n## Steps to reproduce\r\nAttempt to process a chord with couchbase backend using pickle serialization.\r\n\r\n## Expected behavior\r\nChords process correctly, and resulting data is fed to the next task\r\n\r\n## Actual behavior\r\nCelery is unable to unlock the chord from the result backend\r\n\r\n## Celery project info: \r\n```\r\ncelery -A ipaassteprunner report\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.10\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:couchbase://isadmin:**@localhost:8091/tasks\r\n\r\ntask_serializer: 'pickle'\r\nresult_serializer: 'pickle'\r\ndbconfig: <ipaascommon.ipaas_config.DatabaseConfig object at 0x10fbbfe10>\r\ndb_pass: u'********'\r\nIpaasConfig: <class 'ipaascommon.ipaas_config.IpaasConfig'>\r\nimports:\r\n    ('ipaassteprunner.tasks',)\r\nworker_redirect_stdouts: False\r\nDatabaseConfig: u'********'\r\ndb_port: '8091'\r\nipaas_constants: <module 'ipaascommon.ipaas_constants' from '/Library/Python/2.7/site-packages/ipaascommon/ipaas_constants.pyc'>\r\nenable_utc: True\r\ndb_user: 'isadmin'\r\ndb_host: 'localhost'\r\nresult_backend: u'couchbase://isadmin:********@localhost:8091/tasks'\r\nresult_expires: 3600\r\niconfig: <ipaascommon.ipaas_config.IpaasConfig object at 0x10fbbfd90>\r\nbroker_url: u'amqp://guest:********@localhost:5672//'\r\ntask_bucket: 'tasks'\r\naccept_content: ['pickle']\r\n```\r\n### Additional Debug output\r\n```\r\n[2017-12-13 15:39:57,860: INFO/MainProcess] Received task: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2]  ETA:[2017-12-13 20:39:58.853535+00:00] \r\n[2017-12-13 15:39:57,861: DEBUG/MainProcess] basic.qos: prefetch_count->27\r\n[2017-12-13 15:39:58,859: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x10b410b90> (args:('celery.chord_unlock', 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', {'origin': 'gen53678@silo2460', 'lang': 'py', 'task': 'celery.chord_unlock', 'group': None, 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', u'delivery_info': {u'priority': None, u'redelivered': False, u'routing_key': u'celery', u'exchange': u''}, 'expires': None, u'correlation_id': 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', 'retries': 311, 'timelimit': [None, None], 'argsrepr': \"('90c64bef-21ba-42f9-be75-fdd724375a7a', {'chord_size': 2, 'task': 'ipaassteprunner.tasks.transfer_data', 'subtask_type': None, 'kwargs': {}, 'args': (), 'options': {'chord_size': None, 'chain': [...], 'task_id': '9c6b5e1c-2089-4db7-9590-117aeaf782c7', 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', 'reply_to': '0a58093c-6fdd-3458-9a34-7d5e094ac6a8'}, 'immutable': False})\", 'eta': '2017-12-13T20:39:58.853535+00:00', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', u'reply_to':... kwargs:{})\r\n[2017-12-13 15:40:00,061: DEBUG/MainProcess] basic.qos: prefetch_count->26\r\n[2017-12-13 15:40:00,065: DEBUG/MainProcess] Task accepted: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] pid:53679\r\n[2017-12-13 15:40:00,076: INFO/ForkPoolWorker-6] Task celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()\r\n```\r\n\r\n### Stack trace from chord unlocking failure\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/builtins.py\", line 75, in unlock_chord\r\n    raise self.retry(countdown=interval, max_retries=max_retries)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/task.py\", line 689, in retry\r\n    raise ret\r\nRetry: Retry in 1s\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yutkin": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4438", "title": "Celery doesn't write RECEIVED state into MongoDB", "body": "When a number of tasks in a queue surpass a number of workers, new added tasks are not writing in a backend. In other words, I want to write task state (RECEIVED) into DB immediately after its invocation. It is possible?\r\n\r\nP.S. I'm using MongoDB as backend. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "canassa": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4426", "title": "Task is executed twice when the worker restarts", "body": "Currently using Celery 4.1.0\r\n\r\n## Steps to reproduce\r\n\r\nStart a new project using RabbitMQ and register the following task:\r\n\r\n```python\r\nfrom django.core.cache import cache\r\n\r\n@shared_task(bind=True)\r\ndef test_1(self):\r\n    if not cache.add(self.request.id, 1):\r\n        raise Exception('Duplicated task {}'.format(self.request.id))\r\n```\r\n\r\nNow start 2 workers. I used gevent with a concurrency of 25 for this test:\r\n\r\n```\r\ncelery worker -A my_proj -Q my_queue -P gevent -c 25\r\n```\r\n\r\nOpen a python shell and fire a a bunch of tasks:\r\n\r\n```python\r\nfrom myproj.tasks import test_1\r\n\r\nfor i in range(10000):\r\n    test_1.apply_async()\r\n```\r\n\r\nNow quickly do a warm shutdown (Ctrl+c) in one of the workers while it's still processing the tasks, you should see the errors popping in the second worker:\r\n\r\n```\r\nERROR    Task my_proj.tasks.test_1[e28e6760-1371-49c9-af87-d196c59375e9] raised unexpected: Exception('Duplicated task e28e6760-1371-49c9-af87-d196c59375e9',)\r\nTraceback (most recent call last):\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/code/scp/python/my_proj/tasks.py\", line 33, in test_1\r\n    raise Exception('Duplicated task {}'.format(self.request.id))\r\nException: Duplicated task e28e6760-1371-49c9-af87-d196c59375e9\r\n```\r\n\r\n## Expected behavior\r\n\r\nSince I am not using late acknowledgment and I am not killing the workers I wasn't expecting the tasks to execute again.\r\n\r\n## Actual behavior\r\n\r\nThe tasking are being executed twice, this is causing some problems in our servers because we restart our works every 15 minutes or so in order to avoid memory leaks.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kn-id": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4424", "title": "Celery Crash: Unrecoverable error when using QApplication in main process", "body": "Error happens when there's some queues already added before worker started and max task per child is 1\r\n\r\n## Checklist\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.12\r\n                         billiard:3.5.0.3 py-amqp:2.2.2\r\n      platform -> system:Linux arch:64bit, ELF imp:CPython\r\n      loader   -> celery.loaders.app.AppLoader\r\n      settings -> transport:amqp results:disabled\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n1. create instance of QApplication when main worker started\r\n```\r\n@celeryd_after_setup.connect\r\nfrom PyQt4.QtGui import QApplication\r\ndef init_worker(sender, **k):\r\n    QApplication([])\r\n```\r\n2. create task\r\n```\r\n@app.task\r\ndef job():\r\n    print 'hello'\r\n```\r\n3. add some queues (2 - 3 per thread. so if there's 4 worker child then there's 8 or more queues)\r\n\r\n4. start worker with max task per child 1\r\n```\r\ncelery worker -A proj --loglevel=INFO --max-tasks-per-child=1\r\n```\r\n\r\n## Expected behavior\r\n- run queues successfuly\r\n## Actual behavior\r\nCelery Crash after 1 queue per child\r\n```\r\n2017-12-07 10:20:19,594: INFO/MainProcess] Received task: proj.tasks.job[c6413dc2-ad62-4033-a69b-39556276f789]  \r\n[2017-12-07 10:20:19,595: INFO/MainProcess] Received task: proj.tasks.job[f1c10b1c-03ae-4220-9c7f-2cdf4afc61e3]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[d54f4554-4517-470f-8e14-adedcb93a46e]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[6255e5e6-d4c8-4d87-8075-642bca9e6a6d]  \r\n[2017-12-07 10:20:19,700: INFO/ForkPoolWorker-1] Task proj.tasks.job[ca856d5c-f3cc-45d4-9fbc-665753f5d1d2] succeeded in 0.00115608799388s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-4] Task proj.tasks.job[9ae27611-e8e5-4e08-9815-1e56e2ad1565] succeeded in 0.00130111300678s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-3] Task proj.tasks.job[f5aa7c6a-4142-4a38-8814-c60424196826] succeeded in 0.00129756200477s: None\r\n[2017-12-07 10:20:19,702: INFO/ForkPoolWorker-2] Task proj.tasks.job[eb13b5c5-8865-4992-8b9e-6672c909fd59] succeeded in 0.00100053900678s: None\r\n[2017-12-07 10:20:19,710: INFO/MainProcess] Received task: proj.tasks.job[01700061-c69c-4f4c-abf2-e6ba200772bd]  \r\n[2017-12-07 10:20:19,711: INFO/MainProcess] Received task: proj.tasks.job[a27d7a7b-2c58-4689-8b98-2c0a4ceaea9f]  \r\n[2017-12-07 10:20:19,713: INFO/MainProcess] Received task: proj.tasks.job[4c4a5685-23d5-4178-89cc-9ce4ad5a3509]  \r\n[2017-12-07 10:20:19,714: INFO/MainProcess] Received task: proj.tasks.job[44a079a3-aacf-48c5-a76b-a061bdced1d6]\r\n[2017-12-07 01:41:03,591: CRITICAL/MainProcess] Unrecoverable error: AttributeError(\"'error' object has no attribute 'errno'\",)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/async/hub.py\", line 354, in create_loop\r\n    cb(*cbargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 444, in _event_process_exit\r\n    self.maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1307, in maintain_pool\r\n    self._maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1298, in _maintain_pool\r\n    joined = self._join_exited_workers()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1165, in _join_exited_workers\r\n    self.process_flush_queues(worker)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 1175, in process_flush_queues\r\n    readable, _, _ = _select(fds, None, fds, timeout=0.01)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 183, in _select\r\n    if exc.errno == errno.EINTR:\r\nAttributeError: 'error' object has no attribute 'errno'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jenstroeger": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4420", "title": "How to unpack serialzied task arguments?", "body": "When iterate over all currently scheduled tasks\r\n```python\r\nfor task in chain.from_iterable(my_app.control.inspect().scheduled().values()):\r\n    print(task)\r\n```\r\nI get a dictionary `task['request']` which contains a serialization of the tasks\u2019 arguments in `task['request']['args']` (and `'kwargs`). Both are strings:\r\n```\r\n'args': \"('5', {'a': 'b'})\",\r\n'kwargs': '{}', \r\n```\r\nIt\u2019s not [JSON](https://www.json.org/) nor [msgpack](https://msgpack.org/). How can I unpack that `args` string into a Python tuple again? Celery must have a helper function for that somewhere? (Anything to do with `argsrepr` and `kwargsrepr` and [`saferepr`](https://github.com/celery/celery/blob/master/celery/utils/saferepr.py)?)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Chris7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/ba2dec7956782c84068ef779e554fb07de524beb", "message": "Propagate arguments to chains inside groups (#4481)\n\n* Remove self._frozen from _chain run method\r\n\r\n* Add in explicit test for group results in chain"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4482", "title": "Add docker-compose and base dockerfile for development", "body": "This adds a docker based development environment. This removes the need for users to install their own rabbit/redis/virtual environment/etc. to begin development of celery. I use this myself (since after moving to docker I have essentially nothing installed on my computer anymore) and saw interest in it from issue #4334 so thought a PR may be appropriate to share my setup.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "auvipy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/028dbe4a4d6786d56ed30ea49971cc5415fffb4b", "message": "update version to 4.2.0"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4459", "title": "[wip] #3021 bug fix ", "body": "Fixes #3021", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zpl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/442f42b7084ff03cb730ca4f452c3a47d9b8d701", "message": "task_replace chord inside chord fix (fixes #4368) (#4369)\n\n* task_replace chord inside chord fix\r\n\r\n* Complete fix for replace inside chords with tests\r\n\r\n* Add integration tests for add_to_chord\r\n\r\n* Fix JSON serialisation in tests\r\n\r\n* Raise exception when replacing signature has a chord"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4302", "title": "Ignore celery.exception.Ignore on autoretry", "body": "Autoretry for task should ignore celery.exception.Ignore, which is generated by self.replace()\r\n\r\notherwise, it goes to infinite loop.\r\n\r\n```python\r\n@app.task(autoretry_for=(Exception,), default_retry_delay=1,\r\n          max_retries=None,\r\n          bind=True,acks_late=True)\r\ndef TaskA(self):\r\n    raise self.replace(TaskB.s()) # Always retrying because of replace\r\n\r\n```\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdufresne": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/5eba340aae2e994091afb7a0ed7839e7d944ee13", "message": "Pass python_requires argument to setuptools (#4479)\n\nHelps pip decide what version of the library to install.\r\n\r\nhttps://packaging.python.org/tutorials/distributing-packages/#python-requires\r\n\r\n> If your project only runs on certain Python versions, setting the\r\n> python_requires argument to the appropriate PEP 440 version specifier\r\n> string will prevent pip from installing the project on other Python\r\n> versions.\r\n\r\nhttps://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords\r\n\r\n> python_requires\r\n>\r\n> A string corresponding to a version specifier (as defined in PEP 440)\r\n> for the Python version, used to specify the Requires-Python defined in\r\n> PEP 345."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "freakboy3742": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a4abe149aa00b0f85024a6cac64fd984cb2d0a6b", "message": "Refs #4356: Handle \"hybrid\" messages that have moved between Celery versions (#4358)\n\n* handle \"hybrid\" messages which have passed through a protocol 1 and protocol 2 consumer in its life.\r\n\r\nwe detected an edgecase which is proofed out in https://gist.github.com/ewdurbin/ddf4b0f0c0a4b190251a4a23859dd13c#file-readme-md which mishandles messages which have been retried by a 3.1.25, then a 4.1.0, then again by a 3.1.25 consumer. as an extension, this patch handles the \"next\" iteration of these mutant payloads.\r\n\r\n* explicitly construct proto2 from \"hybrid\" messages\r\n\r\n* remove unused kwarg\r\n\r\n* fix pydocstyle check\r\n\r\n* flake8 fixes\r\n\r\n* correct fix for misread pydocstyle error"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "djw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9ce3df9962830a6f9e0e68005bdeec1092e314e4", "message": "Corrected the default visibility timeout (#4476)\n\nAccording to kombu, the default visibility timeout is 30 minutes.\r\n\r\nhttps://github.com/celery/kombu/blob/3a7cdb07c9bf75b54282274d711af15ca6ad5d9f/kombu/transport/SQS.py#L85"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alexgarel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0ffd36fbf9343fe2f6ef7744a14ebfbec5ac86b6", "message": "request on_timeout now ignores soft time limit exception (fixes #4412) (#4473)\n\n* request on_timeout now ignores soft time limit exception (closes #4412)\r\n\r\n* fix quality"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "georgepsarakis": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a7915054d0e1e896c9ccf5ff0497dd8e3d5ed541", "message": "Integration test to verify PubSub unsubscriptions (#4468)\n\n* [Redis Backend] Integration test to verify PubSub unsubscriptions\r\n\r\n* Import sequence for isort check\r\n\r\n* Re-order integration tasks import"}, {"url": "https://api.github.com/repos/celery/celery/commits/9ab0971fe28462b667895d459d198ef6dd761c89", "message": "Add --diff flag in isort in order to display changes (#4469)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pachewise": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c8f9b7fbab3fe8a8de5cbae388fca4edf54bf503", "message": "Fixes #4452 - Clearer Django settings documentation (#4467)\n\n* reword django settings section in first steps\r\n\r\n* anchor link for django admonition\r\n\r\n* mention django-specific settings config"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hclihn": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/3ca1a54e65762ccd61ce728b3e3dfcb622fc0c90", "message": "Allow the shadow kwarg and the shadow_name method to set shadow properly (#4381)\n\n* Allow the shadow kwarg and the shadow_name method to set shadow properly \r\n\r\nThe shadow_name option in the @app.task() decorator (which overrides the shadow_name method in the Task class) and the shadow keyword argument of Task.apply_async() don't work as advertised.\r\nThis moves the shadow=... out of the 'if self.__self__ is not None:' block and allows shadow to be set by the shadow keyword argument of Task.apply_async() or the shadow_name method in the Task class (via, say, the shadow_name option in the @app.task() decorator).\r\n\r\n* Added a test to cover calling shadow_name().\r\n\r\n* Sort imports.\r\n\r\n* Fix missing import."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AlexHill": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fde58ad677a1e28effd1ac13f1f08f7132392463", "message": "Run chord_unlock on same queue as chord body - fixes #4337 (#4448)"}, {"url": "https://api.github.com/repos/celery/celery/commits/25f5e29610b2224122cf10d5252de92b4efe3e81", "message": "Support chords with empty headers (#4443)"}, {"url": "https://api.github.com/repos/celery/celery/commits/7ef809f41c1e0db2f6813c9c3a66553ca83c0c69", "message": "Add bandit baseline file with contents this time"}, {"url": "https://api.github.com/repos/celery/celery/commits/fdf0928b9b5698622c3b8806e2bca2d134df7fa3", "message": "Add bandit baseline file"}, {"url": "https://api.github.com/repos/celery/celery/commits/10f06ea1df75f109bf08fb8d42f9977cabcd7e0e", "message": "Fix length-1 and nested chords (#4393 #4055 #3885 #3597 #3574 #3323) (#4437)\n\n* Don't convert single-task chord to chain\r\n\r\n* Fix evaluation of nested chords"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pokoli": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/63c747889640bdea7753e83373a3a3e0dffc4bd9", "message": "Add celery_tryton integration on framework list (#4446)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "azaitsev": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/83872030b00a1ac75597ed3fc0ed34d9f664c6c1", "message": "Fixed wrong value in example of celery chain (#4444)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matteius": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/976515108a4357397a3821332e944bb85550dfa2", "message": "make astimezone call in localize more safe (#4324)\n\nmake astimezone call in localize more safe; with tests"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "myw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/4dc8c001d063a448d598f4bbc94056812cf15fc8", "message": "Add Mikhail Wolfson to CONTRIBUTORS.txt (#4439)"}, {"url": "https://api.github.com/repos/celery/celery/commits/dd2cdd9c4f8688f965d7b5658fa4956d083a7b8b", "message": "Resolve TypeError on `.get` from nested groups (#4432)\n\n* Accept and pass along the `on_interval` in ResultSet.get\r\n\r\nOtherwise, calls to .get or .join on ResultSets fail on nested groups.\r\nFixes #4274\r\n\r\n* Add a unit test that verifies the fixed behavior\r\n\r\nVerified that the unit test fails on master, but passes on the patched version. The\r\nnested structure of results was borrowed from #4274\r\n\r\n* Wrap long lines\r\n\r\n* Add integration test for #4274 use case\r\n\r\n* Switch to a simpler, group-only-based integration test\r\n\r\n* Flatten expected integration test result\r\n\r\n* Added back testcase from #4274 and skip it if the backend under test does not support native joins.\r\n\r\n* Fix lint.\r\n\r\n* Enable only if chords are allowed.\r\n\r\n* Fix access to message."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "johnarnold": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4490", "title": "Add task properties to AsyncResult, store in backend", "body": "*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PauloPeres": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4484", "title": "Adding the CMDS for Celery and Celery Beat to Run on Azure WebJob", "body": "This is what worked for us!\r\n\r\n## Description\r\nCreated the .cmd files to be used on Azure.\r\nJust going into Web Jobs on Azure servers and creating a Continuous Web Job, and uploading the zip files of the celery should work.\r\nEach folder should have a separeted Web Job.\r\nAlso whoever use should take a look where their celery package is\r\n\r\n\r\nThis pull request don't fix any bugs, it's just a new \"Helper\" for the ones who want to use Celery into Azure servers.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zengdahuaqusong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4475", "title": "separate backend database from login database", "body": "by setting 'backend_database' in MONGODB_BACKEND_SETTINGS, users can separate backend database from login database. Which means they can authenticate mongodb with 'database', while data is actually writing to 'backend_database'\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4474", "title": "Replace TaskFormatter with TaskFilter", "body": "Replacing `TaskFormatter` with a `TaskFilter` as that can be more easily reused when overriding the logging system.\r\n\r\nI've left the `TaskFormatter` in but marked it as deprecated in case people are using it.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "neaket360pi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4472", "title": "Cleanup the mailbox's producer pool after forking", "body": "Fixes https://github.com/celery/celery/issues/3751\r\n\r\nIf the mailbox is used before forking the workers will\r\nnot be able to broadcast messages.  This is because the producer pool\r\ninstance on the mail box will be `closed.`  This fix will cause the\r\nmailbox to load the producer pool again after forking.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charettes": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4456", "title": "Perform a serialization roundtrip on eager apply_async.", "body": "Fixes #4008 \r\n\r\n/cc @AlexHill ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jurrian": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4442", "title": "Fix for get() follow_parents exception handling (#1014)", "body": "## Description\r\n\r\nWhen working with chains, the situation might be that the first chain task has failed. Calling `x.get(propagate=True, follow_parents=True)` should cope with that since it checks if the parent tasks have raised exceptions.\r\n\r\nHowever, in my experience, when `x.get(propagate=True, follow_parents=True)` is called the failed task is still pending at that moment and will become failed seconds later. This creates a bug in the behaviour of `follow_parents`.\r\n\r\nThis fix calls `get(propagate=True, follow_parents=False)` on each parent instead, which will cause it to raise directly when the parent task becomes failed. It looks like `maybe_throw()` can be safely replaced since it is called by `get()` at some other place.\r\n\r\nIn order to be more comprehensive, I extended the documentation for `follow_parents`.\r\n\r\nI am not experienced enough in this project to see the whole picture, so please advise on possible problems that might arise with this fix for #1014 .\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Checkroth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4285", "title": "Add failing test for broker url priority ref issue #4284", "body": "## Description\r\nThis PR is to assist in showing the issue described here: https://github.com/celery/celery/issues/4284\r\n\r\nAll this PR does is add a failing test that _should_ be passing, if the issue above is resolved.\r\n\r\nThis PR does not solve the issue mentioned. I leave that up to the discretion of a more knowledgeable party, if they decide that it is indeed an issue at all. I do not expect this PR to be merged unless the issue is resolved and you want this test to remain in the repository.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jamesmallen": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4262", "title": "Disable backend for beat", "body": "## Description\r\n\r\nWhen using `beat` (at least in its standalone form), it is not necessary to subscribe to events on the result backend. These subscriptions happen in the `on_task_call` method of the backend. This PR ensures that no `SUBSCRIBE` messages are sent.\r\n\r\nFixes #4261 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bluestann": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4241", "title": "Use Cherami as a broker", "body": "Hi, \r\n\r\nI have added support for celery to use [Cherami](https://eng.uber.com/cherami/) as a broker. \r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "harukaeru": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4239", "title": "Changed raise RuntimeError to RuntimeWarning when task_always_eager is True", "body": "## Description\r\nThis PR changes RuntimeError to RuntimeWarning when `task_always_eager` is True.\r\n\r\nSometimes I tested what is related to celery task using `task_always_eager` in celery v3.\r\n(The tests are not only celery task but codes using celery task and other codes)\r\n\r\nI know the config cannot be completely tested async task but it's useful when doing rush works.\r\nI never use them in the production code but in the test, I use.\r\n\r\nI read the issue (https://github.com/celery/celery/issues/2275) was produced and the commit (https://github.com/celery/celery/commit/c71cd08fc72742efbfc846a81020939aa3692501) resolved the above.\r\n\r\n\r\nI almost agree with them but people who want to test perfectly only don't turn on `task_always_eager`.\r\nOthers who want to test synchronously also want to use `task_always_eager`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "erebus1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4227", "title": "fix crontab description of month_of_year behaviour", "body": "\r\n## Description\r\nIn [docs](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#crontab-schedules) said:\r\n\r\n> crontab(0, 0, month_of_year='*/3') -> Execute on the first month of every quarter.\r\n\r\nWhich is a bit confused, because, in fact it will run task each day on the first month of every quarter.\r\n\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rpkilby": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4212", "title": "Fix celery_worker test fixture", "body": "This is an attempt to fix #4088. Thanks to @karenc for providing a [breakdown](https://github.com/celery/celery/issues/4088#issuecomment-321287239) of what's happening.\r\n\r\nSide note:\r\nUsing `celery_worker` over `celery_session_worker` in the integration tests is a bit slower, given the worker startup/teardown for each test. This could be faster if the worker was scoped at a module level, but it's not possible to change the scope a fixture. The [recommendation](https://github.com/pytest-dev/pytest/issues/2300) is to simply create a fixture per scope. \r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bbgwilbur": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4077", "title": "Catch NotRegistered exception for errback", "body": "If the errback task is not registered in the currently running worker, the arity_greater check will fail. We can just assume its going to work as an old-style signature and deal with the possible error if it doesn't later.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChillarAnand": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4013", "title": "Updated commands to kill celery workers", "body": "```\r\nchillar+  1696 26093  1 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:MainProcess] -active- (worker -l info -A t)\r\nchillar+  1715  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-1]\r\nchillar+  1716  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-2]\r\nchillar+  1717  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-3]\r\nchillar+  1718  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-4]\r\n```\r\nWith latest version, celery worker process names seems changed. So, the commands used to kill those process needs to be updated.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Taywee": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3852", "title": "celery.beat.Scheduler: fix _when to make tz-aware (#3851)", "body": "Fixes #3851 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JackDanger": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3838", "title": "Removing last references to task_method", "body": "AFAICT the code removed in this PR only served to support the\n(now-removed) celery.contrib.methods.task_method() function.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gugu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3834", "title": "Handle ignore_result", "body": "## Description\r\n\r\nCurrently (according to my checks and code grep) ignore_result option is a stub, it does nothing. This patch skips backend calls for tasks with `ignore_result=True`\r\n\r\nThis is needed to minimize effect of broken redis backend support", "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3815", "title": "Chord does not clean up subscribed channels in redis. Fixes #3812", "body": "This pull requests fixes issue, when chord gets result for subtasks, but does not do `UNSUBSCRIBE` command for them.\r\n\r\n**What does this patch fix**:\r\n\r\nWithout this patch `chord(task() for i in range(50))` creates 51 subscriptions. After patch it will create 1\r\n\r\n**What this patch does not fix**:\r\n\r\nEvery task, which result is not consumed, creates redis subscription. Even if `ignore_result` is specified. I plan to submit separate patch for this issue\r\n\r\nI think, that changing old slow redis behavior to new fast and broken was not a good idea, but as soon as choice made, we need to make it usable", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "justdoit0823": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3757", "title": "Add supporting for task execution working with tornado coroutine", "body": "With this future, we can use tornado coroutine in celery app task when pool implementation is gevent. The main idea here is using a standalone tornado ioloop thread to execute coroutine task. If a task is a coroutine, the executor will register a callback on the tornado ioloop and switch to the related gevent hub. Here using a callback means that the coroutine will be totally executed inside the tornado ioloop thread. When the coroutine is finished, the executor will spawn a new greenlet which will switch to the previous task greenlet with the coroutine's result. Then task greenlet returns the result to the outer function inside the same greenlet. \r\n\r\nWe can write code in celery task as following:\r\n\r\n\r\n```python\r\nfrom celery import app\r\nfrom tornado import gen\r\n\r\n@app.task\r\n@gen.coroutine\r\ndef task_A():\r\n\r\n    process_1()\r\n    res = yield get_something()\r\n    do_something_with_res()\r\n    return res\r\n\r\n@gen.coroutine\r\ndef get_something():\r\n\r\n    res = {}\r\n    return res\r\n```\r\n\r\nThis is interesting when we use the tornado framework to write a project, we can easily divide partial logic into a celery task without any modifications. And in tornado 5.0, which will support Python 3 native coroutine, this future can connect more things together. I think people will like this.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "regisb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3684", "title": "Refer worker request info to absolute time", "body": "Previously, the time_start attribute of worker request objects refered\r\nto a timestamp relative to the monotonic time value. This caused\r\ntime_start attributes to be at a time far in the past. We fix this by\r\ncalling the on_accepted callback with an absolute time_accepted\r\nattribute.\r\n\r\nNote that the current commit does not change the\r\ncelery.concurrency.base API, although it would probably make sense to\r\nrename the \"monotonic\" named argument to \"time\".\r\n\r\nThis fixes the problem described in http://stackoverflow.com/questions/20091505/celery-task-with-a-time-start-attribute-in-1970/\r\n\r\nNote that there are many different ways to solve this problem; in the proposed implementation I choose to modify the default value of a keyword argument that is AFAIK never used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcsaaddupuy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3592", "title": "fixes exception deserialization when not using pickle", "body": "fix for #3586 \r\n\r\nThis PR try to fix how exceptions are deserialized when using a serializer different than pickle.\r\n\r\nThis avoid to [create new types](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L46) for exceptions, by doing 2 things : \r\n\r\n- store the exception module in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L243-L245) and reuse it in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L249-L258) instead of using raw `__name__` as module name\r\n\r\n- in [ celery/celery/utils/serialization.py::create_exception_cls](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L86-L98), try to find the exception class either from  `__builtins__`, or from the excetion module. Fallback on current behavior (which may still be wrong)\r\n\r\nAlso, it uses `exc.args` in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L247) and pass it to the exception constructor in [celery/celery/backends/base.py::Backend::exception_to_python](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L255), instead of using `str(exc.message)` which could lead to unwanted behavior\r\n\r\nThis won't work in every cases. If a class is defined locally in a function, this code won't be able to import the exception class using `import_module` and the old (wrong) behavior will still be used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "astewart-twist": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3293", "title": "Deserialize json string prior to inclusion in CouchDB doc", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chadrik": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3043", "title": "group and chord: subtasks are not executed until entire generator is consumed", "body": "When passing a generator to `group` or `chord`, sub-tasks are not submitted to the backend for execution until the entire generator is consumed.  The expected result is that subtasks are submitted during iteration, as soon as they are yielded.  This can have a big impact on performance if generators yield subtasks over a long period of time.\n\nI also opened issue #3021 on the subject.  [This explanation](https://github.com/celery/celery/issues/3021#issuecomment-176729202) from @eli-green I think is pretty on point.\n\nSo far I've only added support for redis.  I started looking at the other backends but I ran out of time.  It would be great to get some feedback on what I have so far and to get some thoughts on how difficult it will be to add for the other backends.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "m4ddav3": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2881", "title": "Database Backend: Use configured serializer, instead of always pickling.", "body": "Use the BLOB as an sa.BLOB\nSerialise the result an add to the db as bytes\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ask": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15545", "body": "README: Fix typo \"task.register\" -> \"tasks.register\". Closed by 8b5685e5b11f8987ba56c28ccb47f6c139541384. Thanks gregoirecachet)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546", "body": "What version is this? I thought I had fixed this already.\n\nThe examples in README should be updated. I think the best way of defining tasks now is using a Task class:\n\n```\n from celery.task import Task\n\n class MyTask(Task):\n     name = \"myapp.mytask\"\n\n     def run(self, x, y):\n         return x * y\n```\n\nand then\n\n```\n>>> from myapp.tasks import MyTask\n>>> MyTask.delay(2, 2)\n```\n\nas this makes it easier later to define a default routing_key for your task etc.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205", "body": "This works now. I remember it didn't at some point, and I remember I fixed it, so unless it still doesn't work for you I'm closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554", "body": "oh, that's bad. Got to get rid of yadayada dependency anyway, it's been an old trashbag for utilities, and all that is used from it now is the PickledObjectField.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555", "body": "Remove yadayada dependency. that means we've copy+pasted the\nPickledObjectField, when will djangosnippets ever die? :( Closed by fb582312905c5a1e001b6713be78ee2154b13204. Thansk\ngregoirecachet!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182", "body": "I'm not sure if that's so bad. Test requirements is not the same as install requirements. Sad the Django test runner is so broken. Maybe I can get it in somewhere else. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353", "body": "Add the test-runner from yadayada into the repo so we don't depend on yadayada\nanymore. Closed by af9ba75e195fc740493c9af6dbe84105b369d640.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428", "body": "Seems to be fine now. We'll re-open the issue if anyone says otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255", "body": "Implemented by 048d67f4bfb37c75f0a5d3dd4d0b4e05da400185 +  89626c59ee3a4da1e36612449f43362799ac0305\nAnd it really _is fast_ compared to the database/key-value store backends which uses polling to wait for the result.\n\nTo enable this back-end add the following setting to your settings.py/celeryconf.py:\n\n```\nCELERY_BACKEND=\"amqp\"\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017", "body": "A sample implementation has been commited (794582ebb278b2f96080a4cf4a68f1e77c3b003b + 93f6c1810c1051f8bdea6a7eae21d111997388d00 + fe62c47cb04723af738192087b40caefd27cab6a ) but not tested yet. Currently seeking anyone willing to test this feature.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115", "body": "Task retries seems to be working with tests passing. Closed by 41a38bb25fcacb48ee925e4319d35af9ab89d2bf\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721", "body": "Use basic.consume instead of basic.get to receive messages. Closed by 72fa505c7dfcf52c3215c276de67e10728898e70. This\nmeans the CELERY_QUEUE_WAKEUP_AFTER and CELERY_EMPTY_MSG_EMIT_EVERY settings,\nand the -w|--wakeup-after command line option to celeryd has been removed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902", "body": "If delay() is hanging, I'm guessing it's not because of the database, but because it can't get a connection to the AMQP server. (amqplib's default timeout must be very high). Is the broker running? Are you running RabbitMQ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071", "body": "I set the default connection timeout to be 4 seconds. Closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627", "body": "Re-opening the issue as setting the amqplib connection timeout didn't resolve it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160", "body": "This is actually an issue with RabbitMQ and will be fixed in the 1.7 release. I added the following to the celery FAQ:\n\n RabbitMQ hangs if it isn't able to authenticate the current user,\nthe password doesn't match or the user does not have access to the vhost\nspecified.  Be sure to check your RabbitMQ logs\n(`/var/log/rabbitmq/rabbit.log` on most systems), it usually contains a\nmessage describing the reason.\n\nThis will be fixed in the RabbitMQ 1.7 release.\n\nFor more information see the relevant thread on the rabbitmq-discuss mailing\nlist: http://bit.ly/iTTbD\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924", "body": "AsyncResult.ready() was always True. Closed by 4775a4c279179c17784bb72dc329f9a9d442ff0a. Thanks talentless.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110", "body": "Oh. That's indeed a problem with the installation. Could you try to install it using pip?\n\n```\n$ easy_install pip\n$ pip install celery\n```\n\nI will fix it as soon as possible, but in the mean time you could use pip.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111", "body": "Only use README as long_description if the file exists so easy_install don't\nbreak. Closed by e8845afc1a53aeab5b30d82dea29de32eb46b1d6. Thanks tobycatlin\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169", "body": "The consumerset branch was merged into master in c01a9885bbb8c83846b3770364fe208977a093fd (original contribution: screeley/celery@e2d0a56c913c66f69bf0040c9b76f74f0bb7dbd8). Big thanks to Sean Creeley.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472", "body": "Fixed in  faaa58ca717f230fe8b65e4804ad709265b18d5a\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432", "body": "By the way; This worked before we started using auto_ack=True, and basic.get.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448", "body": "Wait with message acknowledgement until the task has actually been executed.\nClosed by ef3f82bcf3b4b3de6584dcfc4b189ddadb4f50e6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017", "body": "This now merged into master. On second thought Munin plugins for these stats doesn't make sense, at least not generally. You could use this to make munin-plugins though, it's more like the groundwork for something more interesting later. (and it's useful for profiling right away)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966", "body": "Make TaskSet.run() respect message options from the Task class. Closed by 03d30a32de3502b73bca370cb0e70863c0ad3dd2.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29867", "body": "Thanks for pointing that out. It's a bug, indeed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38526", "body": "I added the importlib backport module as a dependency, is that >= 2.6 only? I guess I could add django==1.1 as a dependency, but I think there's someone using 1.0.x still.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38984", "body": "2.7 you mean? Yeah, but I added http://pypi.python.org/pypi/importlib as a dependency. Does it work with Python >= 2.4?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38985", "body": "Ah, I just saw the trove classifiers, all the way down to 2.3.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/51698", "body": "This commit was actually authored by me, just forgot to reset my `GIT_AUTHOR_NAME`, and `GIT_AUTHOR_EMAIL` env variables.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/51698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/52587", "body": "damn. Seems I forgot to reset my GIT_AUTHOR_\\* settings :(\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/52587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/82477", "body": "Ah, ok. That's good to know! I wasn't sure about this. Also, I'm wondering what holds the connection, is it the engine or the session?\nCreating a new connection for every operation is probably a bad idea, but not sure if it does some connection pooling by default.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91900", "body": "lol, yeah. I guess this is borderline.\n\nYou have the ability to supply your own connection, if you do we can't close the connection for you.\n\nAbove it says:\n\n```\nconn = connection or establish_connection(connect_timeout=connect_timeout)\n```\n\nso `connection or conn.close`, is \"if user didn't supply a connection, close the connection we established\".\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91937", "body": "Forgot that it's using the `@with_connection decorator` which takes care of this automatically :/\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/352668", "body": "Thanks!  Fixed in 154431f2c4ff04515000462ede70e205672e1751\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280", "body": "Is this deliberate?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588", "body": "Why did you remove the `Content-Type`?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074", "body": "Think there's a race here if it enters `time.sleep()`, and the putlock is released when the pools state != RUN.  The task will be sent to the queue in this case.  I'll fix it before I merge.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078", "body": "Also changed the interval to 1.0 as we discussed on IRC.  The shutdown process is already almost always delayed by at least one second because of other thread sleeps, so it doesn't make a difference (apart from less CPU usage in online mode).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181", "body": "Actually, the Pool implementation shouldn't depend on Celery, it's more of a patch for fixes and features we need for multiprocessing.\nSo this option should be added to `Pool.__init__`, then passed on from `celery.worker.WorkController`, then `celery.concurrency.processes.TaskPool`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183", "body": "and, oh yeah, `celery.conf` is deprecated to be removed in 3.0, so you don't have to add anything to it.\n\nWhen added to `WorkController`, it works in the case where you instantiate the worker without configuration too,\ne.g: `WorkController(worker_lost_wait=20)`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375", "body": "Ah, this was just a typo, I copied the document...\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380", "body": "It's on my TODO, I have to write it\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043", "body": "You can't terminate a reserved task, as it hasn't been executed yet.\n\nIt adds the id of the task to the list of revoked tasks, this is then checked again when the worker is about to execute the task (moves from reserved -> active).  If you only search the reserved requests then the terminate functionality would be useless (it wouldn't have any process to kill)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052", "body": "the other changes look great\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034", "body": "Revoked check for reserved/ETA tasks happens here:\nhttps://github.com/celery/celery/blob/3.0/celery/worker/job.py#L185-186\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089", "body": "> Also, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n\nStill, you shouldn't terminate a reserved task.  If a reserved task is revoked it should be dropped _before_ any pool worker starts to execute it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191", "body": "Hmm, maybe we could remove the `set` here, so that it preserves the original order.\n\nIt's not terribly bad if it imports the same module twice after all\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552", "body": "For Python 2.6 you have to include positions: {0} {1} {2}\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484", "body": "Celery related code should not be called by this method as it should be decoupled from Celery.  You need to find a way to support this by exposing it in the API.\nE.g. `Worker(on_shutdown=signals.worker_process_shutdown.send)`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390", "body": "Since created is True, it would also call 'on_node_join' in this instance, even if it just left.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331", "body": "Should this be logged?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "gcachet": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15549", "body": "It's master. I figured out this way to define tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557", "body": "Thanks! Removing yadayada dependency was my first though also, but I wasn't sure about the implications.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "talentless": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18501", "body": "No problem!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tobycatlin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18135", "body": "pip installed ok. I haven't tried the source code out of git. \n\nThanks for the quick response\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "brosner": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18434", "body": "My best guess is we've called terminate more than once? Will need some investigation.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "nikitka": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29856", "body": "why after **import** you use import celeryconfig ? this is work?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "brettcannon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/38473", "body": "Django as of (I believe) 1.1 has importlib included w/ it under django.util.importlib, so you don't need to rely on Python 2.6 to get import_module.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/38959", "body": "importlib was added in Python 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "paltman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/79488", "body": "This is why the test now fails -- now that is no longer 29 minutes past the hour, but 30, it is now due to run, while the assertion was to make sure it handled the case when it wasn't due, which when the mocked value was 29 minutes past the hour it properly returned False.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/79488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/204916", "body": "Was there a test for this failure before the fix?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/204916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nvie": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/81557", "body": "Nice and tidy.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/81557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "haridsv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/82469", "body": "Is this creating a new engine every time a new session is needed? As per sqlalchemy documentation, engine should be created only one time, unless of course the connection URL itself is changing.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/82478", "body": "Yes, the engine by default has a connection pool enabled: http://www.sqlalchemy.org/docs/05/reference/sqlalchemy/pooling.html#connection-pool-configuration\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jonozzz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/91703", "body": "I don't quite understand this one... why \"connection\" and why \"or\" ?\nI think it should be:\n    conn and conn.close()\n\nBecause when connection is None, conn.close() will be executed anyway.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "simonz05": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/103991", "body": "Nice, the previous one was aweful. Good to have the side-bar back.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/103991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Kami": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/104086", "body": "I agree, great work.\n\nThe blue (first?) version wasn't bad either, but the previous version was kinda step in the wrong direction.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/104086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adamn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/123743", "body": "$VIRTUALENV was removed - does it still work with a virtualenv?  It doesn't seem to for me on Ubuntu 10.04\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/123743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zen4ever": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/131915", "body": "Thanks for the fix. That was fast.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/131915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "joshdrake": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136184", "body": "Was just about to post an issue on this. I was surprised by this too, but even subclasses of accepted types must have adapters registered for database backends. Here's documentation on the process for pyscopg2:\n\nhttp://initd.org/psycopg/docs/advanced.html#adapting-new-types\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zzzeek": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136191", "body": "you need to look at TypeDecorator:\n\nhttp://www.sqlalchemy.org/docs/reference/sqlalchemy/types.html?highlight=typedecorator#sqlalchemy.types.TypeDecorator\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "passy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/147340", "body": "The dot should be inside the comment. It's a syntax error otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/147340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dcramer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/175143", "body": "I should note I have no idea what this does, I stole it form somewhere on the internet and it fixed the problems :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/175143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "shimon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/270710", "body": "Why did the date_done entry get removed? This is useful information that other backends seem to provide.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/270710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "enlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/352650", "body": "this should be _kill(...)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mher": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/595269", "body": "https://bitbucket.org/jezdez/sphinx-pypi-upload/issue/1/minor-patch-for-namespace-modules-etc patch should be applied before launching  paver upload_pypi_docs. upload_pypi_docs fails if .build contains empty subdirectories.\n\nIs there a way to automate this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/595269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810", "body": "I think it would be better to move registry._set_default_serializer('auth') to setup_security\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "steeve": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600", "body": "yes, this allows for the result backend to sub on this, allowing it not to poll\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "mitar": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921", "body": "As I explained, the content is urlencoded combination of parameters, not JSON. Why it would be `application/json`? We do not serialize to JSON (at this stage) anywhere. But urllib does urlencode it. If we remove manual override, then urllib does the right thing and sets it to `application/x-www-form-urlencoded` (it also sets `Content-Length` properly). This can Django (or any other receiver) then properly decode. So, urllib sends content as `application/x-www-form-urlencoded` and header should match that.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brendoncrawford": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182", "body": "Ok, I'll submit a new patch within the next week or so.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188", "body": "Ok, ill take a stab at it. Might take a few tries, but I think I can get it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ztlpn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747", "body": "Well I have not been able to check it with the latest version yet, but at least on version 3.0.6 this does not happen! If reserved but not yet active task is revoked, no check against revoked list is made when task becomes active. Instead it executes normally.\n\nAlso, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ambv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809", "body": "SIGTERM is not 9.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810", "body": "Ditto.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553", "body": "This style of string formatting is only available to Python 2.7+. I believe Celery is still supporting 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "VRGhost": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911", "body": "Well, it should be\n\n```\nwarnings.warn(\"%s consumed store_result call with args %s, %s\" % (self.__class__.__name__, args, kwargs))\n```\n\nthan. :-)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "dmtaub": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779", "body": "Ok, I will work on an api-based version soon. Depending on whether I need\nto write zmq interprocess communication this week, it might end up being\nless important to merge our branches at this particular moment :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "andrewkittredge": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195", "body": "this is wrong.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ionelmc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835", "body": "I added this to help with debugging #1785. I still think we should have it (it could indicate other problems).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}}, "4": {"xunto": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4491", "title": "[BUG] ready() always returns False for groups", "body": "## Steps to reproduce\r\n\r\n```python\r\nsent = group(\r\n    celery_app.signature('task1', args=arg),\r\n    celery_app.signature('task2', args=arg),\r\n    celery_app.signature('task3', args=arg)\r\n).apply_async()\r\n\r\nwhile not sent.ready():\r\n    pass\r\n\r\nprint(\"test\")\r\n```\r\n\r\n## Expected behavior\r\nI expect ```ready()``` to return ```True``` when all tasks are finished.\r\n\r\n## Actual behavior\r\nLooks like this code will never finish execution as ```ready()``` always return ```False``` even if all tasks are finished. Tested on celery master branch.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nitinmeharia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4489", "title": "django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread", "body": "### Application details:\r\n```\r\n    software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.4\r\n            billiard:3.5.0.3 redis:2.10.6\r\n    platform -> system:Darwin arch:64bit imp:CPython\r\n    loader   -> celery.loaders.app.AppLoader\r\n    settings -> transport:redis results:redis://localhost:6379/2\r\n```\r\n\r\n### Error log\r\n```\r\n    Signal handler <bound method DjangoWorkerFixup.on_worker_process_init of <celery.fixups.django.DjangoWorkerFixup object at 0x108e46fd0>> raised: DatabaseError(\"DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\",)\r\n    Traceback (most recent call last):\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/utils/dispatch/signal.py\", line 227, in send\r\n        response = receiver(signal=self, sender=sender, **named)\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 154, in on_worker_process_init\r\n        self._close_database()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 186, in _close_database\r\n        conn.close()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 283, in close\r\n        self.validate_thread_sharing()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 542, in validate_thread_sharing\r\n        % (self.alias, self._thread_ident, thread.get_ident())\r\n    django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\r\n```\r\n\r\n## Steps to reproduce\r\n```\r\ncelery multi restart w1 -A proj -l info\r\n```\r\nIts a django 1.11 application where we are getting this error.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kurara": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4488", "title": "launching worker from python: error with configuration from object.", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nVersion celery: 4.1.0 (latentcall)\r\n\r\nI'm trying to launch a worker from python code. When I use the class CeleryCommand with 'worker' option, it works. But if I add the option '--detach' or 'multi' the broker configuration is wrong. The code is:\r\n\r\n```\r\napp.config_from_object(config_module)\r\ncelerycmd = CeleryCommand(app)\r\ncelerycmd.execute_from_commandline(argv=[prog_name, 'worker', 'app_name', pidfile, logfile, loglevel])\r\n```\r\nor (which is not working)\r\n\r\n`celerycmd.execute_from_commandline(argv=[prog_name, 'multi', 'start', 'app_name', pidfile, logfile, loglevel])`\r\n\r\n# Expected behavior\r\nGet the same broker as in the configuration, not the default one\r\n\r\n## Actual behavior\r\n\r\nlog_file:\r\n\r\n```\r\n[2018-01-15 15:08:58,471: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2018-01-15 15:08:58,506: INFO/MainProcess] mingle: searching for neighbors\r\n[2018-01-15 15:08:59,647: INFO/MainProcess] mingle: all alone\r\n[2018-01-15 15:08:59,672: INFO/MainProcess] app_name@hostname ready.\r\n```\r\n\r\nFYI: I'm using lower case configuration. \r\n\r\nI could provide you what the object has in debug mode. Just ask me and I post it. When I debugged, I think the 'app' had the configuration of the file when it was inside the function _execute_from_commandline_, I can't understand when it loses it.\r\n\r\nI tought that maybe the problem is that I don't provide the configuration at the begining of the file, where app is declared, but in a function. So I tested to add the broker directly when I create the app: \r\n`app = Celery('app_name', broker_url='broker@...')`\r\nbut it didn't work either.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MShekow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4486", "title": "Memory hogging in client when using RPC (with RabbitMQ)", "body": "## Description\r\nIn my scenario I have a client program that puts thousands of tasks on the queue (`generate_data.delay()`). The workers produce a result that is of considerable size (suppose each result uses 1 MB of memory). The result is pickled back, and the client processes the results in some way, whenever results are available. In other words, once `AsyncResult.ready() == True`, I `get()` the result and do something with it.\r\nUsing `objgraph` I found out that celery never releases the result data.\r\n\r\n## My configuration\r\n```\r\nSoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.2\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Windows arch:32bit, WindowsPE imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:rpc:///\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\nresult_backend: 'rpc:///'\r\nresult_serializer: 'pickle'\r\ntask_serializer: 'pickle'\r\naccept_content: ['pickle']\r\n```\r\n\r\n\r\n## Steps to reproduce\r\nWorker program simply returns large objects, e.g.:\r\n```\r\nclass Data:\r\n    def __init__(self):\r\n        self.data = b'1' * 1024 * 1024\r\n\r\n@app.task\r\ndef generate_data() -> Data:\r\n    time.sleep(random.uniform(0, 0.2))\r\n    d = Data()\r\n    return d\r\n```\r\nThe client program retrieves results whenever they are available:\r\n```\r\nresults = {}\r\n\r\nfor i in range(2000):\r\n    async_result = generate_data.delay()\r\n    results[async_result] = True\r\n\r\nlogger.info(\"Put {} jobs on process queue!\".format(len(results)))\r\n\r\nresults_collected = 0\r\nwhile True:\r\n    # get results that are ready now\r\n    ready_results = [async_result for async_result, _ in results.items() if async_result.ready()]\r\n    if not ready_results:\r\n        time.sleep(10)\r\n        continue\r\n\r\n    results_collected += len(ready_results)\r\n\r\n    logger.info(\"Processing {} results. Got {} results so far\".format(len(ready_results), results_collected))\r\n    for ready_result in ready_results:\r\n        # we don't actually use the data - a real program would process the data somehow\r\n        result_data = ready_result.get()  # type: Data\r\n        del results[ready_result]\r\n\r\n    gc.collect()\r\n\r\n    # Exit loop once all results were processed:\r\n    if not results:\r\n        break\r\nlogger.info(\"Finished collecting all results\")\r\n```\r\n\r\n## Expected behavior\r\nWhen my client no longer has a reference to neither the actual data returned by `AsyncResult.get()`, nor the `AsyncResult` object itself, the memory of the data and the `AsyncResult` should be freed by celery.\r\n\r\n## Actual behavior\r\nMemory is being hogged so quickly that my client process (32-bit) dies soon. The reason is the huge size of `MESSAGE_BUFFER_MAX` (8192) and the oddly-hardcoded `bufmaxsize` (1000) in `BufferMap`. Are there any reasons for these huge buffers?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thedrow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4483", "title": "Add test coverage for #4356", "body": "#4356 is a critical fix for a bug that occurs when messages migrate between Celery 3 and Celery 4 clusters.\r\nDue to it's severity It was merged without proper test coverage.\r\nWe need to ensure this code is covered by the appropriate unit tests.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4434", "title": "Run the integration tests in a different build stage", "body": "We need to find a way to run the integration tests in a different build stage so that the unit tests will before them.\r\nThe unit tests are much quicker to execute and thus should run first to free up build resources for other contributors.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4423", "title": "Document that tasks are now documented automatically by celery", "body": "See https://github.com/celery/celery/pull/4422", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2547666a1ea13b27bc13ef296ae43a163ecd4ab3", "message": "Don't cover this branch as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/3af6a635cf90a4452db4e87c2326579ebad750c2", "message": "Merge branch 'master' into master"}, {"url": "https://api.github.com/repos/celery/celery/commits/bd0ed23c81b20fd75c0e2188fcf78e4d74898953", "message": "Use editable installs to measure code coverage correctly."}, {"url": "https://api.github.com/repos/celery/celery/commits/0a0fc0fbf698d30e9b6be29661e0d447548cc47c", "message": "Report coverage to terminal as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/4b4bf2a4eee65871d3e9c0e96d94b23aa2ee602c", "message": "Report coverage correctly (#4445)\n\n* Report coverage correctly.\r\n\r\nAs it turns out this repository does not report coverage to codecov at all since the path to the executables has changed at some point.\r\nThis should fix the problem.\r\n\r\n* Readd code coverage badge."}, {"url": "https://api.github.com/repos/celery/celery/commits/dbd59d9fc988ad0f04ae710c32066c5ff62374ef", "message": "Added bandit to the build matrix."}, {"url": "https://api.github.com/repos/celery/celery/commits/2ae00362179897968cfbd1fc8d61b648356769a7", "message": "Added bandit to lint for security issues."}, {"url": "https://api.github.com/repos/celery/celery/commits/56b94c327244fad4933706fbddc02eeea508d21a", "message": "Prettify test output."}, {"url": "https://api.github.com/repos/celery/celery/commits/ebd98fa4d36bb8003c2f46dbd16e9888af13720f", "message": "Parallel doc lints (#4435)\n\n* Bump sphinx.\r\n\r\n* Update copyright year. Mark the celerydocs & the celery.contrib.sphinx extensions as read_parallel_safe.\r\n\r\n* Install from git for now :(\r\n\r\n* Fix flake8 errors."}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4268", "title": "Add Var.CI integration", "body": "This pull request aims to demonstrate the power of [VarCI](https://var.ci/), the missing assistant for GitHub issues.\n\n--\n_Automated response by [Var.CI](https://var.ci)_ :robot:", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3982", "title": "Added failing test cases for #3885", "body": "The tests were contributed by @robpogorzelski\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nAttempt to fix #3885 without hurting eager execution of the canvas.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PromyLOPh": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4480", "title": "Application is not thread-safe", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.4\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:rpc:///\r\n\r\nresult_backend: 'rpc:///'\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\n```\r\n\r\n## Steps to reproduce\r\n\r\nUsing code from the user manual, but two threads running concurrently:\r\n\r\n```python\r\nfrom celery import Celery\r\nimport time\r\nfrom threading import Thread\r\n\r\napp = Celery('test', broker='amqp://guest@localhost//', backend='rpc://')\r\n\r\n@app.task(bind=True)\r\ndef hello(self, a, b):\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 50})\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 90})\r\n    time.sleep(1)\r\n    return 'hello world: %i' % (a+b)\r\n\r\nif __name__ == '__main__':\r\n    def run ():\r\n        handle = hello.delay (1, 2)\r\n        print (handle.get ())\r\n    t1 = Thread (target=run)\r\n    t2 = Thread (target=run)\r\n    t1.start ()\r\n    t2.start ()\r\n    t1.join ()\r\n    t2.join ()\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\nhello world: 3\r\nhello world: 3\r\n```\r\n\r\n## Actual behavior\r\n\r\nDifferent exceptions, depending on timing. For instance:\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 456, in channel\r\n    return self.channels[channel_id]\r\nKeyError: None\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 56, in start\r\n    self._connection.default_channel, [initial_queue],\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 821, in default_channel\r\n    self._default_channel = self.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 266, in channel\r\n    chan = self.transport.create_channel(self.connection)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/transport/pyamqp.py\", line 100, in create_channel\r\n    return connection.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 459, in channel\r\n    channel.open()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 432, in open\r\n    spec.Channel.Open, 's', ('',), wait=spec.Channel.OpenOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 468, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 473, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 252, in read_frame\r\n    payload = read(size)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 417, in _read\r\n    s = recv(n - len(rbuf))\r\nsocket.timeout: timed out\r\n```\r\nor\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 59, in start\r\n    self._consumer.consume()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 477, in consume\r\n    self._basic_consume(T, no_ack=no_ack, nowait=False)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 598, in _basic_consume\r\n    no_ack=no_ack, nowait=nowait)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/entity.py\", line 737, in consume\r\n    arguments=self.consumer_arguments)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 1564, in basic_consume\r\n    wait=None if nowait else spec.Basic.ConsumeOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 471, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 476, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 254, in read_frame\r\n    'Received {0:#04x} while expecting 0xce'.format(ch))\r\namqp.exceptions.UnexpectedFrame: Received 0x3c while expecting 0xce\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kzidane": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4471", "title": "DisabledBackend when starting flask --with-threads?", "body": "I'm trying to use Celery for one of my applications and experiencing a strange behavior that I'm not sure why it's caused.\r\n\r\nTo replicate, here's a simple Flask app:\r\n\r\n    # application.py\r\n    from celery.contrib.abortable import AbortableAsyncResult\r\n    from flask import Flask\r\n    from tasks import add\r\n\r\n    app = Flask(__name__)\r\n\r\n    @app.route(\"/\")\r\n    def index():\r\n        # start the task and return its id\r\n        return add.delay(42, 50).task_id\r\n\r\n\r\n    @app.route(\"/state/<task_id>\")\r\n    def result(task_id):\r\n        # return current task state\r\n        return AbortableAsyncResult(task_id).state\r\n\r\nand here's a Celery app and a task:\r\n\r\n    # tasks.py\r\n    from celery import Celery\r\n    from celery.contrib.abortable import AbortableTask\r\n\r\n\r\n    app = Celery(\r\n        \"tasks\",\r\n        # use sqlite database as result backend (also tried rpc://)\r\n        backend=\"db+sqlite:///celerydb.sqlite\",\r\n        broker=\"pyamqp://localhost\"\r\n    )\r\n\r\n    @app.task(bind=True, base=AbortableTask)\r\n    def add(self, x, y):\r\n        return x + y\r\n\r\nRunning the Celery worker:\r\n\r\n    $ celery -A tasks worker --loglevel=info\r\n     -------------- celery@7677a80760b4 v4.1.0 (latentcall)\r\n    ---- **** ----- \r\n    --- * ***  * -- Linux-4.10.0-42-generic-x86_64-with-debian-jessie-sid 2018-01-02 20:22:48\r\n    -- * - **** --- \r\n    - ** ---------- [config]\r\n    - ** ---------- .> app:         tasks:0x7f339a52dfd0\r\n    - ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n    - ** ---------- .> results:     sqlite:///celerydb.sqlite\r\n    - *** --- * --- .> concurrency: 8 (prefork)\r\n    -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n    --- ***** ----- \r\n     -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n    [tasks]\r\n      . tasks.add\r\n\r\n    [2018-01-02 20:22:48,585: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n    [2018-01-02 20:22:48,592: INFO/MainProcess] mingle: searching for neighbors\r\n    [2018-01-02 20:22:49,608: INFO/MainProcess] mingle: all alone\r\n    [2018-01-02 20:22:49,636: INFO/MainProcess] celery@7677a80760b4 ready.\r\n\r\n\r\nRunning the Flask app:\r\n\r\n    $ FLASK_APP=application.py flask run --with-threads\r\n      * Serving Flask app \"application\"\r\n      * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\r\n\r\n\r\nHitting `/` with `curl` starts the task and returns its id without any problems:\r\n\r\n    $ curl http://localhost:5000\r\n    f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nCelery's output at this point:\r\n\r\n    [2018-01-02 20:29:28,974: INFO/MainProcess] Received task: tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef]\r\n    [2018-01-02 20:29:29,000: INFO/ForkPoolWorker-1] Task tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef] succeeded in 0.02414485500776209s: 92\r\n\r\nBut trying to get the state of the task \r\n\r\n    $ curl http://localhost:5000/state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nresults in the following error:\r\n\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n\r\neven though the backend seems to be configured per the `backend` argument to `Celery` and its output? I also tried setting `CELERY_RESULT_BACKEND` and `result_backend` using `app.conf.update`, but no luck!\r\n\r\nWhat's interesting is that this problem disappears if I drop the `--with-threads` option from the `flask run` command. Any idea why this might be caused and how to work around it if possible?\r\n\r\nAdditional details:\r\n\r\n    $ celery --version\r\n    4.1.0 (latentcall)\r\n    $ flask --version\r\n    Flask 0.12.2\r\n    Python 3.6.0 (default, Oct 30 2017, 05:46:44) \r\n    [GCC 4.8.4]\r\n\r\nFull traceback:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1982, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1614, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1517, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/_compat.py\", line 33, in reraise\r\n    raise value\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1612, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1598, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n      File \"/root/application.py\", line 14, in result\r\n    return AbortableAsyncResult(task_id).state\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 436, in state\r\n    return self._get_task_meta()['status']\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 375, in _get_task_meta\r\n    return self._maybe_set_cache(self.backend.get_task_meta(self.id))\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/backends/base.py\", line 352, in get_task_meta\r\n    meta = self._get_task_meta_for(task_id)\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n    127.0.0.1 - - [02/Jan/2018 20:46:28] \"GET /state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef HTTP/1.1\" 500 -\r\n\r\nThank you!", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fnordian": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4465", "title": "Celery.close() leaks redis connections", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.3\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\n\r\n```python\r\nimport time\r\nfrom celery import Celery\r\n\r\ndef run_celery_task(taskname):\r\n    with Celery(broker='redis://redis:6379/0', backend='redis://redis:6379/0') as celery:\r\n        res = celery.send_task(taskname)\r\n        print(res)\r\n\r\nfor i in range(0, 100):\r\n    run_celery_task(\"test\")\r\n\r\ntime.sleep(100)\r\n```\r\n\r\n```bash\r\nnetstat -tn | grep 6379 | grep ESTABLISHED | wc -l\r\n```\r\n## Expected behavior\r\n\r\nWhen the `with Celery`-block terminates, I expect all connections to redis being closed.\r\n\r\n## Actual behavior\r\n\r\nNot all connections are closed. When the for-loop finishes, there > 100 open connections to redis.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kimice": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4464", "title": "Maximum recursion depth exceeded while calling a Python object", "body": "Hi, when I call celery apply_async function, sometimes it raise Exception like this. It seems like getting config failed. This bug can't always reappear. I guess celery may be not init correctly. I'm so confused with this bug.\r\n\r\ncelery==4.1.0\r\n\r\nI init celery with flask like this.\r\n\r\n```\r\ndef make_celery(app):\r\n    celery = Celery(app.import_name, backend=app.config['CELERY_RESULT_BACKEND'],\r\n                    broker=app.config['CELERY_BROKER_URL'])\r\n    celery.conf.update(app.config)\r\n    celery.config_from_object('App.celery_custom.celery_config')\r\n    TaskBase = celery.Task\r\n    class ContextTask(TaskBase):\r\n        abstract = True\r\n        def __call__(self, *args, **kwargs):\r\n            with app.app_context():\r\n                return TaskBase.__call__(self, *args, **kwargs)\r\n    celery.Task = ContextTask\r\n    return celery\r\n\r\ncelery_app = make_celery(flask_app)\r\n\r\n@celery_app.task(bind=True)\r\ndef checkInstance(self, a, b):\r\n    pass\r\n\r\ncheck_task = checkInstance.apply_async(args=['123', '123'], queue='123')\r\n```\r\n\r\n```\r\nLOG:\r\n[2017-12-26 08:42:58,659]: logs_util.py[line:64] [pid:25680] ERROR Traceback (most recent call last):\r\n  File \"./App/views/experiment_views.py\", line 377, in create_experiment_and_run\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/task.py\", line 521, in apply_async\r\n    if app.conf.task_always_eager:\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 431, in __getitem__\r\n    return getitem(k)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 280, in __getitem__\r\n    return mapping[_key]\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/utils/objects.py\", line 44, in __get__\r\n    value = obj.__dict__[self.__name__] = self.__get(obj)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 148, in data\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 911, in _finalize_pending_conf\r\n    conf = self._conf = self._load_config()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 921, in _load_config\r\n    self.loader.config_from_object(self._config_source)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 128, in config_from_object\r\n    obj = self._smart_import(obj, imp=self.import_from_cwd)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 146, in _smart_import\r\n    return imp(path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 106, in import_from_cwd\r\n    package=package,\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/imports.py\", line 100, in import_from_cwd\r\n    with cwd_in_path():\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 84, in helper\r\n    return GeneratorContextManager(func(*args, **kwds))\r\nRuntimeError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lexabug": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4462", "title": "INFO log messages land to stderr", "body": "I've set up a basic application with Django 2.0 and Celery 4.1.0 with debug task (as it described [here](http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html#using-celery-with-django)) and one custom task in application tasks module.\r\nMy module code look like this:\r\n```\r\n# Create your tasks here\r\nfrom __future__ import absolute_import, unicode_literals\r\nfrom celery import shared_task\r\nfrom django.conf import settings\r\nfrom celery.utils.log import get_task_logger\r\n\r\nlogger = get_task_logger(__name__)\r\n\r\n@shared_task(name='validate_user_email', ignore_result=True, bind=True)\r\ndef validate_user_email(self, user_id):\r\n    logger.info('%s email verified', user_id)\r\n```\r\n\r\nWhen I redirect streams (stdout and stderr) to different logs  (info.log and error.log) info.log is silent while error.log contains log entries with levels WARNING and INFO.\r\n\r\nCommand I execute celery with is: `celery -A email_validation worker -Q validate --concurrency 5 --maxtasksperchild 100 -l info > info.log 2> error.log`\r\n\r\nCelery config:\r\n```\r\nCELERY_BROKER_URL = '******************'\r\nCELERY_BROKER_HEARTBEAT = 900\r\nCELERY_BROKER_HEARTBEAT_CHECKRATE = 15\r\nCELERY_RESULT_BACKEND = 'amqp'\r\nCELERY_WORKER_PREFETCH_MULTIPLIER = 1\r\nCELERY_WORKER_MAX_TASKS_PER_CHILD = 100\r\nCELERY_TASK_ACKS_LATE = True\r\nCELERY_ENABLE_UTC = False\r\nCELERY_TIMEZONE = 'US/Eastern'\r\nCELERY_WORKER_DISABLE_RATE_LIMITS = True\r\nCELERY_EVENT_QUEUE_TTL = 1\r\nCELERY_EVENT_QUEUE_EXPIRES = 60\r\nCELERY_RESULT_EXPIRES = 3600\r\nCELERY_TASK_IGNORE_RESULT = True\r\nCELERY_WORKER_HIJACK_ROOT_LOGGER = True\r\n```\r\n\r\nHow can make celery to post INFO log messages to stdout? ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cajbecu": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4457", "title": "Connection to broker lost. Trying to re-establish the connection: OSError: [Errno 9] Bad file descriptor", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Software\r\ncelery==4.1.0\r\nkombu==4.1.0\r\namqp==2.2.2\r\nPython 3.6.1\r\nbroker: rabbitmq 3.6.14\r\nresult backend: redis\r\n\r\n## Steps to reproduce\r\n1. celery -A proj worker -Q Q1 --autoscale=10,1 -Ofair --without-gossip --without-mingle --heartbeat-interval=60 -n Q1\r\n2. celery lost connection to broker\r\n3. after restarting affected worker the connection is successfully re-established and the worker starts processing tasks\r\n\r\n## Expected behavior\r\ncelery should re-establish connection to broker\r\n\r\n## Actual behavior\r\ncelery tries to re-establish connection to broker but fails with this error message (which is repeated every second) until manually restarted:\r\n```\r\n[user] celery.worker.consumer.consumer WARNING 2017-12-18 00:38:27,078 consumer: \r\nConnection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/loops.py\", line 47, in asynloop\r\n    obj.controller.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/worker.py\", line 217, in register_with_event_loop\r\n    description='hub.register',\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 151, in send_all\r\n    fun(parent, *args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/components.py\", line 178, in register_with_event_loop\r\n    w.pool.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/prefork.py\", line 134, in register_with_event_loop\r\n    return reg(loop)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in register_with_event_loop\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in <listcomp>\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 207, in add_reader\r\n    return self.add(fds, callback, READ | ERR, args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 158, in add\r\n    self.poller.register(fd, flags)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/utils/eventio.py\", line 67, in register\r\n    self._epoll.register(fd, events)\r\nOSError: [Errno 9] Bad file descriptor\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4457/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thiagogalesi": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4454", "title": "Celery does not consider authSource on mongodb backend URLs", "body": "Version: Celery 4.0.2 (from looking at the changes since then it seems there is no change addressing this issue here: https://github.com/celery/celery/commits/master/celery/backends/mongodb.py )\r\n\r\n(Edit) Confirmed with the following versions as well:\r\namqp==2.2.2\r\nbilliard==3.5.0.3\r\ncelery==4.1.0\r\nkombu==4.1.0\r\npymongo==3.6.0\r\n\r\nCelery Report\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.8\r\n            billiard:3.5.0.3 py-amqp:2.2.1\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\n\r\n## Steps to reproduce\r\n\r\nGive Celery a Backend URL pointing to a MongoDB instance with authentication and username/password (user/pwd set on the Admin DB by default) in the format:\r\n\r\nmongodb://user:pass@your-server/your_db?authSource=admin\r\n\r\n(Please see http://api.mongodb.com/python/current/examples/authentication.html#default-database-and-authsource and http://api.mongodb.com/python/current/api/pymongo/mongo_client.html?highlight=authsource )\r\n\r\n## Expected behavior\r\n\r\nCelery authenticates the user in the admin database (this is the same as passing --authenticationDatabase to the mongo client or the same url to MongoClient)\r\n\r\n## Actual behavior\r\n\r\nCelery tries to authenticate the user on the your_db database (failing to authenticate)\r\n\r\n## Workaround (not recommended)\r\n\r\nChange the db on the URL to /admin (this db shouldn't be used to store arbitrary data normally)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yanliguo": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4451", "title": "Celery (4.1.0) worker stops to consume new message when actives is empty  ", "body": "Hi there,\r\n   I was using celery to dispatch some long running task recently, and finding a way to disable prefetch. Now the config is:\r\n\r\nacks_late = True\r\nconcurrency = 1\r\nprefetch_multiplier = 1\r\n-Ofair\r\n\r\nActually, workers are still prefetching tasks. And I also have a monitor job to revoke tasks when a task is stuck in reserved state for a long time (let's say 5 minutes) or the task is outputing valid result.  \r\n\r\nOne thing wired is that, some workers stopped consuming new tasks when the actives is empty and the reseved task is revoked.  Has anybody ever met with this issue ? any help will be appreciated.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4450", "title": "PENDING state, what if it meant just one thing?", "body": "According to docs, PENDING state has the following meaning:\r\n\r\n> Task is waiting for execution or unknown. Any task id that\u2019s not known is implied to be in the pending state.\r\n\r\nAre there plans to make pending mean one thing only? It is quite confusing to handle a state that can mean \"waiting\" or \"lost\". ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfresh613": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4449", "title": "ValueFormatError when processing chords with couchbase result backend", "body": "Hi it seems like when I attempt to process groups of chords, the couchbase result backend is consistently failing to unlock the chord when reading from the db:\r\n\r\n`celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()`\r\n\r\nThis behavior does not occur with the redis result backend, i can switch between them and see that the error unlocking only occurs on couchbase.\r\n\r\n## Steps to reproduce\r\nAttempt to process a chord with couchbase backend using pickle serialization.\r\n\r\n## Expected behavior\r\nChords process correctly, and resulting data is fed to the next task\r\n\r\n## Actual behavior\r\nCelery is unable to unlock the chord from the result backend\r\n\r\n## Celery project info: \r\n```\r\ncelery -A ipaassteprunner report\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.10\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:couchbase://isadmin:**@localhost:8091/tasks\r\n\r\ntask_serializer: 'pickle'\r\nresult_serializer: 'pickle'\r\ndbconfig: <ipaascommon.ipaas_config.DatabaseConfig object at 0x10fbbfe10>\r\ndb_pass: u'********'\r\nIpaasConfig: <class 'ipaascommon.ipaas_config.IpaasConfig'>\r\nimports:\r\n    ('ipaassteprunner.tasks',)\r\nworker_redirect_stdouts: False\r\nDatabaseConfig: u'********'\r\ndb_port: '8091'\r\nipaas_constants: <module 'ipaascommon.ipaas_constants' from '/Library/Python/2.7/site-packages/ipaascommon/ipaas_constants.pyc'>\r\nenable_utc: True\r\ndb_user: 'isadmin'\r\ndb_host: 'localhost'\r\nresult_backend: u'couchbase://isadmin:********@localhost:8091/tasks'\r\nresult_expires: 3600\r\niconfig: <ipaascommon.ipaas_config.IpaasConfig object at 0x10fbbfd90>\r\nbroker_url: u'amqp://guest:********@localhost:5672//'\r\ntask_bucket: 'tasks'\r\naccept_content: ['pickle']\r\n```\r\n### Additional Debug output\r\n```\r\n[2017-12-13 15:39:57,860: INFO/MainProcess] Received task: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2]  ETA:[2017-12-13 20:39:58.853535+00:00] \r\n[2017-12-13 15:39:57,861: DEBUG/MainProcess] basic.qos: prefetch_count->27\r\n[2017-12-13 15:39:58,859: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x10b410b90> (args:('celery.chord_unlock', 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', {'origin': 'gen53678@silo2460', 'lang': 'py', 'task': 'celery.chord_unlock', 'group': None, 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', u'delivery_info': {u'priority': None, u'redelivered': False, u'routing_key': u'celery', u'exchange': u''}, 'expires': None, u'correlation_id': 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', 'retries': 311, 'timelimit': [None, None], 'argsrepr': \"('90c64bef-21ba-42f9-be75-fdd724375a7a', {'chord_size': 2, 'task': 'ipaassteprunner.tasks.transfer_data', 'subtask_type': None, 'kwargs': {}, 'args': (), 'options': {'chord_size': None, 'chain': [...], 'task_id': '9c6b5e1c-2089-4db7-9590-117aeaf782c7', 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', 'reply_to': '0a58093c-6fdd-3458-9a34-7d5e094ac6a8'}, 'immutable': False})\", 'eta': '2017-12-13T20:39:58.853535+00:00', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', u'reply_to':... kwargs:{})\r\n[2017-12-13 15:40:00,061: DEBUG/MainProcess] basic.qos: prefetch_count->26\r\n[2017-12-13 15:40:00,065: DEBUG/MainProcess] Task accepted: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] pid:53679\r\n[2017-12-13 15:40:00,076: INFO/ForkPoolWorker-6] Task celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()\r\n```\r\n\r\n### Stack trace from chord unlocking failure\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/builtins.py\", line 75, in unlock_chord\r\n    raise self.retry(countdown=interval, max_retries=max_retries)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/task.py\", line 689, in retry\r\n    raise ret\r\nRetry: Retry in 1s\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yutkin": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4438", "title": "Celery doesn't write RECEIVED state into MongoDB", "body": "When a number of tasks in a queue surpass a number of workers, new added tasks are not writing in a backend. In other words, I want to write task state (RECEIVED) into DB immediately after its invocation. It is possible?\r\n\r\nP.S. I'm using MongoDB as backend. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "canassa": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4426", "title": "Task is executed twice when the worker restarts", "body": "Currently using Celery 4.1.0\r\n\r\n## Steps to reproduce\r\n\r\nStart a new project using RabbitMQ and register the following task:\r\n\r\n```python\r\nfrom django.core.cache import cache\r\n\r\n@shared_task(bind=True)\r\ndef test_1(self):\r\n    if not cache.add(self.request.id, 1):\r\n        raise Exception('Duplicated task {}'.format(self.request.id))\r\n```\r\n\r\nNow start 2 workers. I used gevent with a concurrency of 25 for this test:\r\n\r\n```\r\ncelery worker -A my_proj -Q my_queue -P gevent -c 25\r\n```\r\n\r\nOpen a python shell and fire a a bunch of tasks:\r\n\r\n```python\r\nfrom myproj.tasks import test_1\r\n\r\nfor i in range(10000):\r\n    test_1.apply_async()\r\n```\r\n\r\nNow quickly do a warm shutdown (Ctrl+c) in one of the workers while it's still processing the tasks, you should see the errors popping in the second worker:\r\n\r\n```\r\nERROR    Task my_proj.tasks.test_1[e28e6760-1371-49c9-af87-d196c59375e9] raised unexpected: Exception('Duplicated task e28e6760-1371-49c9-af87-d196c59375e9',)\r\nTraceback (most recent call last):\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/code/scp/python/my_proj/tasks.py\", line 33, in test_1\r\n    raise Exception('Duplicated task {}'.format(self.request.id))\r\nException: Duplicated task e28e6760-1371-49c9-af87-d196c59375e9\r\n```\r\n\r\n## Expected behavior\r\n\r\nSince I am not using late acknowledgment and I am not killing the workers I wasn't expecting the tasks to execute again.\r\n\r\n## Actual behavior\r\n\r\nThe tasking are being executed twice, this is causing some problems in our servers because we restart our works every 15 minutes or so in order to avoid memory leaks.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kn-id": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4424", "title": "Celery Crash: Unrecoverable error when using QApplication in main process", "body": "Error happens when there's some queues already added before worker started and max task per child is 1\r\n\r\n## Checklist\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.12\r\n                         billiard:3.5.0.3 py-amqp:2.2.2\r\n      platform -> system:Linux arch:64bit, ELF imp:CPython\r\n      loader   -> celery.loaders.app.AppLoader\r\n      settings -> transport:amqp results:disabled\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n1. create instance of QApplication when main worker started\r\n```\r\n@celeryd_after_setup.connect\r\nfrom PyQt4.QtGui import QApplication\r\ndef init_worker(sender, **k):\r\n    QApplication([])\r\n```\r\n2. create task\r\n```\r\n@app.task\r\ndef job():\r\n    print 'hello'\r\n```\r\n3. add some queues (2 - 3 per thread. so if there's 4 worker child then there's 8 or more queues)\r\n\r\n4. start worker with max task per child 1\r\n```\r\ncelery worker -A proj --loglevel=INFO --max-tasks-per-child=1\r\n```\r\n\r\n## Expected behavior\r\n- run queues successfuly\r\n## Actual behavior\r\nCelery Crash after 1 queue per child\r\n```\r\n2017-12-07 10:20:19,594: INFO/MainProcess] Received task: proj.tasks.job[c6413dc2-ad62-4033-a69b-39556276f789]  \r\n[2017-12-07 10:20:19,595: INFO/MainProcess] Received task: proj.tasks.job[f1c10b1c-03ae-4220-9c7f-2cdf4afc61e3]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[d54f4554-4517-470f-8e14-adedcb93a46e]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[6255e5e6-d4c8-4d87-8075-642bca9e6a6d]  \r\n[2017-12-07 10:20:19,700: INFO/ForkPoolWorker-1] Task proj.tasks.job[ca856d5c-f3cc-45d4-9fbc-665753f5d1d2] succeeded in 0.00115608799388s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-4] Task proj.tasks.job[9ae27611-e8e5-4e08-9815-1e56e2ad1565] succeeded in 0.00130111300678s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-3] Task proj.tasks.job[f5aa7c6a-4142-4a38-8814-c60424196826] succeeded in 0.00129756200477s: None\r\n[2017-12-07 10:20:19,702: INFO/ForkPoolWorker-2] Task proj.tasks.job[eb13b5c5-8865-4992-8b9e-6672c909fd59] succeeded in 0.00100053900678s: None\r\n[2017-12-07 10:20:19,710: INFO/MainProcess] Received task: proj.tasks.job[01700061-c69c-4f4c-abf2-e6ba200772bd]  \r\n[2017-12-07 10:20:19,711: INFO/MainProcess] Received task: proj.tasks.job[a27d7a7b-2c58-4689-8b98-2c0a4ceaea9f]  \r\n[2017-12-07 10:20:19,713: INFO/MainProcess] Received task: proj.tasks.job[4c4a5685-23d5-4178-89cc-9ce4ad5a3509]  \r\n[2017-12-07 10:20:19,714: INFO/MainProcess] Received task: proj.tasks.job[44a079a3-aacf-48c5-a76b-a061bdced1d6]\r\n[2017-12-07 01:41:03,591: CRITICAL/MainProcess] Unrecoverable error: AttributeError(\"'error' object has no attribute 'errno'\",)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/async/hub.py\", line 354, in create_loop\r\n    cb(*cbargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 444, in _event_process_exit\r\n    self.maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1307, in maintain_pool\r\n    self._maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1298, in _maintain_pool\r\n    joined = self._join_exited_workers()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1165, in _join_exited_workers\r\n    self.process_flush_queues(worker)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 1175, in process_flush_queues\r\n    readable, _, _ = _select(fds, None, fds, timeout=0.01)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 183, in _select\r\n    if exc.errno == errno.EINTR:\r\nAttributeError: 'error' object has no attribute 'errno'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jenstroeger": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4420", "title": "How to unpack serialzied task arguments?", "body": "When iterate over all currently scheduled tasks\r\n```python\r\nfor task in chain.from_iterable(my_app.control.inspect().scheduled().values()):\r\n    print(task)\r\n```\r\nI get a dictionary `task['request']` which contains a serialization of the tasks\u2019 arguments in `task['request']['args']` (and `'kwargs`). Both are strings:\r\n```\r\n'args': \"('5', {'a': 'b'})\",\r\n'kwargs': '{}', \r\n```\r\nIt\u2019s not [JSON](https://www.json.org/) nor [msgpack](https://msgpack.org/). How can I unpack that `args` string into a Python tuple again? Celery must have a helper function for that somewhere? (Anything to do with `argsrepr` and `kwargsrepr` and [`saferepr`](https://github.com/celery/celery/blob/master/celery/utils/saferepr.py)?)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Chris7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/ba2dec7956782c84068ef779e554fb07de524beb", "message": "Propagate arguments to chains inside groups (#4481)\n\n* Remove self._frozen from _chain run method\r\n\r\n* Add in explicit test for group results in chain"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4482", "title": "Add docker-compose and base dockerfile for development", "body": "This adds a docker based development environment. This removes the need for users to install their own rabbit/redis/virtual environment/etc. to begin development of celery. I use this myself (since after moving to docker I have essentially nothing installed on my computer anymore) and saw interest in it from issue #4334 so thought a PR may be appropriate to share my setup.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "auvipy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/028dbe4a4d6786d56ed30ea49971cc5415fffb4b", "message": "update version to 4.2.0"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4459", "title": "[wip] #3021 bug fix ", "body": "Fixes #3021", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zpl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/442f42b7084ff03cb730ca4f452c3a47d9b8d701", "message": "task_replace chord inside chord fix (fixes #4368) (#4369)\n\n* task_replace chord inside chord fix\r\n\r\n* Complete fix for replace inside chords with tests\r\n\r\n* Add integration tests for add_to_chord\r\n\r\n* Fix JSON serialisation in tests\r\n\r\n* Raise exception when replacing signature has a chord"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4302", "title": "Ignore celery.exception.Ignore on autoretry", "body": "Autoretry for task should ignore celery.exception.Ignore, which is generated by self.replace()\r\n\r\notherwise, it goes to infinite loop.\r\n\r\n```python\r\n@app.task(autoretry_for=(Exception,), default_retry_delay=1,\r\n          max_retries=None,\r\n          bind=True,acks_late=True)\r\ndef TaskA(self):\r\n    raise self.replace(TaskB.s()) # Always retrying because of replace\r\n\r\n```\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdufresne": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/5eba340aae2e994091afb7a0ed7839e7d944ee13", "message": "Pass python_requires argument to setuptools (#4479)\n\nHelps pip decide what version of the library to install.\r\n\r\nhttps://packaging.python.org/tutorials/distributing-packages/#python-requires\r\n\r\n> If your project only runs on certain Python versions, setting the\r\n> python_requires argument to the appropriate PEP 440 version specifier\r\n> string will prevent pip from installing the project on other Python\r\n> versions.\r\n\r\nhttps://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords\r\n\r\n> python_requires\r\n>\r\n> A string corresponding to a version specifier (as defined in PEP 440)\r\n> for the Python version, used to specify the Requires-Python defined in\r\n> PEP 345."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "freakboy3742": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a4abe149aa00b0f85024a6cac64fd984cb2d0a6b", "message": "Refs #4356: Handle \"hybrid\" messages that have moved between Celery versions (#4358)\n\n* handle \"hybrid\" messages which have passed through a protocol 1 and protocol 2 consumer in its life.\r\n\r\nwe detected an edgecase which is proofed out in https://gist.github.com/ewdurbin/ddf4b0f0c0a4b190251a4a23859dd13c#file-readme-md which mishandles messages which have been retried by a 3.1.25, then a 4.1.0, then again by a 3.1.25 consumer. as an extension, this patch handles the \"next\" iteration of these mutant payloads.\r\n\r\n* explicitly construct proto2 from \"hybrid\" messages\r\n\r\n* remove unused kwarg\r\n\r\n* fix pydocstyle check\r\n\r\n* flake8 fixes\r\n\r\n* correct fix for misread pydocstyle error"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "djw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9ce3df9962830a6f9e0e68005bdeec1092e314e4", "message": "Corrected the default visibility timeout (#4476)\n\nAccording to kombu, the default visibility timeout is 30 minutes.\r\n\r\nhttps://github.com/celery/kombu/blob/3a7cdb07c9bf75b54282274d711af15ca6ad5d9f/kombu/transport/SQS.py#L85"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alexgarel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0ffd36fbf9343fe2f6ef7744a14ebfbec5ac86b6", "message": "request on_timeout now ignores soft time limit exception (fixes #4412) (#4473)\n\n* request on_timeout now ignores soft time limit exception (closes #4412)\r\n\r\n* fix quality"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "georgepsarakis": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a7915054d0e1e896c9ccf5ff0497dd8e3d5ed541", "message": "Integration test to verify PubSub unsubscriptions (#4468)\n\n* [Redis Backend] Integration test to verify PubSub unsubscriptions\r\n\r\n* Import sequence for isort check\r\n\r\n* Re-order integration tasks import"}, {"url": "https://api.github.com/repos/celery/celery/commits/9ab0971fe28462b667895d459d198ef6dd761c89", "message": "Add --diff flag in isort in order to display changes (#4469)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pachewise": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c8f9b7fbab3fe8a8de5cbae388fca4edf54bf503", "message": "Fixes #4452 - Clearer Django settings documentation (#4467)\n\n* reword django settings section in first steps\r\n\r\n* anchor link for django admonition\r\n\r\n* mention django-specific settings config"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hclihn": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/3ca1a54e65762ccd61ce728b3e3dfcb622fc0c90", "message": "Allow the shadow kwarg and the shadow_name method to set shadow properly (#4381)\n\n* Allow the shadow kwarg and the shadow_name method to set shadow properly \r\n\r\nThe shadow_name option in the @app.task() decorator (which overrides the shadow_name method in the Task class) and the shadow keyword argument of Task.apply_async() don't work as advertised.\r\nThis moves the shadow=... out of the 'if self.__self__ is not None:' block and allows shadow to be set by the shadow keyword argument of Task.apply_async() or the shadow_name method in the Task class (via, say, the shadow_name option in the @app.task() decorator).\r\n\r\n* Added a test to cover calling shadow_name().\r\n\r\n* Sort imports.\r\n\r\n* Fix missing import."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AlexHill": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fde58ad677a1e28effd1ac13f1f08f7132392463", "message": "Run chord_unlock on same queue as chord body - fixes #4337 (#4448)"}, {"url": "https://api.github.com/repos/celery/celery/commits/25f5e29610b2224122cf10d5252de92b4efe3e81", "message": "Support chords with empty headers (#4443)"}, {"url": "https://api.github.com/repos/celery/celery/commits/7ef809f41c1e0db2f6813c9c3a66553ca83c0c69", "message": "Add bandit baseline file with contents this time"}, {"url": "https://api.github.com/repos/celery/celery/commits/fdf0928b9b5698622c3b8806e2bca2d134df7fa3", "message": "Add bandit baseline file"}, {"url": "https://api.github.com/repos/celery/celery/commits/10f06ea1df75f109bf08fb8d42f9977cabcd7e0e", "message": "Fix length-1 and nested chords (#4393 #4055 #3885 #3597 #3574 #3323) (#4437)\n\n* Don't convert single-task chord to chain\r\n\r\n* Fix evaluation of nested chords"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pokoli": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/63c747889640bdea7753e83373a3a3e0dffc4bd9", "message": "Add celery_tryton integration on framework list (#4446)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "azaitsev": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/83872030b00a1ac75597ed3fc0ed34d9f664c6c1", "message": "Fixed wrong value in example of celery chain (#4444)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matteius": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/976515108a4357397a3821332e944bb85550dfa2", "message": "make astimezone call in localize more safe (#4324)\n\nmake astimezone call in localize more safe; with tests"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "myw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/4dc8c001d063a448d598f4bbc94056812cf15fc8", "message": "Add Mikhail Wolfson to CONTRIBUTORS.txt (#4439)"}, {"url": "https://api.github.com/repos/celery/celery/commits/dd2cdd9c4f8688f965d7b5658fa4956d083a7b8b", "message": "Resolve TypeError on `.get` from nested groups (#4432)\n\n* Accept and pass along the `on_interval` in ResultSet.get\r\n\r\nOtherwise, calls to .get or .join on ResultSets fail on nested groups.\r\nFixes #4274\r\n\r\n* Add a unit test that verifies the fixed behavior\r\n\r\nVerified that the unit test fails on master, but passes on the patched version. The\r\nnested structure of results was borrowed from #4274\r\n\r\n* Wrap long lines\r\n\r\n* Add integration test for #4274 use case\r\n\r\n* Switch to a simpler, group-only-based integration test\r\n\r\n* Flatten expected integration test result\r\n\r\n* Added back testcase from #4274 and skip it if the backend under test does not support native joins.\r\n\r\n* Fix lint.\r\n\r\n* Enable only if chords are allowed.\r\n\r\n* Fix access to message."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "johnarnold": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4490", "title": "Add task properties to AsyncResult, store in backend", "body": "*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PauloPeres": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4484", "title": "Adding the CMDS for Celery and Celery Beat to Run on Azure WebJob", "body": "This is what worked for us!\r\n\r\n## Description\r\nCreated the .cmd files to be used on Azure.\r\nJust going into Web Jobs on Azure servers and creating a Continuous Web Job, and uploading the zip files of the celery should work.\r\nEach folder should have a separeted Web Job.\r\nAlso whoever use should take a look where their celery package is\r\n\r\n\r\nThis pull request don't fix any bugs, it's just a new \"Helper\" for the ones who want to use Celery into Azure servers.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zengdahuaqusong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4475", "title": "separate backend database from login database", "body": "by setting 'backend_database' in MONGODB_BACKEND_SETTINGS, users can separate backend database from login database. Which means they can authenticate mongodb with 'database', while data is actually writing to 'backend_database'\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4474", "title": "Replace TaskFormatter with TaskFilter", "body": "Replacing `TaskFormatter` with a `TaskFilter` as that can be more easily reused when overriding the logging system.\r\n\r\nI've left the `TaskFormatter` in but marked it as deprecated in case people are using it.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "neaket360pi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4472", "title": "Cleanup the mailbox's producer pool after forking", "body": "Fixes https://github.com/celery/celery/issues/3751\r\n\r\nIf the mailbox is used before forking the workers will\r\nnot be able to broadcast messages.  This is because the producer pool\r\ninstance on the mail box will be `closed.`  This fix will cause the\r\nmailbox to load the producer pool again after forking.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charettes": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4456", "title": "Perform a serialization roundtrip on eager apply_async.", "body": "Fixes #4008 \r\n\r\n/cc @AlexHill ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jurrian": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4442", "title": "Fix for get() follow_parents exception handling (#1014)", "body": "## Description\r\n\r\nWhen working with chains, the situation might be that the first chain task has failed. Calling `x.get(propagate=True, follow_parents=True)` should cope with that since it checks if the parent tasks have raised exceptions.\r\n\r\nHowever, in my experience, when `x.get(propagate=True, follow_parents=True)` is called the failed task is still pending at that moment and will become failed seconds later. This creates a bug in the behaviour of `follow_parents`.\r\n\r\nThis fix calls `get(propagate=True, follow_parents=False)` on each parent instead, which will cause it to raise directly when the parent task becomes failed. It looks like `maybe_throw()` can be safely replaced since it is called by `get()` at some other place.\r\n\r\nIn order to be more comprehensive, I extended the documentation for `follow_parents`.\r\n\r\nI am not experienced enough in this project to see the whole picture, so please advise on possible problems that might arise with this fix for #1014 .\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Checkroth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4285", "title": "Add failing test for broker url priority ref issue #4284", "body": "## Description\r\nThis PR is to assist in showing the issue described here: https://github.com/celery/celery/issues/4284\r\n\r\nAll this PR does is add a failing test that _should_ be passing, if the issue above is resolved.\r\n\r\nThis PR does not solve the issue mentioned. I leave that up to the discretion of a more knowledgeable party, if they decide that it is indeed an issue at all. I do not expect this PR to be merged unless the issue is resolved and you want this test to remain in the repository.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jamesmallen": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4262", "title": "Disable backend for beat", "body": "## Description\r\n\r\nWhen using `beat` (at least in its standalone form), it is not necessary to subscribe to events on the result backend. These subscriptions happen in the `on_task_call` method of the backend. This PR ensures that no `SUBSCRIBE` messages are sent.\r\n\r\nFixes #4261 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bluestann": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4241", "title": "Use Cherami as a broker", "body": "Hi, \r\n\r\nI have added support for celery to use [Cherami](https://eng.uber.com/cherami/) as a broker. \r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "harukaeru": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4239", "title": "Changed raise RuntimeError to RuntimeWarning when task_always_eager is True", "body": "## Description\r\nThis PR changes RuntimeError to RuntimeWarning when `task_always_eager` is True.\r\n\r\nSometimes I tested what is related to celery task using `task_always_eager` in celery v3.\r\n(The tests are not only celery task but codes using celery task and other codes)\r\n\r\nI know the config cannot be completely tested async task but it's useful when doing rush works.\r\nI never use them in the production code but in the test, I use.\r\n\r\nI read the issue (https://github.com/celery/celery/issues/2275) was produced and the commit (https://github.com/celery/celery/commit/c71cd08fc72742efbfc846a81020939aa3692501) resolved the above.\r\n\r\n\r\nI almost agree with them but people who want to test perfectly only don't turn on `task_always_eager`.\r\nOthers who want to test synchronously also want to use `task_always_eager`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "erebus1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4227", "title": "fix crontab description of month_of_year behaviour", "body": "\r\n## Description\r\nIn [docs](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#crontab-schedules) said:\r\n\r\n> crontab(0, 0, month_of_year='*/3') -> Execute on the first month of every quarter.\r\n\r\nWhich is a bit confused, because, in fact it will run task each day on the first month of every quarter.\r\n\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rpkilby": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4212", "title": "Fix celery_worker test fixture", "body": "This is an attempt to fix #4088. Thanks to @karenc for providing a [breakdown](https://github.com/celery/celery/issues/4088#issuecomment-321287239) of what's happening.\r\n\r\nSide note:\r\nUsing `celery_worker` over `celery_session_worker` in the integration tests is a bit slower, given the worker startup/teardown for each test. This could be faster if the worker was scoped at a module level, but it's not possible to change the scope a fixture. The [recommendation](https://github.com/pytest-dev/pytest/issues/2300) is to simply create a fixture per scope. \r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bbgwilbur": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4077", "title": "Catch NotRegistered exception for errback", "body": "If the errback task is not registered in the currently running worker, the arity_greater check will fail. We can just assume its going to work as an old-style signature and deal with the possible error if it doesn't later.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChillarAnand": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4013", "title": "Updated commands to kill celery workers", "body": "```\r\nchillar+  1696 26093  1 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:MainProcess] -active- (worker -l info -A t)\r\nchillar+  1715  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-1]\r\nchillar+  1716  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-2]\r\nchillar+  1717  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-3]\r\nchillar+  1718  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-4]\r\n```\r\nWith latest version, celery worker process names seems changed. So, the commands used to kill those process needs to be updated.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Taywee": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3852", "title": "celery.beat.Scheduler: fix _when to make tz-aware (#3851)", "body": "Fixes #3851 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JackDanger": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3838", "title": "Removing last references to task_method", "body": "AFAICT the code removed in this PR only served to support the\n(now-removed) celery.contrib.methods.task_method() function.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gugu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3834", "title": "Handle ignore_result", "body": "## Description\r\n\r\nCurrently (according to my checks and code grep) ignore_result option is a stub, it does nothing. This patch skips backend calls for tasks with `ignore_result=True`\r\n\r\nThis is needed to minimize effect of broken redis backend support", "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3815", "title": "Chord does not clean up subscribed channels in redis. Fixes #3812", "body": "This pull requests fixes issue, when chord gets result for subtasks, but does not do `UNSUBSCRIBE` command for them.\r\n\r\n**What does this patch fix**:\r\n\r\nWithout this patch `chord(task() for i in range(50))` creates 51 subscriptions. After patch it will create 1\r\n\r\n**What this patch does not fix**:\r\n\r\nEvery task, which result is not consumed, creates redis subscription. Even if `ignore_result` is specified. I plan to submit separate patch for this issue\r\n\r\nI think, that changing old slow redis behavior to new fast and broken was not a good idea, but as soon as choice made, we need to make it usable", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "justdoit0823": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3757", "title": "Add supporting for task execution working with tornado coroutine", "body": "With this future, we can use tornado coroutine in celery app task when pool implementation is gevent. The main idea here is using a standalone tornado ioloop thread to execute coroutine task. If a task is a coroutine, the executor will register a callback on the tornado ioloop and switch to the related gevent hub. Here using a callback means that the coroutine will be totally executed inside the tornado ioloop thread. When the coroutine is finished, the executor will spawn a new greenlet which will switch to the previous task greenlet with the coroutine's result. Then task greenlet returns the result to the outer function inside the same greenlet. \r\n\r\nWe can write code in celery task as following:\r\n\r\n\r\n```python\r\nfrom celery import app\r\nfrom tornado import gen\r\n\r\n@app.task\r\n@gen.coroutine\r\ndef task_A():\r\n\r\n    process_1()\r\n    res = yield get_something()\r\n    do_something_with_res()\r\n    return res\r\n\r\n@gen.coroutine\r\ndef get_something():\r\n\r\n    res = {}\r\n    return res\r\n```\r\n\r\nThis is interesting when we use the tornado framework to write a project, we can easily divide partial logic into a celery task without any modifications. And in tornado 5.0, which will support Python 3 native coroutine, this future can connect more things together. I think people will like this.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "regisb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3684", "title": "Refer worker request info to absolute time", "body": "Previously, the time_start attribute of worker request objects refered\r\nto a timestamp relative to the monotonic time value. This caused\r\ntime_start attributes to be at a time far in the past. We fix this by\r\ncalling the on_accepted callback with an absolute time_accepted\r\nattribute.\r\n\r\nNote that the current commit does not change the\r\ncelery.concurrency.base API, although it would probably make sense to\r\nrename the \"monotonic\" named argument to \"time\".\r\n\r\nThis fixes the problem described in http://stackoverflow.com/questions/20091505/celery-task-with-a-time-start-attribute-in-1970/\r\n\r\nNote that there are many different ways to solve this problem; in the proposed implementation I choose to modify the default value of a keyword argument that is AFAIK never used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcsaaddupuy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3592", "title": "fixes exception deserialization when not using pickle", "body": "fix for #3586 \r\n\r\nThis PR try to fix how exceptions are deserialized when using a serializer different than pickle.\r\n\r\nThis avoid to [create new types](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L46) for exceptions, by doing 2 things : \r\n\r\n- store the exception module in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L243-L245) and reuse it in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L249-L258) instead of using raw `__name__` as module name\r\n\r\n- in [ celery/celery/utils/serialization.py::create_exception_cls](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L86-L98), try to find the exception class either from  `__builtins__`, or from the excetion module. Fallback on current behavior (which may still be wrong)\r\n\r\nAlso, it uses `exc.args` in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L247) and pass it to the exception constructor in [celery/celery/backends/base.py::Backend::exception_to_python](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L255), instead of using `str(exc.message)` which could lead to unwanted behavior\r\n\r\nThis won't work in every cases. If a class is defined locally in a function, this code won't be able to import the exception class using `import_module` and the old (wrong) behavior will still be used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "astewart-twist": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3293", "title": "Deserialize json string prior to inclusion in CouchDB doc", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chadrik": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3043", "title": "group and chord: subtasks are not executed until entire generator is consumed", "body": "When passing a generator to `group` or `chord`, sub-tasks are not submitted to the backend for execution until the entire generator is consumed.  The expected result is that subtasks are submitted during iteration, as soon as they are yielded.  This can have a big impact on performance if generators yield subtasks over a long period of time.\n\nI also opened issue #3021 on the subject.  [This explanation](https://github.com/celery/celery/issues/3021#issuecomment-176729202) from @eli-green I think is pretty on point.\n\nSo far I've only added support for redis.  I started looking at the other backends but I ran out of time.  It would be great to get some feedback on what I have so far and to get some thoughts on how difficult it will be to add for the other backends.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "m4ddav3": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2881", "title": "Database Backend: Use configured serializer, instead of always pickling.", "body": "Use the BLOB as an sa.BLOB\nSerialise the result an add to the db as bytes\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ask": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15545", "body": "README: Fix typo \"task.register\" -> \"tasks.register\". Closed by 8b5685e5b11f8987ba56c28ccb47f6c139541384. Thanks gregoirecachet)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546", "body": "What version is this? I thought I had fixed this already.\n\nThe examples in README should be updated. I think the best way of defining tasks now is using a Task class:\n\n```\n from celery.task import Task\n\n class MyTask(Task):\n     name = \"myapp.mytask\"\n\n     def run(self, x, y):\n         return x * y\n```\n\nand then\n\n```\n>>> from myapp.tasks import MyTask\n>>> MyTask.delay(2, 2)\n```\n\nas this makes it easier later to define a default routing_key for your task etc.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205", "body": "This works now. I remember it didn't at some point, and I remember I fixed it, so unless it still doesn't work for you I'm closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554", "body": "oh, that's bad. Got to get rid of yadayada dependency anyway, it's been an old trashbag for utilities, and all that is used from it now is the PickledObjectField.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555", "body": "Remove yadayada dependency. that means we've copy+pasted the\nPickledObjectField, when will djangosnippets ever die? :( Closed by fb582312905c5a1e001b6713be78ee2154b13204. Thansk\ngregoirecachet!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182", "body": "I'm not sure if that's so bad. Test requirements is not the same as install requirements. Sad the Django test runner is so broken. Maybe I can get it in somewhere else. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353", "body": "Add the test-runner from yadayada into the repo so we don't depend on yadayada\nanymore. Closed by af9ba75e195fc740493c9af6dbe84105b369d640.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428", "body": "Seems to be fine now. We'll re-open the issue if anyone says otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255", "body": "Implemented by 048d67f4bfb37c75f0a5d3dd4d0b4e05da400185 +  89626c59ee3a4da1e36612449f43362799ac0305\nAnd it really _is fast_ compared to the database/key-value store backends which uses polling to wait for the result.\n\nTo enable this back-end add the following setting to your settings.py/celeryconf.py:\n\n```\nCELERY_BACKEND=\"amqp\"\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017", "body": "A sample implementation has been commited (794582ebb278b2f96080a4cf4a68f1e77c3b003b + 93f6c1810c1051f8bdea6a7eae21d111997388d00 + fe62c47cb04723af738192087b40caefd27cab6a ) but not tested yet. Currently seeking anyone willing to test this feature.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115", "body": "Task retries seems to be working with tests passing. Closed by 41a38bb25fcacb48ee925e4319d35af9ab89d2bf\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721", "body": "Use basic.consume instead of basic.get to receive messages. Closed by 72fa505c7dfcf52c3215c276de67e10728898e70. This\nmeans the CELERY_QUEUE_WAKEUP_AFTER and CELERY_EMPTY_MSG_EMIT_EVERY settings,\nand the -w|--wakeup-after command line option to celeryd has been removed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902", "body": "If delay() is hanging, I'm guessing it's not because of the database, but because it can't get a connection to the AMQP server. (amqplib's default timeout must be very high). Is the broker running? Are you running RabbitMQ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071", "body": "I set the default connection timeout to be 4 seconds. Closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627", "body": "Re-opening the issue as setting the amqplib connection timeout didn't resolve it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160", "body": "This is actually an issue with RabbitMQ and will be fixed in the 1.7 release. I added the following to the celery FAQ:\n\n RabbitMQ hangs if it isn't able to authenticate the current user,\nthe password doesn't match or the user does not have access to the vhost\nspecified.  Be sure to check your RabbitMQ logs\n(`/var/log/rabbitmq/rabbit.log` on most systems), it usually contains a\nmessage describing the reason.\n\nThis will be fixed in the RabbitMQ 1.7 release.\n\nFor more information see the relevant thread on the rabbitmq-discuss mailing\nlist: http://bit.ly/iTTbD\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924", "body": "AsyncResult.ready() was always True. Closed by 4775a4c279179c17784bb72dc329f9a9d442ff0a. Thanks talentless.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110", "body": "Oh. That's indeed a problem with the installation. Could you try to install it using pip?\n\n```\n$ easy_install pip\n$ pip install celery\n```\n\nI will fix it as soon as possible, but in the mean time you could use pip.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111", "body": "Only use README as long_description if the file exists so easy_install don't\nbreak. Closed by e8845afc1a53aeab5b30d82dea29de32eb46b1d6. Thanks tobycatlin\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169", "body": "The consumerset branch was merged into master in c01a9885bbb8c83846b3770364fe208977a093fd (original contribution: screeley/celery@e2d0a56c913c66f69bf0040c9b76f74f0bb7dbd8). Big thanks to Sean Creeley.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472", "body": "Fixed in  faaa58ca717f230fe8b65e4804ad709265b18d5a\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432", "body": "By the way; This worked before we started using auto_ack=True, and basic.get.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448", "body": "Wait with message acknowledgement until the task has actually been executed.\nClosed by ef3f82bcf3b4b3de6584dcfc4b189ddadb4f50e6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017", "body": "This now merged into master. On second thought Munin plugins for these stats doesn't make sense, at least not generally. You could use this to make munin-plugins though, it's more like the groundwork for something more interesting later. (and it's useful for profiling right away)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966", "body": "Make TaskSet.run() respect message options from the Task class. Closed by 03d30a32de3502b73bca370cb0e70863c0ad3dd2.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280", "body": "Is this deliberate?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588", "body": "Why did you remove the `Content-Type`?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074", "body": "Think there's a race here if it enters `time.sleep()`, and the putlock is released when the pools state != RUN.  The task will be sent to the queue in this case.  I'll fix it before I merge.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078", "body": "Also changed the interval to 1.0 as we discussed on IRC.  The shutdown process is already almost always delayed by at least one second because of other thread sleeps, so it doesn't make a difference (apart from less CPU usage in online mode).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181", "body": "Actually, the Pool implementation shouldn't depend on Celery, it's more of a patch for fixes and features we need for multiprocessing.\nSo this option should be added to `Pool.__init__`, then passed on from `celery.worker.WorkController`, then `celery.concurrency.processes.TaskPool`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183", "body": "and, oh yeah, `celery.conf` is deprecated to be removed in 3.0, so you don't have to add anything to it.\n\nWhen added to `WorkController`, it works in the case where you instantiate the worker without configuration too,\ne.g: `WorkController(worker_lost_wait=20)`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375", "body": "Ah, this was just a typo, I copied the document...\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380", "body": "It's on my TODO, I have to write it\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043", "body": "You can't terminate a reserved task, as it hasn't been executed yet.\n\nIt adds the id of the task to the list of revoked tasks, this is then checked again when the worker is about to execute the task (moves from reserved -> active).  If you only search the reserved requests then the terminate functionality would be useless (it wouldn't have any process to kill)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052", "body": "the other changes look great\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034", "body": "Revoked check for reserved/ETA tasks happens here:\nhttps://github.com/celery/celery/blob/3.0/celery/worker/job.py#L185-186\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089", "body": "> Also, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n\nStill, you shouldn't terminate a reserved task.  If a reserved task is revoked it should be dropped _before_ any pool worker starts to execute it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191", "body": "Hmm, maybe we could remove the `set` here, so that it preserves the original order.\n\nIt's not terribly bad if it imports the same module twice after all\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552", "body": "For Python 2.6 you have to include positions: {0} {1} {2}\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484", "body": "Celery related code should not be called by this method as it should be decoupled from Celery.  You need to find a way to support this by exposing it in the API.\nE.g. `Worker(on_shutdown=signals.worker_process_shutdown.send)`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390", "body": "Since created is True, it would also call 'on_node_join' in this instance, even if it just left.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331", "body": "Should this be logged?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "gcachet": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15549", "body": "It's master. I figured out this way to define tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557", "body": "Thanks! Removing yadayada dependency was my first though also, but I wasn't sure about the implications.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "talentless": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18501", "body": "No problem!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tobycatlin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18135", "body": "pip installed ok. I haven't tried the source code out of git. \n\nThanks for the quick response\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "brosner": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18434", "body": "My best guess is we've called terminate more than once? Will need some investigation.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "steeve": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600", "body": "yes, this allows for the result backend to sub on this, allowing it not to poll\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "mitar": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921", "body": "As I explained, the content is urlencoded combination of parameters, not JSON. Why it would be `application/json`? We do not serialize to JSON (at this stage) anywhere. But urllib does urlencode it. If we remove manual override, then urllib does the right thing and sets it to `application/x-www-form-urlencoded` (it also sets `Content-Length` properly). This can Django (or any other receiver) then properly decode. So, urllib sends content as `application/x-www-form-urlencoded` and header should match that.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brendoncrawford": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182", "body": "Ok, I'll submit a new patch within the next week or so.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188", "body": "Ok, ill take a stab at it. Might take a few tries, but I think I can get it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ztlpn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747", "body": "Well I have not been able to check it with the latest version yet, but at least on version 3.0.6 this does not happen! If reserved but not yet active task is revoked, no check against revoked list is made when task becomes active. Instead it executes normally.\n\nAlso, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ambv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809", "body": "SIGTERM is not 9.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810", "body": "Ditto.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mher": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810", "body": "I think it would be better to move registry._set_default_serializer('auth') to setup_security\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "mlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553", "body": "This style of string formatting is only available to Python 2.7+. I believe Celery is still supporting 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "VRGhost": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911", "body": "Well, it should be\n\n```\nwarnings.warn(\"%s consumed store_result call with args %s, %s\" % (self.__class__.__name__, args, kwargs))\n```\n\nthan. :-)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "dmtaub": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779", "body": "Ok, I will work on an api-based version soon. Depending on whether I need\nto write zmq interprocess communication this week, it might end up being\nless important to merge our branches at this particular moment :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "andrewkittredge": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195", "body": "this is wrong.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ionelmc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835", "body": "I added this to help with debugging #1785. I still think we should have it (it could indicate other problems).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}}, "5": {"xunto": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4491", "title": "[BUG] ready() always returns False for groups", "body": "## Steps to reproduce\r\n\r\n```python\r\nsent = group(\r\n    celery_app.signature('task1', args=arg),\r\n    celery_app.signature('task2', args=arg),\r\n    celery_app.signature('task3', args=arg)\r\n).apply_async()\r\n\r\nwhile not sent.ready():\r\n    pass\r\n\r\nprint(\"test\")\r\n```\r\n\r\n## Expected behavior\r\nI expect ```ready()``` to return ```True``` when all tasks are finished.\r\n\r\n## Actual behavior\r\nLooks like this code will never finish execution as ```ready()``` always return ```False``` even if all tasks are finished. Tested on celery master branch.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4491/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "nitinmeharia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4489", "title": "django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread", "body": "### Application details:\r\n```\r\n    software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.4\r\n            billiard:3.5.0.3 redis:2.10.6\r\n    platform -> system:Darwin arch:64bit imp:CPython\r\n    loader   -> celery.loaders.app.AppLoader\r\n    settings -> transport:redis results:redis://localhost:6379/2\r\n```\r\n\r\n### Error log\r\n```\r\n    Signal handler <bound method DjangoWorkerFixup.on_worker_process_init of <celery.fixups.django.DjangoWorkerFixup object at 0x108e46fd0>> raised: DatabaseError(\"DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\",)\r\n    Traceback (most recent call last):\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/utils/dispatch/signal.py\", line 227, in send\r\n        response = receiver(signal=self, sender=sender, **named)\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 154, in on_worker_process_init\r\n        self._close_database()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/celery/fixups/django.py\", line 186, in _close_database\r\n        conn.close()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 283, in close\r\n        self.validate_thread_sharing()\r\n      File \"/code/tmp/reignite/venv/lib/python3.6/site-packages/django/db/backends/base/base.py\", line 542, in validate_thread_sharing\r\n        % (self.alias, self._thread_ident, thread.get_ident())\r\n    django.db.utils.DatabaseError: DatabaseWrapper objects created in a thread can only be used in that same thread. The object with alias 'default' was created in thread id 140736440837056 and this is thread id 4441538184.\r\n```\r\n\r\n## Steps to reproduce\r\n```\r\ncelery multi restart w1 -A proj -l info\r\n```\r\nIts a django 1.11 application where we are getting this error.\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4489/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kurara": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4488", "title": "launching worker from python: error with configuration from object.", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nVersion celery: 4.1.0 (latentcall)\r\n\r\nI'm trying to launch a worker from python code. When I use the class CeleryCommand with 'worker' option, it works. But if I add the option '--detach' or 'multi' the broker configuration is wrong. The code is:\r\n\r\n```\r\napp.config_from_object(config_module)\r\ncelerycmd = CeleryCommand(app)\r\ncelerycmd.execute_from_commandline(argv=[prog_name, 'worker', 'app_name', pidfile, logfile, loglevel])\r\n```\r\nor (which is not working)\r\n\r\n`celerycmd.execute_from_commandline(argv=[prog_name, 'multi', 'start', 'app_name', pidfile, logfile, loglevel])`\r\n\r\n# Expected behavior\r\nGet the same broker as in the configuration, not the default one\r\n\r\n## Actual behavior\r\n\r\nlog_file:\r\n\r\n```\r\n[2018-01-15 15:08:58,471: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n[2018-01-15 15:08:58,506: INFO/MainProcess] mingle: searching for neighbors\r\n[2018-01-15 15:08:59,647: INFO/MainProcess] mingle: all alone\r\n[2018-01-15 15:08:59,672: INFO/MainProcess] app_name@hostname ready.\r\n```\r\n\r\nFYI: I'm using lower case configuration. \r\n\r\nI could provide you what the object has in debug mode. Just ask me and I post it. When I debugged, I think the 'app' had the configuration of the file when it was inside the function _execute_from_commandline_, I can't understand when it loses it.\r\n\r\nI tought that maybe the problem is that I don't provide the configuration at the begining of the file, where app is declared, but in a function. So I tested to add the broker directly when I create the app: \r\n`app = Celery('app_name', broker_url='broker@...')`\r\nbut it didn't work either.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "MShekow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4486", "title": "Memory hogging in client when using RPC (with RabbitMQ)", "body": "## Description\r\nIn my scenario I have a client program that puts thousands of tasks on the queue (`generate_data.delay()`). The workers produce a result that is of considerable size (suppose each result uses 1 MB of memory). The result is pickled back, and the client processes the results in some way, whenever results are available. In other words, once `AsyncResult.ready() == True`, I `get()` the result and do something with it.\r\nUsing `objgraph` I found out that celery never releases the result data.\r\n\r\n## My configuration\r\n```\r\nSoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.2\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Windows arch:32bit, WindowsPE imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:rpc:///\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\nresult_backend: 'rpc:///'\r\nresult_serializer: 'pickle'\r\ntask_serializer: 'pickle'\r\naccept_content: ['pickle']\r\n```\r\n\r\n\r\n## Steps to reproduce\r\nWorker program simply returns large objects, e.g.:\r\n```\r\nclass Data:\r\n    def __init__(self):\r\n        self.data = b'1' * 1024 * 1024\r\n\r\n@app.task\r\ndef generate_data() -> Data:\r\n    time.sleep(random.uniform(0, 0.2))\r\n    d = Data()\r\n    return d\r\n```\r\nThe client program retrieves results whenever they are available:\r\n```\r\nresults = {}\r\n\r\nfor i in range(2000):\r\n    async_result = generate_data.delay()\r\n    results[async_result] = True\r\n\r\nlogger.info(\"Put {} jobs on process queue!\".format(len(results)))\r\n\r\nresults_collected = 0\r\nwhile True:\r\n    # get results that are ready now\r\n    ready_results = [async_result for async_result, _ in results.items() if async_result.ready()]\r\n    if not ready_results:\r\n        time.sleep(10)\r\n        continue\r\n\r\n    results_collected += len(ready_results)\r\n\r\n    logger.info(\"Processing {} results. Got {} results so far\".format(len(ready_results), results_collected))\r\n    for ready_result in ready_results:\r\n        # we don't actually use the data - a real program would process the data somehow\r\n        result_data = ready_result.get()  # type: Data\r\n        del results[ready_result]\r\n\r\n    gc.collect()\r\n\r\n    # Exit loop once all results were processed:\r\n    if not results:\r\n        break\r\nlogger.info(\"Finished collecting all results\")\r\n```\r\n\r\n## Expected behavior\r\nWhen my client no longer has a reference to neither the actual data returned by `AsyncResult.get()`, nor the `AsyncResult` object itself, the memory of the data and the `AsyncResult` should be freed by celery.\r\n\r\n## Actual behavior\r\nMemory is being hogged so quickly that my client process (32-bit) dies soon. The reason is the huge size of `MESSAGE_BUFFER_MAX` (8192) and the oddly-hardcoded `bufmaxsize` (1000) in `BufferMap`. Are there any reasons for these huge buffers?", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4486/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thedrow": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4483", "title": "Add test coverage for #4356", "body": "#4356 is a critical fix for a bug that occurs when messages migrate between Celery 3 and Celery 4 clusters.\r\nDue to it's severity It was merged without proper test coverage.\r\nWe need to ensure this code is covered by the appropriate unit tests.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4483/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4434", "title": "Run the integration tests in a different build stage", "body": "We need to find a way to run the integration tests in a different build stage so that the unit tests will before them.\r\nThe unit tests are much quicker to execute and thus should run first to free up build resources for other contributors.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/4423", "title": "Document that tasks are now documented automatically by celery", "body": "See https://github.com/celery/celery/pull/4422", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4423/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/2547666a1ea13b27bc13ef296ae43a163ecd4ab3", "message": "Don't cover this branch as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/3af6a635cf90a4452db4e87c2326579ebad750c2", "message": "Merge branch 'master' into master"}, {"url": "https://api.github.com/repos/celery/celery/commits/bd0ed23c81b20fd75c0e2188fcf78e4d74898953", "message": "Use editable installs to measure code coverage correctly."}, {"url": "https://api.github.com/repos/celery/celery/commits/0a0fc0fbf698d30e9b6be29661e0d447548cc47c", "message": "Report coverage to terminal as well."}, {"url": "https://api.github.com/repos/celery/celery/commits/4b4bf2a4eee65871d3e9c0e96d94b23aa2ee602c", "message": "Report coverage correctly (#4445)\n\n* Report coverage correctly.\r\n\r\nAs it turns out this repository does not report coverage to codecov at all since the path to the executables has changed at some point.\r\nThis should fix the problem.\r\n\r\n* Readd code coverage badge."}, {"url": "https://api.github.com/repos/celery/celery/commits/dbd59d9fc988ad0f04ae710c32066c5ff62374ef", "message": "Added bandit to the build matrix."}, {"url": "https://api.github.com/repos/celery/celery/commits/2ae00362179897968cfbd1fc8d61b648356769a7", "message": "Added bandit to lint for security issues."}, {"url": "https://api.github.com/repos/celery/celery/commits/56b94c327244fad4933706fbddc02eeea508d21a", "message": "Prettify test output."}, {"url": "https://api.github.com/repos/celery/celery/commits/ebd98fa4d36bb8003c2f46dbd16e9888af13720f", "message": "Parallel doc lints (#4435)\n\n* Bump sphinx.\r\n\r\n* Update copyright year. Mark the celerydocs & the celery.contrib.sphinx extensions as read_parallel_safe.\r\n\r\n* Install from git for now :(\r\n\r\n* Fix flake8 errors."}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4268", "title": "Add Var.CI integration", "body": "This pull request aims to demonstrate the power of [VarCI](https://var.ci/), the missing assistant for GitHub issues.\n\n--\n_Automated response by [Var.CI](https://var.ci)_ :robot:", "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3982", "title": "Added failing test cases for #3885", "body": "The tests were contributed by @robpogorzelski\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nAttempt to fix #3885 without hurting eager execution of the canvas.\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PromyLOPh": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4480", "title": "Application is not thread-safe", "body": "## Checklist\r\n\r\n- [x] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n```\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.5.4\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:amqp results:rpc:///\r\n\r\nresult_backend: 'rpc:///'\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\n```\r\n\r\n## Steps to reproduce\r\n\r\nUsing code from the user manual, but two threads running concurrently:\r\n\r\n```python\r\nfrom celery import Celery\r\nimport time\r\nfrom threading import Thread\r\n\r\napp = Celery('test', broker='amqp://guest@localhost//', backend='rpc://')\r\n\r\n@app.task(bind=True)\r\ndef hello(self, a, b):\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 50})\r\n    time.sleep(1)\r\n    self.update_state(state=\"PROGRESS\", meta={'progress': 90})\r\n    time.sleep(1)\r\n    return 'hello world: %i' % (a+b)\r\n\r\nif __name__ == '__main__':\r\n    def run ():\r\n        handle = hello.delay (1, 2)\r\n        print (handle.get ())\r\n    t1 = Thread (target=run)\r\n    t2 = Thread (target=run)\r\n    t1.start ()\r\n    t2.start ()\r\n    t1.join ()\r\n    t2.join ()\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\nhello world: 3\r\nhello world: 3\r\n```\r\n\r\n## Actual behavior\r\n\r\nDifferent exceptions, depending on timing. For instance:\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 456, in channel\r\n    return self.channels[channel_id]\r\nKeyError: None\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 56, in start\r\n    self._connection.default_channel, [initial_queue],\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 821, in default_channel\r\n    self._default_channel = self.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/connection.py\", line 266, in channel\r\n    chan = self.transport.create_channel(self.connection)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/transport/pyamqp.py\", line 100, in create_channel\r\n    return connection.channel()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 459, in channel\r\n    channel.open()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 432, in open\r\n    spec.Channel.Open, 's', ('',), wait=spec.Channel.OpenOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 468, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 473, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 252, in read_frame\r\n    payload = read(size)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 417, in _read\r\n    s = recv(n - len(rbuf))\r\nsocket.timeout: timed out\r\n```\r\nor\r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 154, in add_pending_result\r\n    self._maybe_resolve_from_buffer(result)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 160, in _maybe_resolve_from_buffer\r\n    result._maybe_set_cache(self._pending_messages.take(result.id))\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/utils/collections.py\", line 871, in take\r\n    raise self.Empty()\r\nqueue.Empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test.py\", line 19, in run\r\n    print (handle.get ())\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/result.py\", line 193, in get\r\n    self.backend.add_pending_result(self)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 156, in add_pending_result\r\n    self._add_pending_result(result.id, result, weak=weak)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/async.py\", line 166, in _add_pending_result\r\n    self.result_consumer.consume_from(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 81, in consume_from\r\n    return self.start(task_id)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/celery/backends/rpc.py\", line 59, in start\r\n    self._consumer.consume()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 477, in consume\r\n    self._basic_consume(T, no_ack=no_ack, nowait=False)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/messaging.py\", line 598, in _basic_consume\r\n    no_ack=no_ack, nowait=nowait)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/kombu/entity.py\", line 737, in consume\r\n    arguments=self.consumer_arguments)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/channel.py\", line 1564, in basic_consume\r\n    wait=None if nowait else spec.Basic.ConsumeOk,\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 59, in send_method\r\n    return self.wait(wait, returns_tuple=returns_tuple)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/abstract_channel.py\", line 79, in wait\r\n    self.connection.drain_events(timeout=timeout)\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 471, in drain_events\r\n    while not self.blocking_read(timeout):\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/connection.py\", line 476, in blocking_read\r\n    frame = self.transport.read_frame()\r\n  File \"/\u2026/sandbox/lib/python3.5/site-packages/amqp/transport.py\", line 254, in read_frame\r\n    'Received {0:#04x} while expecting 0xce'.format(ch))\r\namqp.exceptions.UnexpectedFrame: Received 0x3c while expecting 0xce\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4480/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kzidane": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4471", "title": "DisabledBackend when starting flask --with-threads?", "body": "I'm trying to use Celery for one of my applications and experiencing a strange behavior that I'm not sure why it's caused.\r\n\r\nTo replicate, here's a simple Flask app:\r\n\r\n    # application.py\r\n    from celery.contrib.abortable import AbortableAsyncResult\r\n    from flask import Flask\r\n    from tasks import add\r\n\r\n    app = Flask(__name__)\r\n\r\n    @app.route(\"/\")\r\n    def index():\r\n        # start the task and return its id\r\n        return add.delay(42, 50).task_id\r\n\r\n\r\n    @app.route(\"/state/<task_id>\")\r\n    def result(task_id):\r\n        # return current task state\r\n        return AbortableAsyncResult(task_id).state\r\n\r\nand here's a Celery app and a task:\r\n\r\n    # tasks.py\r\n    from celery import Celery\r\n    from celery.contrib.abortable import AbortableTask\r\n\r\n\r\n    app = Celery(\r\n        \"tasks\",\r\n        # use sqlite database as result backend (also tried rpc://)\r\n        backend=\"db+sqlite:///celerydb.sqlite\",\r\n        broker=\"pyamqp://localhost\"\r\n    )\r\n\r\n    @app.task(bind=True, base=AbortableTask)\r\n    def add(self, x, y):\r\n        return x + y\r\n\r\nRunning the Celery worker:\r\n\r\n    $ celery -A tasks worker --loglevel=info\r\n     -------------- celery@7677a80760b4 v4.1.0 (latentcall)\r\n    ---- **** ----- \r\n    --- * ***  * -- Linux-4.10.0-42-generic-x86_64-with-debian-jessie-sid 2018-01-02 20:22:48\r\n    -- * - **** --- \r\n    - ** ---------- [config]\r\n    - ** ---------- .> app:         tasks:0x7f339a52dfd0\r\n    - ** ---------- .> transport:   amqp://guest:**@localhost:5672//\r\n    - ** ---------- .> results:     sqlite:///celerydb.sqlite\r\n    - *** --- * --- .> concurrency: 8 (prefork)\r\n    -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\n    --- ***** ----- \r\n     -------------- [queues]\r\n                .> celery           exchange=celery(direct) key=celery\r\n                \r\n\r\n    [tasks]\r\n      . tasks.add\r\n\r\n    [2018-01-02 20:22:48,585: INFO/MainProcess] Connected to amqp://guest:**@127.0.0.1:5672//\r\n    [2018-01-02 20:22:48,592: INFO/MainProcess] mingle: searching for neighbors\r\n    [2018-01-02 20:22:49,608: INFO/MainProcess] mingle: all alone\r\n    [2018-01-02 20:22:49,636: INFO/MainProcess] celery@7677a80760b4 ready.\r\n\r\n\r\nRunning the Flask app:\r\n\r\n    $ FLASK_APP=application.py flask run --with-threads\r\n      * Serving Flask app \"application\"\r\n      * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\r\n\r\n\r\nHitting `/` with `curl` starts the task and returns its id without any problems:\r\n\r\n    $ curl http://localhost:5000\r\n    f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nCelery's output at this point:\r\n\r\n    [2018-01-02 20:29:28,974: INFO/MainProcess] Received task: tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef]\r\n    [2018-01-02 20:29:29,000: INFO/ForkPoolWorker-1] Task tasks.add[f1b752a1-c9c0-4a81-857d-9ecbdd4004ef] succeeded in 0.02414485500776209s: 92\r\n\r\nBut trying to get the state of the task \r\n\r\n    $ curl http://localhost:5000/state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef\r\n\r\n\r\nresults in the following error:\r\n\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n\r\neven though the backend seems to be configured per the `backend` argument to `Celery` and its output? I also tried setting `CELERY_RESULT_BACKEND` and `result_backend` using `app.conf.update`, but no luck!\r\n\r\nWhat's interesting is that this problem disappears if I drop the `--with-threads` option from the `flask run` command. Any idea why this might be caused and how to work around it if possible?\r\n\r\nAdditional details:\r\n\r\n    $ celery --version\r\n    4.1.0 (latentcall)\r\n    $ flask --version\r\n    Flask 0.12.2\r\n    Python 3.6.0 (default, Oct 30 2017, 05:46:44) \r\n    [GCC 4.8.4]\r\n\r\nFull traceback:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1982, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1614, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1517, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/_compat.py\", line 33, in reraise\r\n    raise value\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1612, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/flask/app.py\", line 1598, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n      File \"/root/application.py\", line 14, in result\r\n    return AbortableAsyncResult(task_id).state\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 436, in state\r\n    return self._get_task_meta()['status']\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/result.py\", line 375, in _get_task_meta\r\n    return self._maybe_set_cache(self.backend.get_task_meta(self.id))\r\n      File \"/opt/pyenv/versions/3.6.0/lib/python3.6/site-packages/celery/backends/base.py\", line 352, in get_task_meta\r\n    meta = self._get_task_meta_for(task_id)\r\n    AttributeError: 'DisabledBackend' object has no attribute '_get_task_meta_for'\r\n    127.0.0.1 - - [02/Jan/2018 20:46:28] \"GET /state/f1b752a1-c9c0-4a81-857d-9ecbdd4004ef HTTP/1.1\" 500 -\r\n\r\nThank you!", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4471/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "fnordian": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4465", "title": "Celery.close() leaks redis connections", "body": "## Checklist\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [X] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:3.6.3\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Linux arch:64bit imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\n\r\n```python\r\nimport time\r\nfrom celery import Celery\r\n\r\ndef run_celery_task(taskname):\r\n    with Celery(broker='redis://redis:6379/0', backend='redis://redis:6379/0') as celery:\r\n        res = celery.send_task(taskname)\r\n        print(res)\r\n\r\nfor i in range(0, 100):\r\n    run_celery_task(\"test\")\r\n\r\ntime.sleep(100)\r\n```\r\n\r\n```bash\r\nnetstat -tn | grep 6379 | grep ESTABLISHED | wc -l\r\n```\r\n## Expected behavior\r\n\r\nWhen the `with Celery`-block terminates, I expect all connections to redis being closed.\r\n\r\n## Actual behavior\r\n\r\nNot all connections are closed. When the for-loop finishes, there > 100 open connections to redis.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4465/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Kimice": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4464", "title": "Maximum recursion depth exceeded while calling a Python object", "body": "Hi, when I call celery apply_async function, sometimes it raise Exception like this. It seems like getting config failed. This bug can't always reappear. I guess celery may be not init correctly. I'm so confused with this bug.\r\n\r\ncelery==4.1.0\r\n\r\nI init celery with flask like this.\r\n\r\n```\r\ndef make_celery(app):\r\n    celery = Celery(app.import_name, backend=app.config['CELERY_RESULT_BACKEND'],\r\n                    broker=app.config['CELERY_BROKER_URL'])\r\n    celery.conf.update(app.config)\r\n    celery.config_from_object('App.celery_custom.celery_config')\r\n    TaskBase = celery.Task\r\n    class ContextTask(TaskBase):\r\n        abstract = True\r\n        def __call__(self, *args, **kwargs):\r\n            with app.app_context():\r\n                return TaskBase.__call__(self, *args, **kwargs)\r\n    celery.Task = ContextTask\r\n    return celery\r\n\r\ncelery_app = make_celery(flask_app)\r\n\r\n@celery_app.task(bind=True)\r\ndef checkInstance(self, a, b):\r\n    pass\r\n\r\ncheck_task = checkInstance.apply_async(args=['123', '123'], queue='123')\r\n```\r\n\r\n```\r\nLOG:\r\n[2017-12-26 08:42:58,659]: logs_util.py[line:64] [pid:25680] ERROR Traceback (most recent call last):\r\n  File \"./App/views/experiment_views.py\", line 377, in create_experiment_and_run\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/task.py\", line 521, in apply_async\r\n    if app.conf.task_always_eager:\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 431, in __getitem__\r\n    return getitem(k)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/collections.py\", line 280, in __getitem__\r\n    return mapping[_key]\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/utils/objects.py\", line 44, in __get__\r\n    value = obj.__dict__[self.__name__] = self.__get(obj)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 148, in data\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 911, in _finalize_pending_conf\r\n    conf = self._conf = self._load_config()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/app/base.py\", line 921, in _load_config\r\n    self.loader.config_from_object(self._config_source)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 128, in config_from_object\r\n    obj = self._smart_import(obj, imp=self.import_from_cwd)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 146, in _smart_import\r\n    return imp(path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/loaders/base.py\", line 106, in import_from_cwd\r\n    package=package,\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/utils/imports.py\", line 100, in import_from_cwd\r\n    with cwd_in_path():\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 84, in helper\r\n    return GeneratorContextManager(func(*args, **kwds))\r\nRuntimeError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4464/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "lexabug": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4462", "title": "INFO log messages land to stderr", "body": "I've set up a basic application with Django 2.0 and Celery 4.1.0 with debug task (as it described [here](http://docs.celeryproject.org/en/latest/django/first-steps-with-django.html#using-celery-with-django)) and one custom task in application tasks module.\r\nMy module code look like this:\r\n```\r\n# Create your tasks here\r\nfrom __future__ import absolute_import, unicode_literals\r\nfrom celery import shared_task\r\nfrom django.conf import settings\r\nfrom celery.utils.log import get_task_logger\r\n\r\nlogger = get_task_logger(__name__)\r\n\r\n@shared_task(name='validate_user_email', ignore_result=True, bind=True)\r\ndef validate_user_email(self, user_id):\r\n    logger.info('%s email verified', user_id)\r\n```\r\n\r\nWhen I redirect streams (stdout and stderr) to different logs  (info.log and error.log) info.log is silent while error.log contains log entries with levels WARNING and INFO.\r\n\r\nCommand I execute celery with is: `celery -A email_validation worker -Q validate --concurrency 5 --maxtasksperchild 100 -l info > info.log 2> error.log`\r\n\r\nCelery config:\r\n```\r\nCELERY_BROKER_URL = '******************'\r\nCELERY_BROKER_HEARTBEAT = 900\r\nCELERY_BROKER_HEARTBEAT_CHECKRATE = 15\r\nCELERY_RESULT_BACKEND = 'amqp'\r\nCELERY_WORKER_PREFETCH_MULTIPLIER = 1\r\nCELERY_WORKER_MAX_TASKS_PER_CHILD = 100\r\nCELERY_TASK_ACKS_LATE = True\r\nCELERY_ENABLE_UTC = False\r\nCELERY_TIMEZONE = 'US/Eastern'\r\nCELERY_WORKER_DISABLE_RATE_LIMITS = True\r\nCELERY_EVENT_QUEUE_TTL = 1\r\nCELERY_EVENT_QUEUE_EXPIRES = 60\r\nCELERY_RESULT_EXPIRES = 3600\r\nCELERY_TASK_IGNORE_RESULT = True\r\nCELERY_WORKER_HIJACK_ROOT_LOGGER = True\r\n```\r\n\r\nHow can make celery to post INFO log messages to stdout? ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4462/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "cajbecu": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4457", "title": "Connection to broker lost. Trying to re-establish the connection: OSError: [Errno 9] Bad file descriptor", "body": "## Checklist\r\n\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      (if you are not able to do this, then at least specify the Celery\r\n       version affected).\r\n- [x] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Software\r\ncelery==4.1.0\r\nkombu==4.1.0\r\namqp==2.2.2\r\nPython 3.6.1\r\nbroker: rabbitmq 3.6.14\r\nresult backend: redis\r\n\r\n## Steps to reproduce\r\n1. celery -A proj worker -Q Q1 --autoscale=10,1 -Ofair --without-gossip --without-mingle --heartbeat-interval=60 -n Q1\r\n2. celery lost connection to broker\r\n3. after restarting affected worker the connection is successfully re-established and the worker starts processing tasks\r\n\r\n## Expected behavior\r\ncelery should re-establish connection to broker\r\n\r\n## Actual behavior\r\ncelery tries to re-establish connection to broker but fails with this error message (which is repeated every second) until manually restarted:\r\n```\r\n[user] celery.worker.consumer.consumer WARNING 2017-12-18 00:38:27,078 consumer: \r\nConnection to broker lost. Trying to re-establish the connection...\r\nTraceback (most recent call last):\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/loops.py\", line 47, in asynloop\r\n    obj.controller.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/worker.py\", line 217, in register_with_event_loop\r\n    description='hub.register',\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/bootsteps.py\", line 151, in send_all\r\n    fun(parent, *args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/worker/components.py\", line 178, in register_with_event_loop\r\n    w.pool.register_with_event_loop(hub)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/prefork.py\", line 134, in register_with_event_loop\r\n    return reg(loop)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in register_with_event_loop\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/celery/concurrency/asynpool.py\", line 476, in <listcomp>\r\n    for fd in self._fileno_to_outq]\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 207, in add_reader\r\n    return self.add(fds, callback, READ | ERR, args)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/async/hub.py\", line 158, in add\r\n    self.poller.register(fd, flags)\r\n  File \"/home/user/.envs/user/lib/python3.6/site-packages/kombu/utils/eventio.py\", line 67, in register\r\n    self._epoll.register(fd, events)\r\nOSError: [Errno 9] Bad file descriptor\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4457/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "thiagogalesi": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4454", "title": "Celery does not consider authSource on mongodb backend URLs", "body": "Version: Celery 4.0.2 (from looking at the changes since then it seems there is no change addressing this issue here: https://github.com/celery/celery/commits/master/celery/backends/mongodb.py )\r\n\r\n(Edit) Confirmed with the following versions as well:\r\namqp==2.2.2\r\nbilliard==3.5.0.3\r\ncelery==4.1.0\r\nkombu==4.1.0\r\npymongo==3.6.0\r\n\r\nCelery Report\r\n\r\nsoftware -> celery:4.0.2 (latentcall) kombu:4.0.2 py:2.7.8\r\n            billiard:3.5.0.3 py-amqp:2.2.1\r\nplatform -> system:Linux arch:64bit, ELF imp:CPython\r\nloader   -> celery.loaders.default.Loader\r\nsettings -> transport:amqp results:disabled\r\n\r\n\r\n## Steps to reproduce\r\n\r\nGive Celery a Backend URL pointing to a MongoDB instance with authentication and username/password (user/pwd set on the Admin DB by default) in the format:\r\n\r\nmongodb://user:pass@your-server/your_db?authSource=admin\r\n\r\n(Please see http://api.mongodb.com/python/current/examples/authentication.html#default-database-and-authsource and http://api.mongodb.com/python/current/api/pymongo/mongo_client.html?highlight=authsource )\r\n\r\n## Expected behavior\r\n\r\nCelery authenticates the user in the admin database (this is the same as passing --authenticationDatabase to the mongo client or the same url to MongoClient)\r\n\r\n## Actual behavior\r\n\r\nCelery tries to authenticate the user on the your_db database (failing to authenticate)\r\n\r\n## Workaround (not recommended)\r\n\r\nChange the db on the URL to /admin (this db shouldn't be used to store arbitrary data normally)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4454/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yanliguo": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4451", "title": "Celery (4.1.0) worker stops to consume new message when actives is empty  ", "body": "Hi there,\r\n   I was using celery to dispatch some long running task recently, and finding a way to disable prefetch. Now the config is:\r\n\r\nacks_late = True\r\nconcurrency = 1\r\nprefetch_multiplier = 1\r\n-Ofair\r\n\r\nActually, workers are still prefetching tasks. And I also have a monitor job to revoke tasks when a task is stuck in reserved state for a long time (let's say 5 minutes) or the task is outputing valid result.  \r\n\r\nOne thing wired is that, some workers stopped consuming new tasks when the actives is empty and the reseved task is revoked.  Has anybody ever met with this issue ? any help will be appreciated.\r\n\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4451/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "italomaia": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4450", "title": "PENDING state, what if it meant just one thing?", "body": "According to docs, PENDING state has the following meaning:\r\n\r\n> Task is waiting for execution or unknown. Any task id that\u2019s not known is implied to be in the pending state.\r\n\r\nAre there plans to make pending mean one thing only? It is quite confusing to handle a state that can mean \"waiting\" or \"lost\". ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4450/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "dfresh613": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4449", "title": "ValueFormatError when processing chords with couchbase result backend", "body": "Hi it seems like when I attempt to process groups of chords, the couchbase result backend is consistently failing to unlock the chord when reading from the db:\r\n\r\n`celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()`\r\n\r\nThis behavior does not occur with the redis result backend, i can switch between them and see that the error unlocking only occurs on couchbase.\r\n\r\n## Steps to reproduce\r\nAttempt to process a chord with couchbase backend using pickle serialization.\r\n\r\n## Expected behavior\r\nChords process correctly, and resulting data is fed to the next task\r\n\r\n## Actual behavior\r\nCelery is unable to unlock the chord from the result backend\r\n\r\n## Celery project info: \r\n```\r\ncelery -A ipaassteprunner report\r\n\r\nsoftware -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.10\r\n            billiard:3.5.0.3 py-amqp:2.2.2\r\nplatform -> system:Darwin arch:64bit imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:couchbase://isadmin:**@localhost:8091/tasks\r\n\r\ntask_serializer: 'pickle'\r\nresult_serializer: 'pickle'\r\ndbconfig: <ipaascommon.ipaas_config.DatabaseConfig object at 0x10fbbfe10>\r\ndb_pass: u'********'\r\nIpaasConfig: <class 'ipaascommon.ipaas_config.IpaasConfig'>\r\nimports:\r\n    ('ipaassteprunner.tasks',)\r\nworker_redirect_stdouts: False\r\nDatabaseConfig: u'********'\r\ndb_port: '8091'\r\nipaas_constants: <module 'ipaascommon.ipaas_constants' from '/Library/Python/2.7/site-packages/ipaascommon/ipaas_constants.pyc'>\r\nenable_utc: True\r\ndb_user: 'isadmin'\r\ndb_host: 'localhost'\r\nresult_backend: u'couchbase://isadmin:********@localhost:8091/tasks'\r\nresult_expires: 3600\r\niconfig: <ipaascommon.ipaas_config.IpaasConfig object at 0x10fbbfd90>\r\nbroker_url: u'amqp://guest:********@localhost:5672//'\r\ntask_bucket: 'tasks'\r\naccept_content: ['pickle']\r\n```\r\n### Additional Debug output\r\n```\r\n[2017-12-13 15:39:57,860: INFO/MainProcess] Received task: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2]  ETA:[2017-12-13 20:39:58.853535+00:00] \r\n[2017-12-13 15:39:57,861: DEBUG/MainProcess] basic.qos: prefetch_count->27\r\n[2017-12-13 15:39:58,859: DEBUG/MainProcess] TaskPool: Apply <function _fast_trace_task at 0x10b410b90> (args:('celery.chord_unlock', 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', {'origin': 'gen53678@silo2460', 'lang': 'py', 'task': 'celery.chord_unlock', 'group': None, 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', u'delivery_info': {u'priority': None, u'redelivered': False, u'routing_key': u'celery', u'exchange': u''}, 'expires': None, u'correlation_id': 'e3139ae5-a67d-4f0c-8c54-73b1e19433d2', 'retries': 311, 'timelimit': [None, None], 'argsrepr': \"('90c64bef-21ba-42f9-be75-fdd724375a7a', {'chord_size': 2, 'task': 'ipaassteprunner.tasks.transfer_data', 'subtask_type': None, 'kwargs': {}, 'args': (), 'options': {'chord_size': None, 'chain': [...], 'task_id': '9c6b5e1c-2089-4db7-9590-117aeaf782c7', 'root_id': '0acd3e0d-7532-445c-8916-b5fc8a6395ab', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', 'reply_to': '0a58093c-6fdd-3458-9a34-7d5e094ac6a8'}, 'immutable': False})\", 'eta': '2017-12-13T20:39:58.853535+00:00', 'parent_id': 'c27c9565-19a6-4683-8180-60f0c25007e9', u'reply_to':... kwargs:{})\r\n[2017-12-13 15:40:00,061: DEBUG/MainProcess] basic.qos: prefetch_count->26\r\n[2017-12-13 15:40:00,065: DEBUG/MainProcess] Task accepted: celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] pid:53679\r\n[2017-12-13 15:40:00,076: INFO/ForkPoolWorker-6] Task celery.chord_unlock[e3139ae5-a67d-4f0c-8c54-73b1e19433d2] retry: Retry in 1s: ValueFormatError()\r\n```\r\n\r\n### Stack trace from chord unlocking failure\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/builtins.py\", line 75, in unlock_chord\r\n    raise self.retry(countdown=interval, max_retries=max_retries)\r\n  File \"/Library/Python/2.7/site-packages/celery/app/task.py\", line 689, in retry\r\n    raise ret\r\nRetry: Retry in 1s\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4449/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "yutkin": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4438", "title": "Celery doesn't write RECEIVED state into MongoDB", "body": "When a number of tasks in a queue surpass a number of workers, new added tasks are not writing in a backend. In other words, I want to write task state (RECEIVED) into DB immediately after its invocation. It is possible?\r\n\r\nP.S. I'm using MongoDB as backend. ", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4438/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "canassa": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4426", "title": "Task is executed twice when the worker restarts", "body": "Currently using Celery 4.1.0\r\n\r\n## Steps to reproduce\r\n\r\nStart a new project using RabbitMQ and register the following task:\r\n\r\n```python\r\nfrom django.core.cache import cache\r\n\r\n@shared_task(bind=True)\r\ndef test_1(self):\r\n    if not cache.add(self.request.id, 1):\r\n        raise Exception('Duplicated task {}'.format(self.request.id))\r\n```\r\n\r\nNow start 2 workers. I used gevent with a concurrency of 25 for this test:\r\n\r\n```\r\ncelery worker -A my_proj -Q my_queue -P gevent -c 25\r\n```\r\n\r\nOpen a python shell and fire a a bunch of tasks:\r\n\r\n```python\r\nfrom myproj.tasks import test_1\r\n\r\nfor i in range(10000):\r\n    test_1.apply_async()\r\n```\r\n\r\nNow quickly do a warm shutdown (Ctrl+c) in one of the workers while it's still processing the tasks, you should see the errors popping in the second worker:\r\n\r\n```\r\nERROR    Task my_proj.tasks.test_1[e28e6760-1371-49c9-af87-d196c59375e9] raised unexpected: Exception('Duplicated task e28e6760-1371-49c9-af87-d196c59375e9',)\r\nTraceback (most recent call last):\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 374, in trace_task\r\n    R = retval = fun(*args, **kwargs)\r\n  File \"/code/virtualenv/CURRENT/lib/python3.5/site-packages/celery/app/trace.py\", line 629, in __protected_call__\r\n    return self.run(*args, **kwargs)\r\n  File \"/code/scp/python/my_proj/tasks.py\", line 33, in test_1\r\n    raise Exception('Duplicated task {}'.format(self.request.id))\r\nException: Duplicated task e28e6760-1371-49c9-af87-d196c59375e9\r\n```\r\n\r\n## Expected behavior\r\n\r\nSince I am not using late acknowledgment and I am not killing the workers I wasn't expecting the tasks to execute again.\r\n\r\n## Actual behavior\r\n\r\nThe tasking are being executed twice, this is causing some problems in our servers because we restart our works every 15 minutes or so in order to avoid memory leaks.", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4426/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "kn-id": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4424", "title": "Celery Crash: Unrecoverable error when using QApplication in main process", "body": "Error happens when there's some queues already added before worker started and max task per child is 1\r\n\r\n## Checklist\r\n- [ ] I have included the output of ``celery -A proj report`` in the issue.\r\n      software -> celery:4.1.0 (latentcall) kombu:4.1.0 py:2.7.12\r\n                         billiard:3.5.0.3 py-amqp:2.2.2\r\n      platform -> system:Linux arch:64bit, ELF imp:CPython\r\n      loader   -> celery.loaders.app.AppLoader\r\n      settings -> transport:amqp results:disabled\r\n- [ ] I have verified that the issue exists against the `master` branch of Celery.\r\n\r\n## Steps to reproduce\r\n1. create instance of QApplication when main worker started\r\n```\r\n@celeryd_after_setup.connect\r\nfrom PyQt4.QtGui import QApplication\r\ndef init_worker(sender, **k):\r\n    QApplication([])\r\n```\r\n2. create task\r\n```\r\n@app.task\r\ndef job():\r\n    print 'hello'\r\n```\r\n3. add some queues (2 - 3 per thread. so if there's 4 worker child then there's 8 or more queues)\r\n\r\n4. start worker with max task per child 1\r\n```\r\ncelery worker -A proj --loglevel=INFO --max-tasks-per-child=1\r\n```\r\n\r\n## Expected behavior\r\n- run queues successfuly\r\n## Actual behavior\r\nCelery Crash after 1 queue per child\r\n```\r\n2017-12-07 10:20:19,594: INFO/MainProcess] Received task: proj.tasks.job[c6413dc2-ad62-4033-a69b-39556276f789]  \r\n[2017-12-07 10:20:19,595: INFO/MainProcess] Received task: proj.tasks.job[f1c10b1c-03ae-4220-9c7f-2cdf4afc61e3]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[d54f4554-4517-470f-8e14-adedcb93a46e]  \r\n[2017-12-07 10:20:19,596: INFO/MainProcess] Received task: proj.tasks.job[6255e5e6-d4c8-4d87-8075-642bca9e6a6d]  \r\n[2017-12-07 10:20:19,700: INFO/ForkPoolWorker-1] Task proj.tasks.job[ca856d5c-f3cc-45d4-9fbc-665753f5d1d2] succeeded in 0.00115608799388s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-4] Task proj.tasks.job[9ae27611-e8e5-4e08-9815-1e56e2ad1565] succeeded in 0.00130111300678s: None\r\n[2017-12-07 10:20:19,701: INFO/ForkPoolWorker-3] Task proj.tasks.job[f5aa7c6a-4142-4a38-8814-c60424196826] succeeded in 0.00129756200477s: None\r\n[2017-12-07 10:20:19,702: INFO/ForkPoolWorker-2] Task proj.tasks.job[eb13b5c5-8865-4992-8b9e-6672c909fd59] succeeded in 0.00100053900678s: None\r\n[2017-12-07 10:20:19,710: INFO/MainProcess] Received task: proj.tasks.job[01700061-c69c-4f4c-abf2-e6ba200772bd]  \r\n[2017-12-07 10:20:19,711: INFO/MainProcess] Received task: proj.tasks.job[a27d7a7b-2c58-4689-8b98-2c0a4ceaea9f]  \r\n[2017-12-07 10:20:19,713: INFO/MainProcess] Received task: proj.tasks.job[4c4a5685-23d5-4178-89cc-9ce4ad5a3509]  \r\n[2017-12-07 10:20:19,714: INFO/MainProcess] Received task: proj.tasks.job[44a079a3-aacf-48c5-a76b-a061bdced1d6]\r\n[2017-12-07 01:41:03,591: CRITICAL/MainProcess] Unrecoverable error: AttributeError(\"'error' object has no attribute 'errno'\",)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/worker.py\", line 203, in start\r\n    self.blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 370, in start\r\n    return self.obj.start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 320, in start\r\n    blueprint.start(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/bootsteps.py\", line 119, in start\r\n    step.start(parent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/consumer/consumer.py\", line 596, in start\r\n    c.loop(*c.loop_args())\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/worker/loops.py\", line 88, in asynloop\r\n    next(loop)\r\n  File \"/usr/local/lib/python2.7/dist-packages/kombu/async/hub.py\", line 354, in create_loop\r\n    cb(*cbargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 444, in _event_process_exit\r\n    self.maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1307, in maintain_pool\r\n    self._maintain_pool()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1298, in _maintain_pool\r\n    joined = self._join_exited_workers()\r\n  File \"/usr/local/lib/python2.7/dist-packages/billiard/pool.py\", line 1165, in _join_exited_workers\r\n    self.process_flush_queues(worker)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 1175, in process_flush_queues\r\n    readable, _, _ = _select(fds, None, fds, timeout=0.01)\r\n  File \"/usr/local/lib/python2.7/dist-packages/celery/concurrency/asynpool.py\", line 183, in _select\r\n    if exc.errno == errno.EINTR:\r\nAttributeError: 'error' object has no attribute 'errno'\r\n```", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4424/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jenstroeger": {"issues": [{"url": "https://api.github.com/repos/celery/celery/issues/4420", "title": "How to unpack serialzied task arguments?", "body": "When iterate over all currently scheduled tasks\r\n```python\r\nfor task in chain.from_iterable(my_app.control.inspect().scheduled().values()):\r\n    print(task)\r\n```\r\nI get a dictionary `task['request']` which contains a serialization of the tasks\u2019 arguments in `task['request']['args']` (and `'kwargs`). Both are strings:\r\n```\r\n'args': \"('5', {'a': 'b'})\",\r\n'kwargs': '{}', \r\n```\r\nIt\u2019s not [JSON](https://www.json.org/) nor [msgpack](https://msgpack.org/). How can I unpack that `args` string into a Python tuple again? Celery must have a helper function for that somewhere? (Anything to do with `argsrepr` and `kwargsrepr` and [`saferepr`](https://github.com/celery/celery/blob/master/celery/utils/saferepr.py)?)", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/4420/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Chris7": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/ba2dec7956782c84068ef779e554fb07de524beb", "message": "Propagate arguments to chains inside groups (#4481)\n\n* Remove self._frozen from _chain run method\r\n\r\n* Add in explicit test for group results in chain"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4482", "title": "Add docker-compose and base dockerfile for development", "body": "This adds a docker based development environment. This removes the need for users to install their own rabbit/redis/virtual environment/etc. to begin development of celery. I use this myself (since after moving to docker I have essentially nothing installed on my computer anymore) and saw interest in it from issue #4334 so thought a PR may be appropriate to share my setup.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "auvipy": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/028dbe4a4d6786d56ed30ea49971cc5415fffb4b", "message": "update version to 4.2.0"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4459", "title": "[wip] #3021 bug fix ", "body": "Fixes #3021", "author_association": "OWNER"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zpl": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/442f42b7084ff03cb730ca4f452c3a47d9b8d701", "message": "task_replace chord inside chord fix (fixes #4368) (#4369)\n\n* task_replace chord inside chord fix\r\n\r\n* Complete fix for replace inside chords with tests\r\n\r\n* Add integration tests for add_to_chord\r\n\r\n* Fix JSON serialisation in tests\r\n\r\n* Raise exception when replacing signature has a chord"}], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4302", "title": "Ignore celery.exception.Ignore on autoretry", "body": "Autoretry for task should ignore celery.exception.Ignore, which is generated by self.replace()\r\n\r\notherwise, it goes to infinite loop.\r\n\r\n```python\r\n@app.task(autoretry_for=(Exception,), default_retry_delay=1,\r\n          max_retries=None,\r\n          bind=True,acks_late=True)\r\ndef TaskA(self):\r\n    raise self.replace(TaskB.s()) # Always retrying because of replace\r\n\r\n```\r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jdufresne": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/5eba340aae2e994091afb7a0ed7839e7d944ee13", "message": "Pass python_requires argument to setuptools (#4479)\n\nHelps pip decide what version of the library to install.\r\n\r\nhttps://packaging.python.org/tutorials/distributing-packages/#python-requires\r\n\r\n> If your project only runs on certain Python versions, setting the\r\n> python_requires argument to the appropriate PEP 440 version specifier\r\n> string will prevent pip from installing the project on other Python\r\n> versions.\r\n\r\nhttps://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords\r\n\r\n> python_requires\r\n>\r\n> A string corresponding to a version specifier (as defined in PEP 440)\r\n> for the Python version, used to specify the Requires-Python defined in\r\n> PEP 345."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "freakboy3742": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a4abe149aa00b0f85024a6cac64fd984cb2d0a6b", "message": "Refs #4356: Handle \"hybrid\" messages that have moved between Celery versions (#4358)\n\n* handle \"hybrid\" messages which have passed through a protocol 1 and protocol 2 consumer in its life.\r\n\r\nwe detected an edgecase which is proofed out in https://gist.github.com/ewdurbin/ddf4b0f0c0a4b190251a4a23859dd13c#file-readme-md which mishandles messages which have been retried by a 3.1.25, then a 4.1.0, then again by a 3.1.25 consumer. as an extension, this patch handles the \"next\" iteration of these mutant payloads.\r\n\r\n* explicitly construct proto2 from \"hybrid\" messages\r\n\r\n* remove unused kwarg\r\n\r\n* fix pydocstyle check\r\n\r\n* flake8 fixes\r\n\r\n* correct fix for misread pydocstyle error"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "djw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/9ce3df9962830a6f9e0e68005bdeec1092e314e4", "message": "Corrected the default visibility timeout (#4476)\n\nAccording to kombu, the default visibility timeout is 30 minutes.\r\n\r\nhttps://github.com/celery/kombu/blob/3a7cdb07c9bf75b54282274d711af15ca6ad5d9f/kombu/transport/SQS.py#L85"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "alexgarel": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/0ffd36fbf9343fe2f6ef7744a14ebfbec5ac86b6", "message": "request on_timeout now ignores soft time limit exception (fixes #4412) (#4473)\n\n* request on_timeout now ignores soft time limit exception (closes #4412)\r\n\r\n* fix quality"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "georgepsarakis": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/a7915054d0e1e896c9ccf5ff0497dd8e3d5ed541", "message": "Integration test to verify PubSub unsubscriptions (#4468)\n\n* [Redis Backend] Integration test to verify PubSub unsubscriptions\r\n\r\n* Import sequence for isort check\r\n\r\n* Re-order integration tasks import"}, {"url": "https://api.github.com/repos/celery/celery/commits/9ab0971fe28462b667895d459d198ef6dd761c89", "message": "Add --diff flag in isort in order to display changes (#4469)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pachewise": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/c8f9b7fbab3fe8a8de5cbae388fca4edf54bf503", "message": "Fixes #4452 - Clearer Django settings documentation (#4467)\n\n* reword django settings section in first steps\r\n\r\n* anchor link for django admonition\r\n\r\n* mention django-specific settings config"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "hclihn": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/3ca1a54e65762ccd61ce728b3e3dfcb622fc0c90", "message": "Allow the shadow kwarg and the shadow_name method to set shadow properly (#4381)\n\n* Allow the shadow kwarg and the shadow_name method to set shadow properly \r\n\r\nThe shadow_name option in the @app.task() decorator (which overrides the shadow_name method in the Task class) and the shadow keyword argument of Task.apply_async() don't work as advertised.\r\nThis moves the shadow=... out of the 'if self.__self__ is not None:' block and allows shadow to be set by the shadow keyword argument of Task.apply_async() or the shadow_name method in the Task class (via, say, the shadow_name option in the @app.task() decorator).\r\n\r\n* Added a test to cover calling shadow_name().\r\n\r\n* Sort imports.\r\n\r\n* Fix missing import."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "AlexHill": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/fde58ad677a1e28effd1ac13f1f08f7132392463", "message": "Run chord_unlock on same queue as chord body - fixes #4337 (#4448)"}, {"url": "https://api.github.com/repos/celery/celery/commits/25f5e29610b2224122cf10d5252de92b4efe3e81", "message": "Support chords with empty headers (#4443)"}, {"url": "https://api.github.com/repos/celery/celery/commits/7ef809f41c1e0db2f6813c9c3a66553ca83c0c69", "message": "Add bandit baseline file with contents this time"}, {"url": "https://api.github.com/repos/celery/celery/commits/fdf0928b9b5698622c3b8806e2bca2d134df7fa3", "message": "Add bandit baseline file"}, {"url": "https://api.github.com/repos/celery/celery/commits/10f06ea1df75f109bf08fb8d42f9977cabcd7e0e", "message": "Fix length-1 and nested chords (#4393 #4055 #3885 #3597 #3574 #3323) (#4437)\n\n* Don't convert single-task chord to chain\r\n\r\n* Fix evaluation of nested chords"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "pokoli": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/63c747889640bdea7753e83373a3a3e0dffc4bd9", "message": "Add celery_tryton integration on framework list (#4446)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "azaitsev": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/83872030b00a1ac75597ed3fc0ed34d9f664c6c1", "message": "Fixed wrong value in example of celery chain (#4444)"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "matteius": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/976515108a4357397a3821332e944bb85550dfa2", "message": "make astimezone call in localize more safe (#4324)\n\nmake astimezone call in localize more safe; with tests"}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "myw": {"issues": [], "commits": [{"url": "https://api.github.com/repos/celery/celery/commits/4dc8c001d063a448d598f4bbc94056812cf15fc8", "message": "Add Mikhail Wolfson to CONTRIBUTORS.txt (#4439)"}, {"url": "https://api.github.com/repos/celery/celery/commits/dd2cdd9c4f8688f965d7b5658fa4956d083a7b8b", "message": "Resolve TypeError on `.get` from nested groups (#4432)\n\n* Accept and pass along the `on_interval` in ResultSet.get\r\n\r\nOtherwise, calls to .get or .join on ResultSets fail on nested groups.\r\nFixes #4274\r\n\r\n* Add a unit test that verifies the fixed behavior\r\n\r\nVerified that the unit test fails on master, but passes on the patched version. The\r\nnested structure of results was borrowed from #4274\r\n\r\n* Wrap long lines\r\n\r\n* Add integration test for #4274 use case\r\n\r\n* Switch to a simpler, group-only-based integration test\r\n\r\n* Flatten expected integration test result\r\n\r\n* Added back testcase from #4274 and skip it if the backend under test does not support native joins.\r\n\r\n* Fix lint.\r\n\r\n* Enable only if chords are allowed.\r\n\r\n* Fix access to message."}], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": []}, "johnarnold": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4490", "title": "Add task properties to AsyncResult, store in backend", "body": "*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "PauloPeres": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4484", "title": "Adding the CMDS for Celery and Celery Beat to Run on Azure WebJob", "body": "This is what worked for us!\r\n\r\n## Description\r\nCreated the .cmd files to be used on Azure.\r\nJust going into Web Jobs on Azure servers and creating a Continuous Web Job, and uploading the zip files of the celery should work.\r\nEach folder should have a separeted Web Job.\r\nAlso whoever use should take a look where their celery package is\r\n\r\n\r\nThis pull request don't fix any bugs, it's just a new \"Helper\" for the ones who want to use Celery into Azure servers.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "zengdahuaqusong": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4475", "title": "separate backend database from login database", "body": "by setting 'backend_database' in MONGODB_BACKEND_SETTINGS, users can separate backend database from login database. Which means they can authenticate mongodb with 'database', while data is actually writing to 'backend_database'\r\n\r\n*Note*: Before submitting this pull request, please review our [contributing\r\nguidelines](https://docs.celeryproject.org/en/master/contributing.html).\r\n\r\n## Description\r\n\r\nPlease describe your pull request.\r\n\r\nNOTE: All patches should be made against master, not a maintenance branch like\r\n3.1, 2.5, etc.  That is unless the bug is already fixed in master, but not in\r\nthat version series.\r\n\r\nIf it fixes a bug or resolves a feature request,\r\nbe sure to link to that issue via (Fixes #4412) for example.\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "robyoung": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4474", "title": "Replace TaskFormatter with TaskFilter", "body": "Replacing `TaskFormatter` with a `TaskFilter` as that can be more easily reused when overriding the logging system.\r\n\r\nI've left the `TaskFormatter` in but marked it as deprecated in case people are using it.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "neaket360pi": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4472", "title": "Cleanup the mailbox's producer pool after forking", "body": "Fixes https://github.com/celery/celery/issues/3751\r\n\r\nIf the mailbox is used before forking the workers will\r\nnot be able to broadcast messages.  This is because the producer pool\r\ninstance on the mail box will be `closed.`  This fix will cause the\r\nmailbox to load the producer pool again after forking.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "charettes": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4456", "title": "Perform a serialization roundtrip on eager apply_async.", "body": "Fixes #4008 \r\n\r\n/cc @AlexHill ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jurrian": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4442", "title": "Fix for get() follow_parents exception handling (#1014)", "body": "## Description\r\n\r\nWhen working with chains, the situation might be that the first chain task has failed. Calling `x.get(propagate=True, follow_parents=True)` should cope with that since it checks if the parent tasks have raised exceptions.\r\n\r\nHowever, in my experience, when `x.get(propagate=True, follow_parents=True)` is called the failed task is still pending at that moment and will become failed seconds later. This creates a bug in the behaviour of `follow_parents`.\r\n\r\nThis fix calls `get(propagate=True, follow_parents=False)` on each parent instead, which will cause it to raise directly when the parent task becomes failed. It looks like `maybe_throw()` can be safely replaced since it is called by `get()` at some other place.\r\n\r\nIn order to be more comprehensive, I extended the documentation for `follow_parents`.\r\n\r\nI am not experienced enough in this project to see the whole picture, so please advise on possible problems that might arise with this fix for #1014 .\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Checkroth": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4285", "title": "Add failing test for broker url priority ref issue #4284", "body": "## Description\r\nThis PR is to assist in showing the issue described here: https://github.com/celery/celery/issues/4284\r\n\r\nAll this PR does is add a failing test that _should_ be passing, if the issue above is resolved.\r\n\r\nThis PR does not solve the issue mentioned. I leave that up to the discretion of a more knowledgeable party, if they decide that it is indeed an issue at all. I do not expect this PR to be merged unless the issue is resolved and you want this test to remain in the repository.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jamesmallen": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4262", "title": "Disable backend for beat", "body": "## Description\r\n\r\nWhen using `beat` (at least in its standalone form), it is not necessary to subscribe to events on the result backend. These subscriptions happen in the `on_task_call` method of the backend. This PR ensures that no `SUBSCRIBE` messages are sent.\r\n\r\nFixes #4261 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bluestann": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4241", "title": "Use Cherami as a broker", "body": "Hi, \r\n\r\nI have added support for celery to use [Cherami](https://eng.uber.com/cherami/) as a broker. \r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "harukaeru": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4239", "title": "Changed raise RuntimeError to RuntimeWarning when task_always_eager is True", "body": "## Description\r\nThis PR changes RuntimeError to RuntimeWarning when `task_always_eager` is True.\r\n\r\nSometimes I tested what is related to celery task using `task_always_eager` in celery v3.\r\n(The tests are not only celery task but codes using celery task and other codes)\r\n\r\nI know the config cannot be completely tested async task but it's useful when doing rush works.\r\nI never use them in the production code but in the test, I use.\r\n\r\nI read the issue (https://github.com/celery/celery/issues/2275) was produced and the commit (https://github.com/celery/celery/commit/c71cd08fc72742efbfc846a81020939aa3692501) resolved the above.\r\n\r\n\r\nI almost agree with them but people who want to test perfectly only don't turn on `task_always_eager`.\r\nOthers who want to test synchronously also want to use `task_always_eager`.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "erebus1": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4227", "title": "fix crontab description of month_of_year behaviour", "body": "\r\n## Description\r\nIn [docs](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html#crontab-schedules) said:\r\n\r\n> crontab(0, 0, month_of_year='*/3') -> Execute on the first month of every quarter.\r\n\r\nWhich is a bit confused, because, in fact it will run task each day on the first month of every quarter.\r\n\r\n\r\n\r\n\r\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "rpkilby": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4212", "title": "Fix celery_worker test fixture", "body": "This is an attempt to fix #4088. Thanks to @karenc for providing a [breakdown](https://github.com/celery/celery/issues/4088#issuecomment-321287239) of what's happening.\r\n\r\nSide note:\r\nUsing `celery_worker` over `celery_session_worker` in the integration tests is a bit slower, given the worker startup/teardown for each test. This could be faster if the worker was scoped at a module level, but it's not possible to change the scope a fixture. The [recommendation](https://github.com/pytest-dev/pytest/issues/2300) is to simply create a fixture per scope. \r\n", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "bbgwilbur": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4077", "title": "Catch NotRegistered exception for errback", "body": "If the errback task is not registered in the currently running worker, the arity_greater check will fail. We can just assume its going to work as an old-style signature and deal with the possible error if it doesn't later.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ChillarAnand": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/4013", "title": "Updated commands to kill celery workers", "body": "```\r\nchillar+  1696 26093  1 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:MainProcess] -active- (worker -l info -A t)\r\nchillar+  1715  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-1]\r\nchillar+  1716  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-2]\r\nchillar+  1717  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-3]\r\nchillar+  1718  1696  0 20:08 pts/26   00:00:00 [celeryd: celery@pavilion:PoolWorker-4]\r\n```\r\nWith latest version, celery worker process names seems changed. So, the commands used to kill those process needs to be updated.", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "Taywee": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3852", "title": "celery.beat.Scheduler: fix _when to make tz-aware (#3851)", "body": "Fixes #3851 ", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "JackDanger": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3838", "title": "Removing last references to task_method", "body": "AFAICT the code removed in this PR only served to support the\n(now-removed) celery.contrib.methods.task_method() function.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "gugu": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3834", "title": "Handle ignore_result", "body": "## Description\r\n\r\nCurrently (according to my checks and code grep) ignore_result option is a stub, it does nothing. This patch skips backend calls for tasks with `ignore_result=True`\r\n\r\nThis is needed to minimize effect of broken redis backend support", "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/pulls/3815", "title": "Chord does not clean up subscribed channels in redis. Fixes #3812", "body": "This pull requests fixes issue, when chord gets result for subtasks, but does not do `UNSUBSCRIBE` command for them.\r\n\r\n**What does this patch fix**:\r\n\r\nWithout this patch `chord(task() for i in range(50))` creates 51 subscriptions. After patch it will create 1\r\n\r\n**What this patch does not fix**:\r\n\r\nEvery task, which result is not consumed, creates redis subscription. Even if `ignore_result` is specified. I plan to submit separate patch for this issue\r\n\r\nI think, that changing old slow redis behavior to new fast and broken was not a good idea, but as soon as choice made, we need to make it usable", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "justdoit0823": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3757", "title": "Add supporting for task execution working with tornado coroutine", "body": "With this future, we can use tornado coroutine in celery app task when pool implementation is gevent. The main idea here is using a standalone tornado ioloop thread to execute coroutine task. If a task is a coroutine, the executor will register a callback on the tornado ioloop and switch to the related gevent hub. Here using a callback means that the coroutine will be totally executed inside the tornado ioloop thread. When the coroutine is finished, the executor will spawn a new greenlet which will switch to the previous task greenlet with the coroutine's result. Then task greenlet returns the result to the outer function inside the same greenlet. \r\n\r\nWe can write code in celery task as following:\r\n\r\n\r\n```python\r\nfrom celery import app\r\nfrom tornado import gen\r\n\r\n@app.task\r\n@gen.coroutine\r\ndef task_A():\r\n\r\n    process_1()\r\n    res = yield get_something()\r\n    do_something_with_res()\r\n    return res\r\n\r\n@gen.coroutine\r\ndef get_something():\r\n\r\n    res = {}\r\n    return res\r\n```\r\n\r\nThis is interesting when we use the tornado framework to write a project, we can easily divide partial logic into a celery task without any modifications. And in tornado 5.0, which will support Python 3 native coroutine, this future can connect more things together. I think people will like this.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "regisb": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3684", "title": "Refer worker request info to absolute time", "body": "Previously, the time_start attribute of worker request objects refered\r\nto a timestamp relative to the monotonic time value. This caused\r\ntime_start attributes to be at a time far in the past. We fix this by\r\ncalling the on_accepted callback with an absolute time_accepted\r\nattribute.\r\n\r\nNote that the current commit does not change the\r\ncelery.concurrency.base API, although it would probably make sense to\r\nrename the \"monotonic\" named argument to \"time\".\r\n\r\nThis fixes the problem described in http://stackoverflow.com/questions/20091505/celery-task-with-a-time-start-attribute-in-1970/\r\n\r\nNote that there are many different ways to solve this problem; in the proposed implementation I choose to modify the default value of a keyword argument that is AFAIK never used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "jcsaaddupuy": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3592", "title": "fixes exception deserialization when not using pickle", "body": "fix for #3586 \r\n\r\nThis PR try to fix how exceptions are deserialized when using a serializer different than pickle.\r\n\r\nThis avoid to [create new types](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L46) for exceptions, by doing 2 things : \r\n\r\n- store the exception module in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L243-L245) and reuse it in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L249-L258) instead of using raw `__name__` as module name\r\n\r\n- in [ celery/celery/utils/serialization.py::create_exception_cls](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/utils/serialization.py#L86-L98), try to find the exception class either from  `__builtins__`, or from the excetion module. Fallback on current behavior (which may still be wrong)\r\n\r\nAlso, it uses `exc.args` in [celery/celery/backends/base.py::Backend::prepare_exception](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L247) and pass it to the exception constructor in [celery/celery/backends/base.py::Backend::exception_to_python](https://github.com/jcsaaddupuy/celery/blob/8d4e613e24f6561fdaafd4e6ede582ceac882804/celery/backends/base.py#L255), instead of using `str(exc.message)` which could lead to unwanted behavior\r\n\r\nThis won't work in every cases. If a class is defined locally in a function, this code won't be able to import the exception class using `import_module` and the old (wrong) behavior will still be used.", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "astewart-twist": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3293", "title": "Deserialize json string prior to inclusion in CouchDB doc", "body": "", "author_association": "CONTRIBUTOR"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "chadrik": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/3043", "title": "group and chord: subtasks are not executed until entire generator is consumed", "body": "When passing a generator to `group` or `chord`, sub-tasks are not submitted to the backend for execution until the entire generator is consumed.  The expected result is that subtasks are submitted during iteration, as soon as they are yielded.  This can have a big impact on performance if generators yield subtasks over a long period of time.\n\nI also opened issue #3021 on the subject.  [This explanation](https://github.com/celery/celery/issues/3021#issuecomment-176729202) from @eli-green I think is pretty on point.\n\nSo far I've only added support for redis.  I started looking at the other backends but I ran out of time.  It would be great to get some feedback on what I have so far and to get some thoughts on how difficult it will be to add for the other backends.\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "m4ddav3": {"issues": [], "commits": [], "pull_requests": [{"url": "https://api.github.com/repos/celery/celery/pulls/2881", "title": "Database Backend: Use configured serializer, instead of always pickling.", "body": "Use the BLOB as an sa.BLOB\nSerialise the result an add to the db as bytes\n", "author_association": "NONE"}], "issue_comments": [], "commit_comments": [], "review_comments": []}, "ask": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15545", "body": "README: Fix typo \"task.register\" -> \"tasks.register\". Closed by 8b5685e5b11f8987ba56c28ccb47f6c139541384. Thanks gregoirecachet)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15545/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546", "body": "What version is this? I thought I had fixed this already.\n\nThe examples in README should be updated. I think the best way of defining tasks now is using a Task class:\n\n```\n from celery.task import Task\n\n class MyTask(Task):\n     name = \"myapp.mytask\"\n\n     def run(self, x, y):\n         return x * y\n```\n\nand then\n\n```\n>>> from myapp.tasks import MyTask\n>>> MyTask.delay(2, 2)\n```\n\nas this makes it easier later to define a default routing_key for your task etc.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15546/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205", "body": "This works now. I remember it didn't at some point, and I remember I fixed it, so unless it still doesn't work for you I'm closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17205/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554", "body": "oh, that's bad. Got to get rid of yadayada dependency anyway, it's been an old trashbag for utilities, and all that is used from it now is the PickledObjectField.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15554/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555", "body": "Remove yadayada dependency. that means we've copy+pasted the\nPickledObjectField, when will djangosnippets ever die? :( Closed by fb582312905c5a1e001b6713be78ee2154b13204. Thansk\ngregoirecachet!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15555/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182", "body": "I'm not sure if that's so bad. Test requirements is not the same as install requirements. Sad the Django test runner is so broken. Maybe I can get it in somewhere else. \n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353", "body": "Add the test-runner from yadayada into the repo so we don't depend on yadayada\nanymore. Closed by af9ba75e195fc740493c9af6dbe84105b369d640.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18353/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428", "body": "Seems to be fine now. We'll re-open the issue if anyone says otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18428/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255", "body": "Implemented by 048d67f4bfb37c75f0a5d3dd4d0b4e05da400185 +  89626c59ee3a4da1e36612449f43362799ac0305\nAnd it really _is fast_ compared to the database/key-value store backends which uses polling to wait for the result.\n\nTo enable this back-end add the following setting to your settings.py/celeryconf.py:\n\n```\nCELERY_BACKEND=\"amqp\"\n```\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/39255/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017", "body": "A sample implementation has been commited (794582ebb278b2f96080a4cf4a68f1e77c3b003b + 93f6c1810c1051f8bdea6a7eae21d111997388d00 + fe62c47cb04723af738192087b40caefd27cab6a ) but not tested yet. Currently seeking anyone willing to test this feature.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115", "body": "Task retries seems to be working with tests passing. Closed by 41a38bb25fcacb48ee925e4319d35af9ab89d2bf\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/38115/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721", "body": "Use basic.consume instead of basic.get to receive messages. Closed by 72fa505c7dfcf52c3215c276de67e10728898e70. This\nmeans the CELERY_QUEUE_WAKEUP_AFTER and CELERY_EMPTY_MSG_EMIT_EVERY settings,\nand the -w|--wakeup-after command line option to celeryd has been removed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17721/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902", "body": "If delay() is hanging, I'm guessing it's not because of the database, but because it can't get a connection to the AMQP server. (amqplib's default timeout must be very high). Is the broker running? Are you running RabbitMQ?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17902/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071", "body": "I set the default connection timeout to be 4 seconds. Closing this issue.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18071/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627", "body": "Re-opening the issue as setting the amqplib connection timeout didn't resolve it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/28627/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160", "body": "This is actually an issue with RabbitMQ and will be fixed in the 1.7 release. I added the following to the celery FAQ:\n\n RabbitMQ hangs if it isn't able to authenticate the current user,\nthe password doesn't match or the user does not have access to the vhost\nspecified.  Be sure to check your RabbitMQ logs\n(`/var/log/rabbitmq/rabbit.log` on most systems), it usually contains a\nmessage describing the reason.\n\nThis will be fixed in the RabbitMQ 1.7 release.\n\nFor more information see the relevant thread on the rabbitmq-discuss mailing\nlist: http://bit.ly/iTTbD\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/43160/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924", "body": "AsyncResult.ready() was always True. Closed by 4775a4c279179c17784bb72dc329f9a9d442ff0a. Thanks talentless.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/17924/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110", "body": "Oh. That's indeed a problem with the installation. Could you try to install it using pip?\n\n```\n$ easy_install pip\n$ pip install celery\n```\n\nI will fix it as soon as possible, but in the mean time you could use pip.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18110/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111", "body": "Only use README as long_description if the file exists so easy_install don't\nbreak. Closed by e8845afc1a53aeab5b30d82dea29de32eb46b1d6. Thanks tobycatlin\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18111/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169", "body": "The consumerset branch was merged into master in c01a9885bbb8c83846b3770364fe208977a093fd (original contribution: screeley/celery@e2d0a56c913c66f69bf0040c9b76f74f0bb7dbd8). Big thanks to Sean Creeley.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/35169/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472", "body": "Fixed in  faaa58ca717f230fe8b65e4804ad709265b18d5a\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18472/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432", "body": "By the way; This worked before we started using auto_ack=True, and basic.get.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18432/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448", "body": "Wait with message acknowledgement until the task has actually been executed.\nClosed by ef3f82bcf3b4b3de6584dcfc4b189ddadb4f50e6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18448/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017", "body": "This now merged into master. On second thought Munin plugins for these stats doesn't make sense, at least not generally. You could use this to make munin-plugins though, it's more like the groundwork for something more interesting later. (and it's useful for profiling right away)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/19017/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966", "body": "Make TaskSet.run() respect message options from the Task class. Closed by 03d30a32de3502b73bca370cb0e70863c0ad3dd2.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18966/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29867", "body": "Thanks for pointing that out. It's a bug, indeed.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29867/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38526", "body": "I added the importlib backport module as a dependency, is that >= 2.6 only? I guess I could add django==1.1 as a dependency, but I think there's someone using 1.0.x still.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38526/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38984", "body": "2.7 you mean? Yeah, but I added http://pypi.python.org/pypi/importlib as a dependency. Does it work with Python >= 2.4?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38984/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/38985", "body": "Ah, I just saw the trove classifiers, all the way down to 2.3.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38985/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/51698", "body": "This commit was actually authored by me, just forgot to reset my `GIT_AUTHOR_NAME`, and `GIT_AUTHOR_EMAIL` env variables.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/51698/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/52587", "body": "damn. Seems I forgot to reset my GIT_AUTHOR_\\* settings :(\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/52587/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/82477", "body": "Ah, ok. That's good to know! I wasn't sure about this. Also, I'm wondering what holds the connection, is it the engine or the session?\nCreating a new connection for every operation is probably a bad idea, but not sure if it does some connection pooling by default.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82477/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91900", "body": "lol, yeah. I guess this is borderline.\n\nYou have the ability to supply your own connection, if you do we can't close the connection for you.\n\nAbove it says:\n\n```\nconn = connection or establish_connection(connect_timeout=connect_timeout)\n```\n\nso `connection or conn.close`, is \"if user didn't supply a connection, close the connection we established\".\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91900/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/91937", "body": "Forgot that it's using the `@with_connection decorator` which takes care of this automatically :/\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91937/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/comments/352668", "body": "Thanks!  Fixed in 154431f2c4ff04515000462ede70e205672e1751\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352668/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280", "body": "Is this deliberate?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112280/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588", "body": "Why did you remove the `Content-Type`?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205588/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074", "body": "Think there's a race here if it enters `time.sleep()`, and the putlock is released when the pools state != RUN.  The task will be sent to the queue in this case.  I'll fix it before I merge.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251074/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078", "body": "Also changed the interval to 1.0 as we discussed on IRC.  The shutdown process is already almost always delayed by at least one second because of other thread sleeps, so it doesn't make a difference (apart from less CPU usage in online mode).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/251078/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181", "body": "Actually, the Pool implementation shouldn't depend on Celery, it's more of a patch for fixes and features we need for multiprocessing.\nSo this option should be added to `Pool.__init__`, then passed on from `celery.worker.WorkController`, then `celery.concurrency.processes.TaskPool`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374181/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183", "body": "and, oh yeah, `celery.conf` is deprecated to be removed in 3.0, so you don't have to add anything to it.\n\nWhen added to `WorkController`, it works in the case where you instantiate the worker without configuration too,\ne.g: `WorkController(worker_lost_wait=20)`.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374183/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375", "body": "Ah, this was just a typo, I copied the document...\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008375/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380", "body": "It's on my TODO, I have to write it\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1008380/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043", "body": "You can't terminate a reserved task, as it hasn't been executed yet.\n\nIt adds the id of the task to the list of revoked tasks, this is then checked again when the worker is about to execute the task (moves from reserved -> active).  If you only search the reserved requests then the terminate functionality would be useless (it wouldn't have any process to kill)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576043/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052", "body": "the other changes look great\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1576052/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034", "body": "Revoked check for reserved/ETA tasks happens here:\nhttps://github.com/celery/celery/blob/3.0/celery/worker/job.py#L185-186\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597034/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089", "body": "> Also, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n\nStill, you shouldn't terminate a reserved task.  If a reserved task is revoked it should be dropped _before_ any pool worker starts to execute it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1597089/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191", "body": "Hmm, maybe we could remove the `set` here, so that it preserves the original order.\n\nIt's not terribly bad if it imports the same module twice after all\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3297191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552", "body": "For Python 2.6 you have to include positions: {0} {1} {2}\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4962552/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484", "body": "Celery related code should not be called by this method as it should be decoupled from Celery.  You need to find a way to support this by exposing it in the API.\nE.g. `Worker(on_shutdown=signals.worker_process_shutdown.send)`\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7127484/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390", "body": "Since created is True, it would also call 'on_node_join' in this instance, even if it just left.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/8476390/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331", "body": "Should this be logged?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304331/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "gcachet": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/15549", "body": "It's master. I figured out this way to define tasks.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15549/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557", "body": "Thanks! Removing yadayada dependency was my first though also, but I wasn't sure about the implications.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/15557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "talentless": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18501", "body": "No problem!\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18501/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "tobycatlin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18135", "body": "pip installed ok. I haven't tried the source code out of git. \n\nThanks for the quick response\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18135/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "commit_comments": [], "review_comments": []}, "brosner": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [{"url": "https://api.github.com/repos/celery/celery/issues/comments/18434", "body": "My best guess is we've called terminate more than once? Will need some investigation.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/issues/comments/18434/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "commit_comments": [], "review_comments": []}, "nikitka": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/29856", "body": "why after **import** you use import celeryconfig ? this is work?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/29856/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "brettcannon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/38473", "body": "Django as of (I believe) 1.1 has importlib included w/ it under django.util.importlib, so you don't need to rely on Python 2.6 to get import_module.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38473/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/38959", "body": "importlib was added in Python 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/38959/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "paltman": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/79488", "body": "This is why the test now fails -- now that is no longer 29 minutes past the hour, but 30, it is now due to run, while the assertion was to make sure it handled the case when it wasn't due, which when the mocked value was 29 minutes past the hour it properly returned False.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/79488/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/comments/204916", "body": "Was there a test for this failure before the fix?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/204916/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "nvie": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/81557", "body": "Nice and tidy.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/81557/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "haridsv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/82469", "body": "Is this creating a new engine every time a new session is needed? As per sqlalchemy documentation, engine should be created only one time, unless of course the connection URL itself is changing.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82469/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}, {"url": "https://api.github.com/repos/celery/celery/comments/82478", "body": "Yes, the engine by default has a connection pool enabled: http://www.sqlalchemy.org/docs/05/reference/sqlalchemy/pooling.html#connection-pool-configuration\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/82478/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "jonozzz": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/91703", "body": "I don't quite understand this one... why \"connection\" and why \"or\" ?\nI think it should be:\n    conn and conn.close()\n\nBecause when connection is None, conn.close() will be executed anyway.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/91703/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "simonz05": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/103991", "body": "Nice, the previous one was aweful. Good to have the side-bar back.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/103991/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "Kami": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/104086", "body": "I agree, great work.\n\nThe blue (first?) version wasn't bad either, but the previous version was kinda step in the wrong direction.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/104086/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "adamn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/123743", "body": "$VIRTUALENV was removed - does it still work with a virtualenv?  It doesn't seem to for me on Ubuntu 10.04\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/123743/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zen4ever": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/131915", "body": "Thanks for the fix. That was fast.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/131915/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "joshdrake": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136184", "body": "Was just about to post an issue on this. I was surprised by this too, but even subclasses of accepted types must have adapters registered for database backends. Here's documentation on the process for pyscopg2:\n\nhttp://initd.org/psycopg/docs/advanced.html#adapting-new-types\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136184/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "zzzeek": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/136191", "body": "you need to look at TypeDecorator:\n\nhttp://www.sqlalchemy.org/docs/reference/sqlalchemy/types.html?highlight=typedecorator#sqlalchemy.types.TypeDecorator\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/136191/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "passy": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/147340", "body": "The dot should be inside the comment. It's a syntax error otherwise.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/147340/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "dcramer": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/175143", "body": "I should note I have no idea what this does, I stole it form somewhere on the internet and it fixed the problems :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/175143/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "shimon": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/270710", "body": "Why did the date_done entry get removed? This is useful information that other backends seem to provide.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/270710/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}], "review_comments": []}, "enlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/352650", "body": "this should be _kill(...)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/352650/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}], "review_comments": []}, "mher": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [{"url": "https://api.github.com/repos/celery/celery/comments/595269", "body": "https://bitbucket.org/jezdez/sphinx-pypi-upload/issue/1/minor-patch-for-namespace-modules-etc patch should be applied before launching  paver upload_pypi_docs. upload_pypi_docs fails if .build contains empty subdirectories.\n\nIs there a way to automate this?\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/comments/595269/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810", "body": "I think it would be better to move registry._set_default_serializer('auth') to setup_security\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/3502810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "steeve": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600", "body": "yes, this allows for the result backend to sub on this, allowing it not to poll\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/112600/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}, "mitar": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921", "body": "As I explained, the content is urlencoded combination of parameters, not JSON. Why it would be `application/json`? We do not serialize to JSON (at this stage) anywhere. But urllib does urlencode it. If we remove manual override, then urllib does the right thing and sets it to `application/x-www-form-urlencoded` (it also sets `Content-Length` properly). This can Django (or any other receiver) then properly decode. So, urllib sends content as `application/x-www-form-urlencoded` and header should match that.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/205921/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "brendoncrawford": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182", "body": "Ok, I'll submit a new patch within the next week or so.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374182/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188", "body": "Ok, ill take a stab at it. Might take a few tries, but I think I can get it.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/374188/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ztlpn": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747", "body": "Well I have not been able to check it with the latest version yet, but at least on version 3.0.6 this does not happen! If reserved but not yet active task is revoked, no check against revoked list is made when task becomes active. Instead it executes normally.\n\nAlso, as the set of reserved tasks includes active tasks as well, terminate works as expected (e.g. terminates active tasks)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1591747/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ambv": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809", "body": "SIGTERM is not 9.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659809/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}, {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810", "body": "Ditto.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/1659810/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "mlavin": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553", "body": "This style of string formatting is only available to Python 2.7+. I believe Celery is still supporting 2.6.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4940553/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "VRGhost": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911", "body": "Well, it should be\n\n```\nwarnings.warn(\"%s consumed store_result call with args %s, %s\" % (self.__class__.__name__, args, kwargs))\n```\n\nthan. :-)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/4960911/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "NONE"}]}, "dmtaub": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779", "body": "Ok, I will work on an api-based version soon. Depending on whether I need\nto write zmq interprocess communication this week, it might end up being\nless important to merge our branches at this particular moment :)\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7128779/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "andrewkittredge": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195", "body": "this is wrong.\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/7473195/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "CONTRIBUTOR"}]}, "ionelmc": {"issues": [], "commits": [], "pull_requests": [], "issue_comments": [], "commit_comments": [], "review_comments": [{"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835", "body": "I added this to help with debugging #1785. I still think we should have it (it could indicate other problems).\n", "reactions": {"url": "https://api.github.com/repos/celery/celery/pulls/comments/9304835/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0}, "author_association": "OWNER"}]}}}}